{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Neurosurfer helps you build intelligent apps that blend LLM reasoning, tools, and retrieval with a ready-to-run FastAPI backend and a React dev UI. Start lean, add power as you go \u2014 CPU-only or GPU-accelerated.</p>"},{"location":"#whats-in-the-box","title":"\ud83d\ude80 What\u2019s in the box","text":"<ul> <li>\ud83e\udd16 Agents: Production-ready patterns for ReAct, SQL, RAG, Router etc. think \u2192 act \u2192 observe \u2192 answer</li> <li>\ud83e\udde0 Models: Unified interface for OpenAI-style and local backends like Transformers/Unsloth, vLLM, Llama.cpp etc.</li> <li>\ud83d\udcda RAG: Simple, swappable retrieval core: embed \u2192 search \u2192 format \u2192 token-aware trimming</li> <li>\u2699\ufe0f FastAPI Server: OpenAI-compatible endpoints for chat + tools \u2014 custom endpoints, chat handlers, RAG etc.</li> <li>\ud83d\udda5\ufe0f NeurowebUI: React chat UI (GPT-style) that communicates with the server out-of-the-box</li> <li>\ud83e\uddea CLI: <code>neurosurfer serve</code> to run server/UI \u2014 custom backend app and UI support</li> </ul>"},{"location":"#news","title":"\ud83d\uddde\ufe0f News","text":"<ul> <li>Agents: ReAct &amp; SQLAgent upgraded with bounded retries, spec-aware input validation, and better final-answer streaming; new ToolsRouterAgent for quick one-shot tool picks.</li> <li>Models: Cleaner OpenAI-style responses across backends; smarter token budgeting + fallbacks when tokenizer isn\u2019t available.</li> <li>Server: Faster startup, better logging/health endpoints, and safer tool execution paths; OpenAI-compatible routes refined for streaming/tool-calling.</li> <li>CLI: <code>serve</code> now runs backend-only or UI-only and auto-injects <code>VITE_BACKEND_URL</code>; new subcommands for ingest/traces to standardize local workflows.</li> </ul> <p>Looking for older updates? Check the repo Releases and Changelog.</p>"},{"location":"#quick-start","title":"\u26a1 Quick Start","text":"<p>A 60-second path from install \u2192 dev server \u2192 your first inference.</p> <p>Install (minimal core): <pre><code>pip install -U neurosurfer\n</code></pre></p> <p>Or full LLM stack (torch, transformers, bnb, unsloth): <pre><code>pip install -U \"neurosurfer[torch]\"\n</code></pre></p> <p>Run the dev server (backend + UI): <pre><code>neurosurfer serve\n</code></pre> - Auto-detects UI; pass <code>--ui-root</code> if needed. First run may <code>npm install</code>. - Backend binds to config defaults; override with flags or envs.</p> <p>Hello LLM Example: <pre><code>from neurosurfer.models.chat_models.transformers import TransformersModel\n\nllm = TransformersModel(\n  model_name=\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n  load_in_4bit=True\n)\nres = llm.ask(user_prompt=\"Say hi!\", system_prompt=\"Be concise.\", stream=False)\nprint(res.choices[0].message.content)\n</code></pre></p>"},{"location":"#high-level-architecture","title":"\ud83c\udfd7\ufe0f High-Level Architecture","text":"<p>Neurosurfer Architecture</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li> <p> Production API</p> <p>Deploy with FastAPI, authentication, chat APIs, and OpenAI-compatible endpoints.</p> <p> Server setup</p> </li> <li> <p> Intelligent Agents</p> <p>Build ReAct, SQL, and RAG agents with minimal code. Each agent type is optimized for specific tasks.</p> <p> Learn about agents</p> </li> <li> <p> Rich Tool Ecosystem</p> <p>Use built-in tools or create your own \u2014 calculators, web calls, files, custom actions.</p> <p> Explore tools</p> </li> <li> <p> RAG System</p> <p>Ingest documents, chunk intelligently, and retrieve relevant context for your LLMs.</p> <p> RAG System</p> </li> <li> <p> Vector Databases</p> <p>Built-in ChromaDB, extensible interface for other stores.</p> <p> Vector stores</p> </li> <li> <p> Multi-LLM Support</p> <p>Work with OpenAI, Transformers/Unsloth, vLLM, Llama.cpp, and OpenAI-compatible APIs.</p> <p> Model docs</p> </li> </ul>"},{"location":"#install-options","title":"\ud83d\udce6 Install Options","text":"<p>pip (recommended) <pre><code>pip install -U neurosurfer\n</code></pre></p> <p>pip + full LLM stack <pre><code>pip install -U \"neurosurfer[torch]\"\n</code></pre></p> <p>From source <pre><code>git clone https://github.com/NaumanHSA/neurosurfer.git\ncd neurosurfer &amp;&amp; pip install -e \".[torch]\"\n</code></pre></p> <p>CUDA notes (Linux x86_64): <pre><code># Wheels bundle CUDA; you just need a compatible NVIDIA driver.\npip install -U torch --index-url https://download.pytorch.org/whl/cu124\n# or CPU-only:\npip install -U torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre></p>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>Licensed under Apache-2.0. See <code>LICENSE</code>.</p>"},{"location":"#support","title":"\ud83c\udf1f Support","text":"<ul> <li>\u2b50 Star the project on GitHub.</li> <li>\ud83d\udcac Ask &amp; share in Discussions: Discussions.</li> <li>\ud83e\udde0 Read the Docs.</li> <li>\ud83d\udc1b File Issues.</li> <li>\ud83d\udd12 Security: report privately to naumanhsa965@gmail.com.</li> </ul>"},{"location":"#citation","title":"\ud83d\udcda Citation","text":"<p>If you use Neurosurfer in your work, please cite:</p> <pre><code>@software{neurosurfer,\n  author       = {Nouman Ahsan and Neurosurfer contributors},\n  title        = {Neurosurfer: A Production-Ready AI Agent Framework},\n  year         = {2025},\n  url          = {https://github.com/NaumanHSA/neurosurfer},\n  version      = {0.1.0},\n  license      = {Apache-2.0}\n}\n</code></pre> <sub>Built with \u2764\ufe0f by the Neurosurfer team"},{"location":"cli/","title":"CLI &amp; Dev Server Guide (Enhanced)","text":"<p>The Neurosurfer CLI is a single command that boots your developer stack: it starts the FastAPI backend, optionally brings up the NeurowebUI. Use it for day-to-day development, local demos, and quick sanity checks of your app and UI.</p> <p>The CLI command is <code>neurosurfer</code>. Its primary subcommand is <code>serve</code>, which starts the backend API and, if desired, the NeurowebUI dev server. The backend binds to host/port from your configuration or flags. The UI is auto\u2011discovered (or provided via <code>--ui-root</code>) and runs either as a Vite dev server or a static build server (if you point at a build dir).</p> <p>Use <code>--help</code> to see available flags: <pre><code>neurosurfer --help\nneurosurfer serve --help\n</code></pre></p>"},{"location":"cli/#what-happens-if-nothing-is-specified","title":"What happens if nothing is specified?","text":"<p>If you run <code>neurosurfer serve</code> with no flags, the CLI boots the built-in example backend (a ready-to-use NeurosurferApp) on your configured host/port and then looks for a bundled UI build; if it finds one, it serves it automatically, otherwise it runs backend-only. For details, see: Built-in Example App and Bundled UI Build.</p>"},{"location":"cli/#quick-navigation","title":"\ud83d\ude80 Quick Navigation","text":"<ul> <li> <p> Overview</p> <p>What the CLI does, command structure, and where it runs.</p> <p> Read Overview</p> </li> <li> <p> App Resolution</p> <p>How files, modules, and <code>NeurosurferApp</code> instances are discovered.</p> <p> Learn Resolution</p> </li> <li> <p> NeurosurferUI</p> <p>How <code>--ui-root</code> is detected, when installs run, Vite dev server args.</p> <p> UI Details</p> </li> <li> <p> Recipes &amp; Examples</p> <p>Common scenarios\u2014backend only, custom modules, public host URLs, and more.</p> <p> Try Recipes</p> </li> <li> <p> Security &amp; Ops</p> <p>CORS, reverse proxy, environment variables, graceful shutdown.</p> <p> Ops Notes</p> </li> </ul>"},{"location":"cli/#key-options","title":"Key Options","text":"<p>Below is a quick reference of the most useful flags. Full reference tables are in Environment &amp; Flags.</p>"},{"location":"cli/#backend-flags-essentials","title":"Backend flags (essentials)","text":"<ul> <li><code>--backend-app</code>: points to the backend app (see App Resolution below)</li> <li><code>--backend-host</code>, <code>--backend-port</code></li> <li><code>--backend-log-level</code></li> <li><code>--backend-reload</code></li> <li><code>--backend-workers</code></li> <li><code>--backend-worker-timeout</code></li> </ul>"},{"location":"cli/#ui-flags-essentials","title":"UI flags (essentials)","text":"<ul> <li><code>--ui-root</code>: path to NeurowebUI (folder with <code>package.json</code>) or a prebuilt static dir with <code>index.html</code></li> <li><code>--ui-host</code>, <code>--ui-port</code></li> <li><code>--ui-strict-port</code></li> <li><code>--ui-open</code> (default: true; pass <code>--ui-open=0</code> or <code>--ui-open=false</code> to disable)</li> <li><code>--npm-install {auto|always|never}</code> (default: <code>auto</code>)</li> <li><code>--only-backend</code> / <code>--only-ui</code></li> </ul> <p>What\u2019s new? The CLI now opens the browser only after the UI is reachable, so your \u201copen UI\u201d message isn\u2019t buried under backend logs.</p>"},{"location":"cli/#backend-app-resolution","title":"Backend App Resolution","text":"<p>When you provide <code>--backend-app</code>, the CLI can interpret three forms and resolve them into a runnable NeurosurferApp instance.</p>"},{"location":"cli/#1-default-no-backend-app","title":"1) Default (no <code>--backend-app</code>)","text":"<p>If omitted, the CLI runs the built\u2011in example shipped with the package: <pre><code>neurosurfer.examples.quickstart_app:ns\n</code></pre></p>"},{"location":"cli/#2-module-path-pkgmoduleattr_or_factory","title":"2) Module path (<code>pkg.module[:attr_or_factory]</code>)","text":"<p>Point to a module and an attribute/factory yielding a <code>NeurosurferApp</code>:</p> <pre><code>neurosurfer serve --backend-app mypkg.api:ns\nneurosurfer serve --backend-app mypkg.api:create_app()\n</code></pre>"},{"location":"cli/#3-file-path-pathtoapppy","title":"3) File path (<code>/path/to/app.py</code>)","text":"<p>Pass a Python file that defines a <code>NeurosurferApp</code> instance; the CLI executes the file in an isolated namespace and scans for the instance:</p> <pre><code>neurosurfer serve --backend-app ./app.py --backend-reload\n</code></pre> <p>File mode expectations</p> <p>Ensure your file creates a <code>NeurosurferApp</code> instance but does not call <code>app.run()</code> at import time. The CLI will run it. Keep direct runs behind <code>if __name__ == '__main__':</code>.</p>"},{"location":"cli/#neurosurferui","title":"NeurosurferUI","text":"<p>The NeurosurferUI is a Vite\u2011powered dev server for development. The CLI can also serve a built UI directory statically.</p>"},{"location":"cli/#root-discovery","title":"Root discovery","text":"<p>If <code>--ui-root</code> is omitted, we try common relative paths (e.g., <code>neurosurferui</code>), or read <code>NEUROSURF_UI_ROOT</code>. If not found, the CLI runs backend only with a helpful log.</p> <pre><code>neurosurfer serve --ui-root /path/to/neurowebui\n</code></pre>"},{"location":"cli/#firstrun-dependency-install","title":"First\u2011run dependency install","text":"<p>With <code>--npm-install auto</code> (default), the CLI runs <code>npm install --force</code> only if <code>node_modules</code> is missing. Use <code>always</code> to force install, or <code>never</code> to skip.</p>"},{"location":"cli/#backend-url-for-the-ui","title":"Backend URL for the UI","text":"<p>If the backend binds to <code>0.0.0.0</code> or <code>::</code>, browsers can\u2019t use that literal host. The CLI injects <code>VITE_BACKEND_URL</code> using <code>NEUROSURF_PUBLIC_HOST</code> (or <code>127.0.0.1</code> by default). Set it explicitly if you\u2019re exposing the backend on a LAN or public IP:</p> <pre><code>export NEUROSURF_PUBLIC_HOST=192.168.1.25\nneurosurfer serve\n</code></pre> <p>Cross\u2011origin requests</p> <p>Use permissive CORS in local dev; in production, restrict to your domains. See Configuration for knobs.</p>"},{"location":"cli/#recipes-examples","title":"Recipes &amp; Examples","text":""},{"location":"cli/#backend-only","title":"Backend only","text":"<pre><code>neurosurfer serve --only-backend --backend-host 0.0.0.0 --backend-port 8081\n</code></pre>"},{"location":"cli/#ui-only-vite-dev","title":"UI only (Vite dev)","text":"<pre><code>neurosurfer serve --only-ui --ui-root /path/to/neurowebui\n</code></pre>"},{"location":"cli/#serve-a-module-attribute","title":"Serve a module attribute","text":"<pre><code>neurosurfer serve --backend-app mypkg.myapp:ns --backend-reload\n</code></pre>"},{"location":"cli/#serve-a-factory","title":"Serve a factory","text":"<pre><code>neurosurfer serve --backend-app mypkg.myapp:create_app()\n</code></pre>"},{"location":"cli/#serve-a-local-file","title":"Serve a local file","text":"<pre><code>neurosurfer serve --backend-app ./app.py --backend-reload\n</code></pre>"},{"location":"cli/#use-a-public-backend-url-for-the-ui","title":"Use a public backend URL for the UI","text":"<pre><code>export NEUROSURF_PUBLIC_HOST=192.168.1.25\nneurosurfer serve\n</code></pre>"},{"location":"cli/#force-ui-dependency-install","title":"Force UI dependency install","text":"<pre><code>neurosurfer serve --npm-install always\n</code></pre>"},{"location":"cli/#strict-ui-port-usage","title":"Strict UI port usage","text":"<pre><code>neurosurfer serve --ui-strict-port --ui-port 5173\n</code></pre>"},{"location":"cli/#security-operations","title":"Security &amp; Operations","text":""},{"location":"cli/#cors-auth","title":"CORS &amp; Auth","text":"<p>Control CORS via config. For private deployments, pair CORS with a reverse proxy (Nginx, Traefik) and your preferred auth. See Configuration for the complete set.</p>"},{"location":"cli/#reverse-proxy-tls","title":"Reverse Proxy &amp; TLS","text":"<p>Terminate TLS at a reverse proxy and forward to the backend. Keep ports non\u2011public where possible. If you must expose the backend directly, enforce strong auth and restrict IP ranges.</p>"},{"location":"cli/#logs-levels","title":"Logs &amp; Levels","text":"<p>Use <code>--backend-log-level</code>. During local debugging, <code>debug</code> can be handy; <code>info</code> is the sane default. UI and backend child logs are piped with <code>[ui]</code> and <code>[api]</code> prefixes. A single banner summarizes URLs after readiness checks.</p>"},{"location":"cli/#graceful-shutdown","title":"Graceful Shutdown","text":"<p>We register <code>SIGINT</code>/<code>SIGTERM</code> handlers. On shutdown, both UI and backend are terminated with a short grace period before a hard kill to avoid lingering watchers/sockets.</p> <p>Non\u2011interactive runs (CI)</p> <p>Set <code>NEUROSURF_SILENCE=1</code> to suppress banners in CI, and pin ports carefully to avoid collisions.</p>"},{"location":"cli/#environment-flags-reference","title":"Environment &amp; Flags Reference","text":""},{"location":"cli/#backend-flags-env","title":"Backend (flags &amp; env)","text":"Setting Flag Env Var Default Description Backend app <code>--backend-app</code> \u2013 <code>neurosurfer.examples.quickstart_app:ns</code> Module attr/factory or file path that yields a <code>NeurosurferApp</code>. Host <code>--backend-host</code> <code>NEUROSURF_BACKEND_HOST</code> from config Bind address for API. Port <code>--backend-port</code> <code>NEUROSURF_BACKEND_PORT</code> from config Bind port for API. Log level <code>--backend-log-level</code> <code>NEUROSURF_BACKEND_LOG</code> from config Logging verbosity (<code>debug</code>, <code>info</code>, etc.). Reload <code>--backend-reload</code> \u2013 <code>false</code> Auto\u2011reload for dev. Workers <code>--backend-workers</code> <code>NEUROSURF_BACKEND_WORKERS</code> from config Number of worker processes. Worker timeout (s) <code>--backend-worker-timeout</code> <code>NEUROSURF_BACKEND_WORKER_TIMEOUT</code> from config Worker timeout in seconds."},{"location":"cli/#ui-flags-env","title":"UI (flags &amp; env)","text":"Setting Flag Env Var Default Description UI root <code>--ui-root</code> <code>NEUROSURF_UI_ROOT</code> auto\u2011detect Path to Vite project (with <code>package.json</code>) or a build dir (<code>index.html</code>). UI host <code>--ui-host</code> <code>NEUROSURF_UI_HOST</code> from config Bind host for Vite/static server. UI port <code>--ui-port</code> <code>NEUROSURF_UI_PORT</code> from config Bind port for Vite/static server. Strict port <code>--ui-strict-port</code> \u2013 <code>false</code> Fail if port is in use (Vite). Open UI in browser <code>--ui-open</code> <code>NEUROSURF_UI_OPEN</code> <code>true</code> Auto\u2011open browser when UI becomes reachable. Accepts <code>1/0</code>, <code>true/false</code>. NPM install policy <code>--npm-install</code> <code>NEUROSURF_NPM_INSTALL</code> <code>auto</code> <code>auto</code>: only if missing <code>node_modules</code>; <code>always</code>: force; <code>never</code>: skip. Only backend <code>--only-backend</code> \u2013 <code>false</code> Run API only. Only UI <code>--only-ui</code> \u2013 <code>false</code> Run UI only."},{"location":"cli/#crosscutting","title":"Cross\u2011cutting","text":"Setting Env Var Default Description Public host for URL composition <code>NEUROSURF_PUBLIC_HOST</code> <code>127.0.0.1</code> Used to craft <code>VITE_BACKEND_URL</code> when API binds <code>0.0.0.0</code>/<code>::</code>. Silence banners &amp; optional\u2011deps warnings <code>NEUROSURF_SILENCE</code> <code>0</code> Set <code>1</code> to reduce noise (e.g., CI). Eager runtime assert (deps) <code>NEUROSURF_EAGER_RUNTIME_ASSERT</code> <code>0</code> Set <code>1</code> to fail fast on missing optional LLM deps. <p>Note on <code>--ui-open</code> default: The modular implementation treats UI auto\u2011open as enabled by default. You can disable it with <code>--ui-open=false</code>, <code>--ui-open=0</code>, or by setting <code>NEUROSURF_UI_OPEN=0</code>.</p>"},{"location":"cli/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Port in use: Use <code>--ui-strict-port</code> or pick a free <code>--ui-port</code>. For the API, change <code>--backend-port</code>.</li> <li>UI can\u2019t reach API: Ensure <code>VITE_BACKEND_URL</code> resolves. Set <code>NEUROSURF_PUBLIC_HOST</code> when binding <code>0.0.0.0</code>.</li> <li><code>npm</code> not found: Install Node.js/npm or run <code>--only-backend</code>.</li> <li>Long password error on register (bcrypt 72\u2011byte limit): Use a shorter password or configure truncation in your auth layer.</li> <li>Static UI: If you point <code>--ui-root</code> at a build folder (must contain <code>index.html</code>), the CLI will serve it via a lightweight static server.</li> </ul>"},{"location":"cli/#ready-to-launch","title":"Ready to launch?","text":"<pre><code># Backend + UI (auto-open, default)\nneurosurfer serve --backend-host 0.0.0.0 --backend-port 8081 --ui-root ./neurosurferui\n\n# Disable auto-open\nneurosurfer serve --ui-open=0\n\n# Backend only\nneurosurfer serve --only-backend --backend-port 8081\n</code></pre>"},{"location":"contributing/","title":"Contributing to Neurosurfer","text":"<p>Thank you not only for using Neurosurfer, but also for being interested in helping it grow! We welcome contributions in all forms \u2014 code, ideas, docs, testing, or simply by spreading the word. Your participation makes the ecosystem better for everyone. \ud83d\udc99</p>"},{"location":"contributing/#how-you-can-help","title":"How You Can Help","text":"<p>There isn\u2019t just one way to contribute. Some people dive into code; others lift the community by sharing knowledge. All of it matters.</p> <ul> <li>Support the community \u2014 answer questions, review small PRs, and share quick tips in discussions.</li> <li>Fix bugs \u2014 reproduce the issue, include a minimal snippet, and submit a clear, targeted fix.</li> <li>Share ideas \u2014 describe the problem, the desired outcome, and any alternatives you considered.</li> <li>Build features \u2014 open an issue to align scope, then ship a focused, well-tested PR.</li> <li>Improve docs \u2014 add concise examples, clarify tricky spots, and keep instructions accurate and current.</li> <li>Spread the word \u2014 write posts, give demos, or simply \u2b50 the repo to help others find it.</li> </ul>"},{"location":"contributing/#submitting-issues-ideas","title":"Submitting Issues &amp; Ideas","text":"<p>We love actionable, well\u2011scoped reports and proposals. A great issue saves everyone time:</p> <ul> <li>Search first. Please check if your topic already exists in Issues/Discussions.</li> <li>Add context. Where are you running (local, Docker, Colab, Kaggle)? Which OS/Python/Neurosurfer version? Which model/backends?</li> <li>Be specific. For bugs, include a minimal snippet (or steps) to reproduce. Screenshots, stack traces, or logs are extremely helpful.</li> <li>Stay focused. One issue per topic keeps the conversation crisp and the fix targeted.</li> </ul> <p>If it\u2019s a feature request, explain the problem you\u2019re solving, the desired outcome, and any alternatives you considered. A tiny pseudo\u2011API example is perfect.</p>"},{"location":"contributing/#pull-requests-prs","title":"Pull Requests (PRs)","text":"<p>PRs are very welcome \u2014 small and focused is best. Before you open one:</p> <ul> <li>Make sure your change builds locally and tests pass.</li> <li>Include tests for new behavior when possible.</li> <li>Update docs if user\u2011facing behavior changes.</li> <li>Keep the description short but clear about what and why.</li> </ul> <p>We review for correctness, clarity, and scope. Friendly feedback is normal; we appreciate your iteration!</p>"},{"location":"contributing/#development-quickstart","title":"Development Quickstart","text":"<p>If you want to run Neurosurfer locally for development or to try changes:</p> <pre><code># Fork on GitHub, then clone your fork\ngit clone https://github.com/&lt;your-username&gt;/neurosurfer.git\ncd neurosurfer\n\n# Create and activate a virtual environment\npython -m venv .venv &amp;&amp; source .venv/bin/activate   # Windows: .venv\\Scripts\\activate\n\n# Install core (pick extras as needed)\npip install -U pip\npip install -e .\n# Optional stacks:\n# pip install -e '.[torch]'      # local model stack\n\n# Run tests\npytest -q\n\n# Lint &amp; format\nblack neurosurfer tests &amp;&amp; ruff check --fix .\n</code></pre> <p>If you\u2019re working on the docs: <pre><code>mkdocs serve   # preview locally at http://127.0.0.1:8000\n</code></pre></p>"},{"location":"contributing/#code-style-testing","title":"Code Style &amp; Testing","text":"<ul> <li>Style: Black for formatting, Ruff for linting, MyPy for typing on changed files.</li> <li>Tests: Keep them minimal, readable, and fast. If you fix a bug, add a test that would have caught it.</li> <li>Scope: Prefer smaller PRs with clear intent over giant changes that are harder to review.</li> </ul>"},{"location":"contributing/#communication-conduct","title":"Communication &amp; Conduct","text":"<p>We aim for a welcoming, inclusive space. Be respectful, kind, and constructive. If you see something that could be better, propose a change \u2014 we\u2019re all building this together.</p> <ul> <li>Issues \u2192 bugs and feature requests  </li> <li>Discussions \u2192 questions, design ideas, showcases  </li> <li>Security \u2192 email naumanhsa965@gmail.com</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions are provided under the project license (Apache\u20112.0).</p> <p>Thank you for helping improve Neurosurfer \u2014 we\u2019re excited to see what you build! \ud83d\ude4c</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to Neurosurfer \u2014 a production\u2011grade AI framework with multi\u2011LLM, RAG, agents, tools, and a FastAPI server. This page walks you through installation, the CLI, and basic Python usage. It\u2019s written in a hybrid style: short explanatory paragraphs with bullets where they help.</p>"},{"location":"getting-started/#quick-navigation","title":"\ud83d\ude80 Quick Navigation","text":"<ul> <li> <p> Installation</p> <p>Install via pip or from source, and learn about CPU/GPU (CUDA/MPS) options.</p> <p> Go to Installation</p> </li> <li> <p> CLI Usage</p> <p>Serve the backend and (optionally) the NeurowebUI with one command.</p> <p> Go to CLI</p> </li> <li> <p> Basic Usage</p> <p>Import models &amp; agents, call an LLM, and plug in RAG.</p> <p> Go to Basic Usage</p> </li> <li> <p> Configuration</p> <p>Configure API keys, models, server options, and environment variables.</p> <p> Open Configuration</p> </li> </ul>"},{"location":"getting-started/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Neurosurfer installs as a lightweight core so you can get started quickly. A typical setup uses Python 3.9+ and a virtual environment. If you plan to run the NeurowebUI locally, you\u2019ll also need Node.js and npm. Hardware acceleration is optional but recommended when working with larger models.</p> <ul> <li>Python <code>&gt;= 3.9</code> and pip (or uv/pipx/poetry)</li> <li>Node.js + npm for the NeurowebUI dev server (optional)</li> <li>GPU (optional): NVIDIA CUDA on Linux x86_64, or Apple Silicon (MPS) on macOS arm64. CPU\u2011only works fine for smaller models or demos.</li> </ul> <p>Keep installs lightweight</p> <p>Neurosurfer\u2019s core installs without heavy model deps. Add the full LLM stack only when you need it via the <code>torch</code> extra.</p>"},{"location":"getting-started/#installation","title":"\ud83e\uddf0 Installation","text":"<p>Installation is flexible: minimal core for APIs, or the full LLM stack for local inference/finetuning. If you\u2019re unsure, start minimal; you can always add the extra later.</p>"},{"location":"getting-started/#minimal-core","title":"Minimal core","text":"<pre><code>pip install -U neurosurfer\n</code></pre> <p>This keeps dependencies light \u2014 ideal for API servers, docs, or CI.</p>"},{"location":"getting-started/#full-llm-stack-recommended-for-model-work","title":"Full LLM stack (recommended for model work)","text":"<pre><code>pip install -U 'neurosurfer[torch]'\n</code></pre> <p>This extra adds <code>torch</code>, <code>transformers</code>, <code>sentence-transformers</code>, <code>accelerate</code>, <code>bitsandbytes</code>, and <code>unsloth</code>.</p> <p>Pin with constraints (optional)</p> <p>If you prefer exact versions, use a constraints/requirements file; the extra is a convenience group.</p>"},{"location":"getting-started/#gpu-cuda-notes-pytorch","title":"GPU / CUDA notes (PyTorch)","text":"<p>If you want GPU acceleration, the pip wheel already bundles CUDA; you do not need to install the system CUDA toolkit. You do need a compatible NVIDIA driver. Use these commands to verify and install appropriately:</p> <ol> <li> <p>Check your GPU and driver <pre><code>nvidia-smi\n</code></pre>    Confirm the driver is active and note the CUDA version the driver supports.</p> </li> <li> <p>Install a CUDA\u2011enabled torch wheel (Linux x86_64): <pre><code># Example for CUDA 12.4 wheels:\npip install -U torch --index-url https://download.pytorch.org/whl/cu124\n</code></pre></p> </li> <li> <p>Install bitsandbytes (optional, Linux x86_64): <pre><code>pip install -U bitsandbytes\n</code></pre>    On macOS/Windows, bitsandbytes wheels are limited; prefer CPU or other quantization strategies.</p> </li> <li> <p>Apple Silicon (MPS): <pre><code>pip install -U torch\n</code></pre>    PyTorch includes MPS support on macOS arm64. No extra toolkit required.</p> </li> <li> <p>CPU\u2011only (portable): <pre><code>pip install -U torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre></p> </li> </ol> <p>Driver/toolkit alignment</p> <p>If <code>torch.cuda.is_available()</code> is <code>False</code> despite a GPU, your driver may be outdated. Update the NVIDIA driver to a version compatible with the wheel you installed (e.g., CU124) and restart. You generally do not need the full CUDA toolkit for pip wheels.</p> <p>Quick GPU sanity check</p> <pre><code>import torch; print('CUDA available:', torch.cuda.is_available())\nif torch.cuda.is_available():\n    print('GPU:', torch.cuda.get_device_name(0))\n</code></pre>"},{"location":"getting-started/#build-from-source","title":"Build from source","text":"<pre><code>git clone https://github.com/NaumanHSA/neurosurfer.git\ncd neurosurfer\npip install -U pip wheel build\npip install -e .            # editable\n# or\npip install -e '.[torch]'   # editable + full LLM stack\n</code></pre>"},{"location":"getting-started/#quick-import-check","title":"Quick import check","text":"<pre><code>python -c \"import neurosurfer; print('ok')\"\n</code></pre> <p>You\u2019ll see a banner; if optional deps are missing, you\u2019ll get a single consolidated warning with install hints.</p> <p>Useful environment flags</p> <ul> <li><code>NEUROSURF_SILENCE=1</code> \u2014 hide banner &amp; warnings  </li> <li><code>NEUROSURF_EAGER_RUNTIME_ASSERT=1</code> \u2014 fail fast at import if LLM deps missing (opt\u2011in)</li> </ul>"},{"location":"getting-started/#cli-usage","title":"\ud83d\udda5\ufe0f CLI Usage","text":"<p>The CLI runs the backend API and (optionally) the NeurowebUI dev server. Start simple with <code>neurosurfer serve</code>, then refine with flags as you scale.</p>"},{"location":"getting-started/#help","title":"Help","text":"<pre><code>neurosurfer --help\nneurosurfer serve --help\n</code></pre>"},{"location":"getting-started/#start-backend-ui-dev","title":"Start backend + UI (dev)","text":"<pre><code>neurosurfer serve\n</code></pre> <p>The backend binds to <code>NEUROSURF_BACKEND_HOST</code> / <code>NEUROSURF_BACKEND_PORT</code> (from config). The UI root is auto\u2011detected; pass <code>--ui-root</code> if needed. On first run, an <code>npm install</code> may occur \u2014 you can control this with <code>--npm-install</code>.</p>"},{"location":"getting-started/#common-options","title":"Common options","text":"<ul> <li>Backend app (<code>--backend-app</code>): module path like <code>pkg.module:app_or_factory()</code> or a Python file with a <code>NeurosurferApp</code> instance. The default is <code>neurosurfer.examples.quickstart_app:ns</code>.</li> <li>Backend: <code>--backend-host</code>, <code>--backend-port</code>, <code>--backend-log-level</code>, <code>--backend-reload</code>, <code>--backend-workers</code>, <code>--backend-worker-timeout</code>.</li> <li>UI: <code>--ui-root</code>, <code>--ui-host</code>, <code>--ui-port</code>, <code>--ui-strict-port</code>, <code>--ui-open</code>, <code>--npm-install {auto|always|never}</code>.</li> <li>Split: <code>--only-backend</code> or <code>--only-ui</code>.</li> </ul>"},{"location":"getting-started/#examples","title":"Examples","text":"<p>Backend only <pre><code>neurosurfer serve --only-backend --backend-host 0.0.0.0 --backend-port 8000\n</code></pre></p> <p>Serve your own file <pre><code>neurosurfer serve --backend-app ./app.py --backend-reload\n</code></pre></p> <p>Serve module <pre><code>neurosurfer serve --backend-app mypkg.myapp:ns\n</code></pre></p> <p>Run UI only <pre><code>neurosurfer serve --only-ui --ui-root /path/to/neurowebui\n</code></pre></p> <p>Public backend URL for UI (when binding 0.0.0.0) <pre><code>export NEUROSURF_PUBLIC_HOST=your.ip.addr\nneurosurfer serve\n</code></pre></p>"},{"location":"getting-started/#basic-usage","title":"\ud83e\udd16 Basic Usage","text":"<p>A minimal model call shows the shape of the API; agents and RAG come next. If LLM dependencies are missing, you\u2019ll get clear, actionable errors instructing you to install the extra.</p>"},{"location":"getting-started/#minimal-model-call","title":"Minimal model call","text":"<pre><code>from neurosurfer.models.chat_models.transformers import TransformersModel\n\nllm = TransformersModel(\n    model_name=\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n    max_seq_length=4096,\n    load_in_4bit=True,\n    enable_thinking=False,\n    stop_words=[],\n)\n\nresp = llm.ask(user_prompt=\"Say hi!\", system_prompt=\"You are helpful.\", stream=False)\nprint(resp.choices[0].message.content)\n</code></pre>"},{"location":"getting-started/#agents","title":"Agents","text":"<pre><code>from neurosurfer.agents import ReActAgent\nfrom neurosurfer.tools import Toolkit\nfrom neurosurfer.models.chat_models.transformers import TransformersModel\n\nllm = TransformersModel(model_name=\"meta-llama/Llama-3.2-3B-Instruct\")\nagent = ReActAgent(toolkit=Toolkit(), llm=llm)\n\nprint(agent.run(\"What is the capital of France?\"))\n</code></pre>"},{"location":"getting-started/#rag-basics","title":"RAG Basics","text":"<pre><code>from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\nfrom neurosurfer.rag.chunker import Chunker\nfrom neurosurfer.rag.filereader import FileReader\nfrom neurosurfer.server.services.rag_orchestrator import RAGOrchestrator\n\nembedder = SentenceTransformerEmbedder(model_name=\"intfloat/e5-large-v2\")\nrag = RAGOrchestrator(\n    embedder=embedder,\n    chunker=Chunker(),\n    file_reader=FileReader(),\n    persist_dir=\"./rag_store\",\n    max_context_tokens=2000,\n    top_k=15,\n    min_top_sim_default=0.35,\n    min_top_sim_when_explicit=0.15,\n    min_sim_to_keep=0.20,\n)\n\naug = rag.apply(\n    actor_id=1,\n    thread_id=\"demo\",\n    user_query=\"Summarize the attached report\",\n    files=[{\"name\":\"report.pdf\",\"content\":\"...base64...\",\"type\":\"application/pdf\"}],\n)\nprint(\"Augmented query:\", aug.augmented_query)\n</code></pre>"},{"location":"getting-started/#configuration","title":"\u2699\ufe0f Configuration","text":"<p>The full configuration guide covers API keys, model providers, server settings, logging, and environment variables. Read it here \u2192 Configuration.</p>"},{"location":"getting-started/#next-steps-help","title":"\u27a1\ufe0f Next Steps &amp; Help","text":"<p>Explore the broader documentation and examples to deepen your setup:</p> <ul> <li>API Reference \u2014 classes, methods, schemas :octicons-arrow-up-right-24</li> <li>Examples \u2014 runnable code samples :octicons-arrow-up-right-24</li> <li>CLI \u2014 command line interface :octicons-arrow-up-right-24</li> </ul> <p>Installation issues? Try the full stack extra: <pre><code>pip install -U 'neurosurfer[torch]'\n</code></pre></p> <p>GPU problems? Verify with <code>nvidia-smi</code>, confirm driver versions, and test <code>torch.cuda.is_available()</code>. For quantization on non\u2011Linux platforms, consider CPU or alternative strategies.</p> <p>Ready? Jump to  Installation or try the  CLI.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API documentation for all Neurosurfer modules and classes.</p>"},{"location":"api-reference/#quick-navigation","title":"\ud83d\udcda Quick Navigation","text":"<ul> <li> <p> Agents</p> <p>ReAct, SQL, and RAG agents for different use cases</p> <p> View agents</p> </li> <li> <p> Models</p> <p>LLM and embedding models from various providers</p> <p> View models</p> </li> <li> <p> RAG System</p> <p>Document ingestion, chunking, and retrieval</p> <p> View RAG</p> </li> <li> <p> Tools</p> <p>Built-in and custom tools for agents</p> <p> View tools</p> </li> <li> <p> Vector Stores</p> <p>Vector database integrations</p> <p> View vector stores</p> </li> <li> <p> Database</p> <p>SQL database utilities</p> <p> View database</p> </li> <li> <p> Server</p> <p>FastAPI server and API endpoints</p> <p> View server</p> </li> </ul>"},{"location":"api-reference/#module-overview","title":"\ud83d\udd0d Module Overview","text":""},{"location":"api-reference/#core-modules","title":"Core Modules","text":"Module Description Key Classes Agents AI agents for various tasks <code>ReActAgent</code>, <code>SQLAgent</code>, <code>RAGRetrieverAgent</code> Models LLM and embedding models <code>OpenAIModel</code>, <code>AnthropicModel</code>, <code>OllamaModel</code>"},{"location":"api-reference/#data-retrieval","title":"Data &amp; Retrieval","text":"Module Description Key Classes RAG Retrieval-augmented generation <code>Chunker</code>, <code>FileReader</code>, <code>RAGIngestor</code> Vector Stores Vector databases <code>ChromaVectorStore</code>, <code>BaseVectorDB</code> Database SQL database utilities <code>SQLDatabase</code>"},{"location":"api-reference/#tools-extensions","title":"Tools &amp; Extensions","text":"Module Description Key Classes Tools Agent tools and toolkit <code>BaseTool</code>, <code>Toolkit</code>, <code>ToolSpec</code> Server Production API server <code>NeurosurferApp</code>, API endpoints"},{"location":"api-reference/#common-use-cases","title":"\ud83c\udfaf Common Use Cases","text":""},{"location":"api-reference/#building-agents","title":"Building Agents","text":"<pre><code>from neurosurfer import ReActAgent\nfrom neurosurfer.models.chat_models.openai import OpenAIModel\n\nmodel = OpenAIModel(model_name=\"gpt-4\")\nagent = ReActAgent(model=model, tools=tools)\n</code></pre> <p>See: ReActAgent | Models | Tools</p>"},{"location":"api-reference/#rag-document-qa","title":"RAG Document Q&amp;A","text":"<pre><code>from neurosurfer import RAGRetrieverAgent\nfrom neurosurfer.rag import RAGIngestor\nfrom neurosurfer.vectorstores import ChromaVectorStore\n\ningestor = RAGIngestor(embedder=embedder, vectorstore=chroma)\nagent = RAGRetrieverAgent(model=model, vectorstore=chroma)\n</code></pre> <p>See: RAGIngestor | ChromaVectorStore</p>"},{"location":"api-reference/#sql-database-queries","title":"SQL Database Queries","text":"<pre><code>from neurosurfer import SQLAgent\nfrom neurosurfer.db import SQLDatabase\n\ndb = SQLDatabase(\"postgresql://...\")\nagent = SQLAgent(model=model, database=db)\n</code></pre> <p>See: SQLAgent | SQLDatabase</p>"},{"location":"api-reference/#documentation-conventions","title":"\ud83d\udcd6 Documentation Conventions","text":""},{"location":"api-reference/#type-hints","title":"Type Hints","text":"<p>All parameters and return values include type hints:</p> <pre><code>def method(param: str, count: int = 10) -&gt; List[str]:\n    ...\n</code></pre>"},{"location":"api-reference/#optional-parameters","title":"Optional Parameters","text":"<p>Parameters with defaults are marked:</p> Parameter Type Default Description <code>required_param</code> <code>str</code> required Must be provided <code>optional_param</code> <code>int</code> <code>10</code> Has default value"},{"location":"api-reference/#examples","title":"Examples","text":"<p>All methods include working examples:</p> <pre><code># Always tested and runnable\nresult = agent.run(\"example query\")\n</code></pre>"},{"location":"api-reference/#external-references","title":"\ud83d\udd17 External References","text":"<ul> <li>GitHub Repository</li> <li>PyPI Package</li> <li>Issue Tracker</li> </ul>"},{"location":"api-reference/#tips-for-using-api-docs","title":"\ud83d\udca1 Tips for Using API Docs","text":"<p>Start with Overview</p> <p>Each section has an overview page explaining concepts before diving into specific classes.</p> <p>Follow Examples</p> <p>All code examples are tested and ready to copy-paste.</p> <p>Check Related Classes</p> <p>Use \"See Also\" sections to discover related functionality.</p> <p>Ready to explore? Start with Agents \u2192 or Models \u2192</p>"},{"location":"api-reference/configuration/","title":"Configuration","text":"<p>Neurosurfer exposes a single import, <code>config</code>, that aggregates settings from environment variables (and <code>.env</code>) with safe defaults. Use these values directly, or pass them into constructors via <code>to_dict()</code> where available.</p>"},{"location":"api-reference/configuration/#app-settings","title":"App settings","text":"<p>Application/server behavior (host/port, CORS, logging) and derived paths/URLs.</p> Setting Env var Default Description <code>app_name</code> <code>APP_APP_NAME</code> <code>Neurosurfer</code> Display name for logs/metadata. <code>dev_version</code> <code>APP_DEV_VERSION</code> <code>1.0.0</code> Version shown in non\u2011prod. <code>prod_version</code> <code>APP_PROD_VERSION</code> <code>1.0.0</code> Version shown in prod. <code>description</code> <code>APP_DESCRIPTION</code> <code>Neurosurfer</code> Human\u2011readable description. <code>host_ip</code> <code>APP_HOST_IP</code> <code>0.0.0.0</code> Bind IP for the server. <code>host_port</code> <code>APP_HOST_PORT</code> <code>8081</code> Bind port for the server. <code>host_protocol</code> <code>APP_HOST_PROTOCOL</code> <code>http</code> Protocol used in <code>host_url</code>. <code>logs_level</code> <code>APP_LOGS_LEVEL</code> <code>info</code> Global logging level. <code>cors_origins</code> <code>APP_CORS_ORIGINS</code> <code>[\"*\"]</code> Allowed browser origins. <code>reload</code> <code>APP_RELOAD</code> <code>false</code> Auto\u2011reload in dev. <code>workers</code> <code>APP_WORKERS</code> <code>1</code> Uvicorn/Gunicorn workers. <code>enable_docs</code> <code>APP_ENABLE_DOCS</code> <code>true</code> Enable <code>/docs</code> UI. <code>temp_path</code> <code>APP_TEMP_PATH</code> <code>temp</code> Scratch dir for temp files. <code>logs_path</code> <code>APP_LOGS_PATH</code> <code>logs</code> Log file directory. <code>database_path</code> <code>APP_DATABASE_PATH</code> <code>./db_storage</code> Root for SQLite &amp; vectors. <code>is_docker</code> <code>APP_IS_DOCKER</code> <code>false</code> Hint to adjust runtime behavior. <code>host_url</code> (derived) \u2014 <code>http://0.0.0.0:8081</code> Computed from protocol/ip/port. <code>vector_store_path</code> (derived) \u2014 <code>./db_storage/codemind_chroma</code> Vector DB folder. <code>database_url</code> (derived) \u2014 \u2014 SQLite DSN used by server. <p>Tip</p> <p>For LAN deployments, <code>config.app.get_dynamic_host_ip()</code> tries to pick a non\u2011loopback interface IP automatically.</p>"},{"location":"api-reference/configuration/#base-model-defaults","title":"Base model defaults","text":"<p>Shared generation parameters for all chat model backends. Use <code>config.base_model.to_dict()</code> to unpack into model initializers.</p> Setting Env var Default Description <code>temperature</code> <code>TEMPERATURE</code> <code>0.7</code> Sampling temperature (0\u20112). <code>max_seq_length</code> <code>MAX_SEQ_LENGTH</code> <code>4096</code> Model context length. <code>max_new_tokens</code> <code>MAX_NEW_TOKENS</code> <code>2000</code> Max generation tokens. <code>top_k</code> <code>TOP_K</code> <code>4</code> Top\u2011k sampling. <code>load_in_4bit</code> <code>LOAD_IN_4BIT</code> <code>false</code> Enable 4\u2011bit quant. <code>enable_thinking</code> <code>ENABLE_THINKING</code> <code>false</code> Enable \u201cthinking mode\u201d. <code>stop_words</code> <code>STOP_WORDS</code> <code>null</code> Hard stop sequences. <code>system_prompt</code> <code>SYSTEM_PROMPT</code> <code>You are a helpful assistant...</code> Default system prompt. <code>verbose</code> <code>VERBOSE</code> <code>false</code> Verbose model logging."},{"location":"api-reference/configuration/#external-database-config","title":"External database config","text":"<p>Credentials for tooling/agents that talk to SQL Server (etc.).</p> Setting Env var Default Description <code>server</code> <code>DB_SERVER</code> <code>localhost</code> Hostname or IP. <code>database</code> <code>DB_DATABASE</code> <code>\"\"</code> Database/schema. <code>username</code> <code>DB_USERNAME</code> <code>\"\"</code> Login username. <code>password</code> <code>DB_PASSWORD</code> <code>\"\"</code> Login password. <code>driver</code> <code>DB_DRIVER</code> <code>ODBC Driver 17 for SQL Server</code> ODBC driver name. <code>port</code> <code>DB_PORT</code> <code>1433</code> TCP port. <p>Tip</p> <p>Keep secrets in your platform\u2019s secret manager (or a private <code>.env</code> in development). Never commit secrets.</p>"},{"location":"api-reference/configuration/#chunker-rag-config","title":"Chunker (RAG) config","text":"<p>Chunking rules for documents and code, used by the RAG pipeline.</p> Setting Env var Default Description <code>fallback_chunk_size</code> <code>CHUNKER_FALLBACK_CHUNK_SIZE</code> <code>25</code> Lines per chunk (line mode). <code>overlap_lines</code> <code>CHUNKER_OVERLAP_LINES</code> <code>3</code> Line overlap between chunks. <code>max_chunk_lines</code> <code>CHUNKER_MAX_CHUNK_LINES</code> <code>1000</code> Safety cap for line chunks. <code>comment_block_threshold</code> <code>CHUNKER_COMMENT_BLOCK_THRESHOLD</code> <code>4</code> Treat N comment lines as a block. <code>char_chunk_size</code> <code>CHUNKER_CHAR_CHUNK_SIZE</code> <code>1000</code> Chars per chunk (char mode). <code>char_overlap</code> <code>CHUNKER_CHAR_OVERLAP</code> <code>150</code> Char overlap between chunks. <code>readme_max_lines</code> <code>CHUNKER_README_MAX_LINES</code> <code>30</code> Lines per README/Markdown chunk. <code>json_chunk_size</code> <code>CHUNKER_JSON_CHUNK_SIZE</code> <code>1000</code> Chars per JSON chunk. <code>fallback_mode</code> <code>CHUNKER_FALLBACK_MODE</code> <code>char</code> <code>char</code>/<code>line</code>/<code>auto</code> strategy. <code>max_returned_chunks</code> <code>CHUNKER_MAX_RETURNED_CHUNKS</code> <code>500</code> Post\u2011sanitize chunk cap. <code>max_total_output_chars</code> <code>CHUNKER_MAX_TOTAL_OUTPUT_CHARS</code> <code>1000000</code> Post\u2011sanitize char cap. <code>min_chunk_non_ws_chars</code> <code>CHUNKER_MIN_CHUNK_NON_WS_CHARS</code> <code>1</code> Drop nearly empty chunks. <p>Note</p> <p><code>fallback_mode=\"auto\"</code> is a good default for mixed code/docs repositories.</p>"},{"location":"api-reference/configuration/#using-config-in-code","title":"Using <code>config</code> in code","text":"<p>Import once and pass settings to the pieces that need them.</p> <pre><code>from neurosurfer.config import config\nfrom neurosurfer.models.chat_models.transformers import TransformersModel\n\nlogger = config.get_logger(\"neurosurfer\")\n\n# Feed base model defaults straight into the backend\nllm = TransformersModel(\n    **{**config.base_model.to_dict(), \"model_name\": \"/models/Qwen3-4B\"},\n    logger=logger,\n)\n\nprint(config.app.host_url)          # server URL\nprint(config.app.database_url)      # SQLite DSN\nprint(config.app.vector_store_path) # vector store path\n</code></pre>"},{"location":"api-reference/configuration/#overriding-specific-values","title":"Overriding specific values","text":"<p>Option A \u2014 environment variables (preferred): <pre><code># .env\nAPP_HOST_PORT=8080\nAPP_CORS_ORIGINS=[\"https://studio.example.com\"]\nTEMPERATURE=0.6\nCHUNKER_FALLBACK_MODE=auto\n</code></pre></p> <p>Option B \u2014 in code (rare; useful for quick experiments): <pre><code>from neurosurfer.config import config\n\nconfig.app.host_port = 8080\nconfig.base_model.temperature = 0.6\nconfig.chunker.fallback_mode = \"auto\"\n</code></pre></p> <p>Tip</p> <p>The <code>config</code> initializer ensures required directories exist (<code>temp</code>, <code>logs</code>, <code>db_storage</code>, and the vector store path), so clean installs work out of the box.</p>"},{"location":"api-reference/agents/","title":"Agents API","text":"<p>AI agents that can reason, use tools, and solve complex tasks\u2014from database analytics to document Q&amp;A to general multi-step workflows.</p>"},{"location":"api-reference/agents/#available-agents","title":"\ud83d\udcda Available Agents","text":"<ul> <li> <p> ReActAgent</p> <p>General-purpose agent using the ReAct (Reasoning + Acting) loop with robust tool routing, validation, and self-repair.</p> <p> Documentation</p> </li> <li> <p> SQLAgent</p> <p>Domain-tuned ReAct agent for SQL: discover schema, generate queries, execute safely, and explain results.</p> <p> Documentation</p> </li> <li> <p> RAGRetrieverAgent</p> <p>Retrieval core for document Q&amp;A: fetch context from a vector store and return safe <code>max_new_tokens</code> budgeting.</p> <p> Documentation</p> </li> </ul>"},{"location":"api-reference/agents/#which-agent-should-i-use","title":"\ud83e\udd14 Which Agent Should I Use?","text":""},{"location":"api-reference/agents/#use-reactagent-when","title":"Use ReActAgent When\u2026","text":"<ul> <li>You need flexible multi-step reasoning and tool use</li> <li>Tasks span file operations, search, code generation, lint/test, and similar workflows</li> <li>You want self-repair on parse/tool errors and input pruning</li> </ul> <p>Typical use cases: research assistant, support bots, automation flows View ReActAgent Documentation \u2192</p>"},{"location":"api-reference/agents/#use-sqlagent-when","title":"Use SQLAgent When\u2026","text":"<ul> <li>You want natural language to SQL with schema discovery and safe execution</li> <li>You need analytics, reporting, and BI dashboards</li> <li>You value clear, natural-language explanations of results</li> </ul> <p>Typical use cases: ad-hoc analytics, KPI reports, data exploration View SQLAgent Documentation \u2192</p>"},{"location":"api-reference/agents/#use-ragagent-when","title":"Use RAGAgent When\u2026","text":"<ul> <li>You have a document knowledge base in a vector store</li> <li>You need a retrieval core that returns prompts and safe budgets</li> <li>You want to plug into any LLM for final generation</li> </ul> <p>Typical use cases: doc Q&amp;A, handbook/KB assistants, code search helpers View RAGAgent Documentation \u2192</p>"},{"location":"api-reference/agents/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"api-reference/agents/#react-agent-with-a-toolkit","title":"ReAct agent with a Toolkit","text":"<pre><code>from neurosurfer.agents.react import ReActAgent, ReActConfig\nfrom neurosurfer.models.chat_models.openai import OpenAIModel\nfrom neurosurfer.tools import Toolkit\nfrom neurosurfer.tools.common.general_query_assistant import GeneralQueryAssistantTool\n\nllm = OpenAIModel(model_name=\"gpt-4o-mini\")\ntoolkit = Toolkit()\ntoolkit.register_tool(GeneralQueryAssistantTool(llm=llm))\n\nagent = ReActAgent(toolkit=toolkit, llm=llm, config=ReActConfig(repair_with_llm=True))\n\nfor chunk in agent.run(\"What is 15% of 250?\"):\n    print(chunk, end=\"\")\n</code></pre>"},{"location":"api-reference/agents/#sql-agent-streaming","title":"SQL agent streaming","text":"<pre><code>from neurosurfer.agents.sql_agent import SQLAgent\nfrom neurosurfer.models.chat_models.openai import OpenAIModel\n\nllm = OpenAIModel(model_name=\"gpt-4o-mini\")\nsql_agent = SQLAgent(\n    llm=llm,\n    db_uri=\"sqlite:///examples/chinook.db\",\n    sample_rows_in_table_info=5,\n)\n\ntranscript = []\nfor chunk in sql_agent.run(\"List the top 5 artists by total sales.\"):\n    transcript.append(chunk)\n\n# If your UI doesn\u2019t suppress special tokens, strip final-answer markers:\nanswer = \"\".join(transcript).split(\"&lt;__final_answer__&gt;\")[-1].split(\"&lt;/__final_answer__&gt;\")[0].strip()\nprint(answer)\n</code></pre>"},{"location":"api-reference/agents/#retrieval-only-pipeline","title":"Retrieval-only pipeline","text":"<pre><code>from neurosurfer.agents.rag import RAGAgent, RAGAgentConfig\nfrom neurosurfer.models.chat_models.openai import OpenAIModel\n\nllm = OpenAIModel(model_name=\"gpt-4o-mini\")\nrag = RAGAgent(llm=llm, vectorstore=chroma_store, embedder=embedder,\n                        config=RAGAgentConfig(top_k=8, normalize_embeddings=True))\n\nresult = rag.retrieve(\n    user_query=\"Summarize the onboarding guide.\",\n    base_system_prompt=\"You are a helpful assistant.\",\n    base_user_prompt=\"{context}\\n\\nQuestion: {query}\",\n)\n\nresponse = llm.ask(\n    system_prompt=result.base_system_prompt,\n    user_prompt=result.base_user_prompt.format(\n        context=result.context,\n        query=\"Summarize the onboarding guide.\",\n    ),\n    max_new_tokens=result.max_new_tokens,\n)\n</code></pre> <p>Note on markers: ReAct/SQL stream final answers wrapped in <code>&lt;__final_answer__&gt;\u2026&lt;/__final_answer__&gt;</code>. Set <code>ReActConfig(skip_special_tokens=True)</code> if your UI handles finalization without markers.</p>"},{"location":"api-reference/agents/#tips-best-practices","title":"\ud83d\udd27 Tips &amp; Best Practices","text":"<ul> <li>Describe tools well: Rich <code>ToolSpec</code> metadata (inputs/returns, when-to-use) improves tool routing and reduces retries.  </li> <li>Self-repair &amp; pruning: Enable <code>repair_with_llm=True</code> and/or <code>allow_input_pruning=True</code> in <code>ReActConfig</code> to make agents resilient to malformed Actions.  </li> <li>Cache SQL schemas: Pre-warm with <code>SQLAgent.train()</code> at startup for low-latency first answers.  </li> <li>Respect token budgets: Use <code>RAGRetrieverAgent</code>\u2019s <code>max_new_tokens</code> and <code>generation_budget</code> to avoid truncation.  </li> <li>Pass runtime context: Extra kwargs to <code>agent.run(...)</code> get merged into tool calls\u2014inject DB handles, feature flags, etc., without exposing them to the LLM (via <code>ToolResponse.extras</code>).</li> </ul>"},{"location":"api-reference/agents/#learn-more","title":"\ud83d\udcda Learn More","text":"<ul> <li>ReActAgent \u2014 Full reasoning loop with tools  </li> <li>SQLAgent \u2014 ReAct agent tailored for databases  </li> <li>RAGRetrieverAgent \u2014 Retrieval-only building block  </li> <li>Tools API \u2014 Build and document custom tools  </li> <li>Models API \u2014 Chat and embedding backends  </li> <li>Examples \u2014 End-to-end notebooks and scripts</li> </ul> <p>Ready to dive in? Start with ReActAgent \u2192</p>"},{"location":"api-reference/agents/rag-agent/","title":"RAGAgent","text":"<p>Module: <code>neurosurfer.agents.rag</code></p>"},{"location":"api-reference/agents/rag-agent/#overview","title":"Overview","text":"<p><code>RAGAgent</code> is a lightweight, modular retrieval core for Retrieval\u2011Augmented Generation (RAG). It is:</p> <ul> <li>Vector\u2011store agnostic \u2014 plugs into any store implementing the <code>BaseVectorDB</code> interface.  </li> <li>Embedder agnostic \u2014 works with any model implementing <code>BaseEmbedder</code>.  </li> <li>LLM/tokenizer agnostic \u2014 uses HuggingFace tokenizers when available, falls back to <code>tiktoken</code> if present, and otherwise applies a robust character\u2011based heuristic to keep prompts under the model\u2019s context window.</li> </ul> <p>It performs three primary steps:</p> <p>1) Retrieve: embed the query and fetch the top\u2011K most similar chunks. 2) Build context: convert retrieved docs into a joined context string (customizable). 3) Budget &amp; trim: fit the context into the LLM window while leaving room for generation; return a <code>RetrieveResult</code> with a safe <code>max_new_tokens</code> value.</p> <p>Optionally, you can call <code>run(...)</code> to perform a full generation (retrieve \u2192 fill prompt \u2192 <code>llm.ask(...)</code>, streaming or not).</p>"},{"location":"api-reference/agents/rag-agent/#package-layout","title":"Package layout","text":"<pre><code>neurosurfer/agents/rag/\n\u251c\u2500 __init__.py                 # exports RAGAgent, RAGAgentConfig, RetrieveResult\n\u251c\u2500 config.py                   # dataclasses for config and results\n\u251c\u2500 token_utils.py              # tokenizer/tokens fallback + trimming utilities\n\u251c\u2500 context_builder.py          # format &amp; join retrieved chunks\n\u251c\u2500 picker.py                   # helper to pick top files by grouped hits\n\u2514\u2500 agent.py                    # main agent implementation\n</code></pre>"},{"location":"api-reference/agents/rag-agent/#constructor","title":"Constructor","text":"<pre><code>RAGAgent(\n    llm: BaseModel,\n    vectorstore: BaseVectorDB,\n    embedder: BaseEmbedder,\n    *,\n    config: RAGAgentConfig | None = None,\n    logger: logging.Logger | None = None,\n    make_source: Callable[[Doc], str] | None = None,\n)\n</code></pre> Parameter Type Description <code>llm</code> <code>BaseModel</code> Any supported chat model. Must expose <code>ask(...)</code> and (ideally) <code>max_seq_length</code>. A tokenizer is optional. <code>vectorstore</code> <code>BaseVectorDB</code> Must expose <code>similarity_search(query_embedding, top_k, metadata_filter, similarity_threshold)</code>. <code>embedder</code> <code>BaseEmbedder</code> Must expose <code>embed(sequence[str], normalize_embeddings=bool) -&gt; list[list[float]]</code>. <code>config</code> <code>RAGAgentConfig</code> | <code>None</code> Retrieval, context\u2011formatting, and budgeting knobs (defaults used when <code>None</code>). <code>logger</code> <code>logging.Logger \\| None</code> Optional logger. <code>make_source</code> <code>Callable[[Doc], str] \\| None</code> Override how each doc\u2019s \u201csource\u201d label is rendered in context (filename, URI, etc.)."},{"location":"api-reference/agents/rag-agent/#ragagentconfig","title":"<code>RAGAgentConfig</code>","text":"<pre><code>@dataclass\nclass RAGAgentConfig:\n    # Retrieval\n    top_k: int = 5\n    similarity_threshold: float | None = None\n\n    # Output budgeting\n    fixed_max_new_tokens: int | None = None\n    auto_output_ratio: float = 0.25\n    min_output_tokens: int = 32\n    safety_margin_tokens: int = 32\n\n    # Context formatting\n    include_metadata_in_context: bool = True\n    context_separator: str = \"\\n\\n---\\n\\n\"\n    context_item_header_fmt: str = \"Source: {source}\"\n    normalize_embeddings: bool = True\n\n    # Tokenizer fallbacks\n    approx_chars_per_token: float = 4.0\n</code></pre>"},{"location":"api-reference/agents/rag-agent/#parameters","title":"Parameters","text":"Parameter Type Description <code>top_k</code> <code>int</code> Default number of chunks to fetch when <code>top_k</code> is not supplied to <code>retrieve(...)</code>. <code>similarity_threshold</code> <code>float \\| None</code> Global similarity floor for retrieval (override per call). <code>fixed_max_new_tokens</code> <code>int \\| None</code> Hard cap for generation. If <code>None</code>, the agent derives a cap dynamically after trimming. <code>auto_output_ratio</code> <code>float</code> When no fixed cap is provided, initial portion of remaining window reserved for output before trimming (refined after trimming). <code>min_output_tokens</code> <code>int</code> Guarantee at least this many tokens remain for generation. <code>safety_margin_tokens</code> <code>int</code> Context margin to avoid overrunning model window due to tokenizer variance. <code>include_metadata_in_context</code> <code>bool</code> When <code>True</code>, prefixes each chunk with <code>context_item_header_fmt.format(source=...)</code>. <code>context_separator</code> <code>str</code> Separator between chunks in the final context string. <code>context_item_header_fmt</code> <code>str</code> Format string for per\u2011chunk header line when metadata is included. <code>normalize_embeddings</code> <code>bool</code> Passed to <code>embedder.embed(...)</code>; set <code>False</code> if your embedder already returns normalized vectors. <code>approx_chars_per_token</code> <code>float</code> Heuristic used when the model has no tokenizer and <code>tiktoken</code> is unavailable. ~4 chars/token is a practical default."},{"location":"api-reference/agents/rag-agent/#results-object","title":"Results object","text":""},{"location":"api-reference/agents/rag-agent/#retrieveresult","title":"<code>RetrieveResult</code>","text":"<p>Returned by <code>retrieve(...)</code> and used by <code>run(...)</code> to fill prompts and set generation limits safely.</p> Field Type Description <code>base_system_prompt</code> <code>str</code> System prompt provided to <code>retrieve(...)</code> <code>base_user_prompt</code> <code>str</code> User prompt template (before inserting context). <code>context</code> <code>str</code> Trimmed context actually used. <code>max_new_tokens</code> <code>int</code> Recommended cap for generation after trimming. <code>base_tokens</code> <code>int</code> Tokens for system + history + user (no context yet). <code>context_tokens_used</code> <code>int</code> Tokens consumed by the trimmed context. <code>token_budget</code> <code>int</code> Model\u2019s total context window (<code>llm.max_seq_length</code> or default). <code>generation_budget</code> <code>int</code> Remaining tokens for output. <code>docs</code> <code>list[Doc]</code> Retrieved docs from the vector store. <code>distances</code> <code>list[float \\| None]</code> One per doc when the vector store returns distances. <code>meta</code> <code>dict</code> Diagnostics (available_for_context, initial cap, margins, etc.)."},{"location":"api-reference/agents/rag-agent/#methods","title":"Methods","text":""},{"location":"api-reference/agents/rag-agent/#retrieve-retrieveresult","title":"<code>retrieve(...) -&gt; RetrieveResult</code>","text":"<pre><code>result = agent.retrieve(\n    user_query=\"Explain vector databases in simple terms.\",\n    base_system_prompt=\"You are a helpful assistant.\",\n    base_user_prompt=\"Use the context to answer.\\n\\n{context}\\n\\nQuestion: {query}\",\n    chat_history=[{\"role\": \"user\", \"content\": \"Hi!\"}],\n    top_k=8,\n    metadata_filter={\"collection\": \"docs\"},\n)\n</code></pre> <p>You can then use the result to build your own LLM call:</p> <pre><code>filled = result.base_user_prompt.format(context=result.context, query=\"Explain vector databases in simple terms.\")\nresponse = llm.ask(\n    system_prompt=result.base_system_prompt,\n    user_prompt=filled,\n    temperature=0.3,\n    max_new_tokens=result.max_new_tokens,\n)\n</code></pre>"},{"location":"api-reference/agents/rag-agent/#run-iteratorstr-str","title":"<code>run(...) -&gt; Iterator[str] | str</code>","text":"<p>Runs retrieve \u2192 fill \u2192 generate for you. When <code>stream=True</code>, it yields generation chunks; otherwise it returns the full string.</p> <pre><code>for token in agent.run(\n    \"List the main components of a RAG system.\",\n    base_system_prompt=\"You are concise.\",\n    base_user_prompt=\"Context:\\n{context}\\n\\nQ: {query}\\nA:\",\n    stream=True,\n    temperature=0.2,\n):\n    print(token, end=\"\")\n</code></pre> <p>Arguments of note (subset):</p> Arg Type Description <code>stream</code> <code>bool</code> If <code>True</code>, yields streaming tokens from <code>llm.ask(..., stream=True)</code>. <code>top_k</code> <code>int \\| None</code> Overrides <code>config.top_k</code> for this call. <code>metadata_filter</code> <code>dict \\| None</code> Forwarded to the vector store. <code>similarity_threshold</code> <code>float \\| None</code> Per\u2011call similarity floor. <code>temperature</code> <code>float \\| None</code> Per\u2011call generation temperature (defaults to 0.3). <code>max_new_tokens</code> <code>int \\| None</code> Per\u2011call cap; if omitted, uses <code>RetrieveResult.max_new_tokens</code>. <code>**llm_kwargs</code> <code>Any</code> Forwarded to <code>llm.ask(...)</code> (e.g., stop sequences)."},{"location":"api-reference/agents/rag-agent/#token-handling-fallbacks","title":"Token handling &amp; fallbacks","text":"<p><code>TokenCounter</code> ensures prompts stay within the model window even without a tokenizer:</p> <ol> <li>HuggingFace path \u2014 if <code>llm.tokenizer</code> exists, we use it (<code>apply_chat_template</code>, fast counting, and exact trimming).  </li> <li>tiktoken path \u2014 if no tokenizer is available but <code>tiktoken</code> is installed, we use <code>cl100k_base</code> for robust counting &amp; trimming.  </li> <li>Heuristic path \u2014 otherwise, we estimate tokens via <code>len(text) / approx_chars_per_token</code> and binary\u2011search on character length to trim precisely enough.</li> </ol> <p>This design guarantees safe budgeting across OpenAI\u2011style clients, HF models, and custom LLM wrappers.</p>"},{"location":"api-reference/agents/rag-agent/#context-formatting","title":"Context formatting","text":"<p>Context serialization is handled by <code>ContextBuilder</code>. By default it:</p> <ul> <li>Adds a header line like <code>Source: &lt;label&gt;</code> (when <code>include_metadata_in_context=True</code>), where <code>&lt;label&gt;</code> is produced by <code>make_source(doc)</code>.  </li> <li>Joins chunks with <code>context_separator</code> (default: <code>\\n\\n---\\n\\n</code>).</li> </ul> <p>You can override <code>make_source</code> in the agent constructor or swap the builder if you need a different format (citations, bullet lists, etc.).</p>"},{"location":"api-reference/agents/rag-agent/#file-picking-helper","title":"File picking helper","text":"<p>When your index spans many files (e.g., a codebase), use the helper in <code>picker.py</code> to find the most promising files for deeper focus:</p> <pre><code>from neurosurfer.agents.rag.picker import pick_files_by_grouped_chunk_hits\n\nfiles = pick_files_by_grouped_chunk_hits(\n    embedder=embedder,\n    vector_db=vectorstore,\n    section_query=\"vector similarity threshold\",\n    candidate_pool_size=200,\n    n_files=5,\n    file_key=\"filename\",\n)\n</code></pre> <p>This performs a wide similarity search, aggregates scores per file, and returns the top\u2011N paths.</p>"},{"location":"api-reference/agents/rag-agent/#best-practices","title":"Best practices","text":"<ul> <li>Keep prompts short: Your <code>base_user_prompt</code> is applied before context insertion. Excess preamble reduces space for retrieved evidence.  </li> <li>Prefer normalized embeddings: Set <code>normalize_embeddings=True</code> unless your embedder already normalizes vectors.  </li> <li>Use <code>fixed_max_new_tokens</code> when needed: For deterministic generations (e.g., latency budgeting), set an explicit cap.  </li> <li>Log budgets: <code>RetrieveResult.meta</code> and <code>generation_budget</code> make it easy to visualize headroom and trim behavior.  </li> <li>Multi\u2011stage retrieval: You can run <code>retrieve(...)</code> multiple times (e.g., coarse \u2192 re\u2011rank) and combine contexts before calling <code>run(...)</code> yourself.</li> </ul>"},{"location":"api-reference/agents/react-agent/","title":"ReActAgent","text":"<p>Module: <code>neurosurfer.agents.react</code></p>"},{"location":"api-reference/agents/react-agent/#overview","title":"Overview","text":"<p><code>ReActAgent</code> implements the ReAct (Reasoning + Acting) loop for complex, tool\u2011using tasks. It streams its reasoning, calls exactly one tool per step, observes results, and either continues iterating or emits a final answer. It is domain\u2011agnostic and can be used for anything: coding assistants, database agents, file managers, research helpers, etc.</p> <p>Key capabilities:</p> <ol> <li>Robust Action parsing \u2014 tolerant JSON extraction from LLM output (handles code fences, trailing commas, partial blocks).</li> <li>Schema\u2011aware input validation \u2014 inputs are validated against each tool\u2019s <code>ToolSpec</code>; optional input pruning safely drops unknown keys.</li> <li>Self\u2011repair \u2014 when parsing or tool calls fail, the agent asks the LLM to repair the Action, with bounded retries.</li> <li>Streaming \u2014 thoughts and final answers are streamed; tool outputs can stream too. Delimiter markers can be suppressed (see <code>skip_special_tokens</code>).</li> <li>Reusable core \u2014 clean config (<code>ReActConfig</code>), retry policy (<code>RetryPolicy</code>), ephemeral memory, and toolkit wiring.</li> </ol> <p><code>ReActAgent</code> is designed to be subclassed for specialized agents (e.g., <code>SQLAgent</code>) while keeping shared behavior in the core.</p>"},{"location":"api-reference/agents/react-agent/#constructor","title":"Constructor","text":"<pre><code>ReActAgent(\n    toolkit: Toolkit,\n    llm: BaseModel,\n    *,\n    logger: logging.Logger | None = None,\n    specific_instructions: str = \"\",\n    config: ReActConfig | None = None,\n)\n</code></pre> Parameter Type Description <code>toolkit</code> <code>Toolkit</code> Registry of tools available to the agent. The agent will render descriptions from the toolkit into its system prompt. <code>llm</code> <code>BaseModel</code> Any supported chat model (OpenAI\u2011style, Transformers/Unsloth, Llama.cpp, vLLM, etc.). Must implement <code>ask(...)</code> and <code>stop_generation()</code>. <code>logger</code> <code>logging.Logger \\| None</code> Optional logger; defaults to module logger. <code>specific_instructions</code> <code>str</code> Extra system prompt addendum to steer behavior for a domain (e.g., SQL policy). <code>config</code> <code>ReActConfig</code> | <code>None</code> Advanced configuration (temperature, retries, pruning, streaming markers, etc.). If <code>None</code>, defaults are used."},{"location":"api-reference/agents/react-agent/#reactconfig","title":"<code>ReActConfig</code>","text":"<pre><code>from dataclasses import dataclass, field\nfrom neurosurfer.agents.react import RetryPolicy\n\n@dataclass\nclass ReActConfig:\n    temperature: float = 0.7\n    max_new_tokens: int = 8000\n    verbose: bool = True\n    allow_input_pruning: bool = True      # drop extra inputs not in ToolSpec\n    repair_with_llm: bool = True          # ask LLM to repair invalid Actions\n    skip_special_tokens: bool = False     # when True, suppresses &lt;__final_answer__&gt; ... markers\n    retry: RetryPolicy = field(default_factory=RetryPolicy)\n</code></pre>"},{"location":"api-reference/agents/react-agent/#reactconfig-parameters","title":"ReActConfig parameters","text":"Parameter Type Description <code>temperature</code> <code>float</code> Default sampling temperature for LLM calls made by the agent loop. Overridable per <code>run(...)</code> call. <code>max_new_tokens</code> <code>int</code> Default token cap for LLM generations inside the agent. Overridable per <code>run(...)</code> call. <code>verbose</code> <code>bool</code> When <code>True</code>, prints additional debug info (e.g., observations) via <code>rich</code>/logger. <code>allow_input_pruning</code> <code>bool</code> If <code>True</code>, unknown keys in Action <code>inputs</code> are dropped before <code>ToolSpec</code> validation. If <code>False</code>, the agent attempts to repair the Action instead. <code>repair_with_llm</code> <code>bool</code> If <code>True</code>, the agent prompts the LLM to output a corrected Action when parsing/validation fails or a tool errors. <code>skip_special_tokens</code> <code>bool</code> If <code>True</code>, the agent does not emit <code>&lt;__final_answer__&gt;</code> / <code>&lt;/__final_answer__&gt;</code> markers during streaming. Use this when your UI handles finalization itself. <code>retry</code> <code>RetryPolicy</code> Controls retries for Action parsing and tool failures (counts and backoff)."},{"location":"api-reference/agents/react-agent/#retrypolicy","title":"<code>RetryPolicy</code>","text":"<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass RetryPolicy:\n    max_parse_retries: int = 2   # attempts to repair missing/invalid Action\n    max_tool_errors: int = 2     # attempts to repair &amp; re-run a failing tool\n    backoff_sec: float = 0.8     # linear backoff per retry\n</code></pre>"},{"location":"api-reference/agents/react-agent/#retrypolicy-parameters","title":"RetryPolicy parameters","text":"Parameter Type Description <code>max_parse_retries</code> <code>int</code> Maximum number of times the agent will attempt to repair and regenerate an Action when none is found or JSON is invalid. <code>max_tool_errors</code> <code>int</code> Maximum number of tool execution retries after using error feedback to repair the Action/inputs. <code>backoff_sec</code> <code>float</code> Base number of seconds to wait between retries (linear backoff: multiplied by attempt index)."},{"location":"api-reference/agents/react-agent/#response-action-format","title":"Response &amp; Action Format","text":""},{"location":"api-reference/agents/react-agent/#reasoning-final-answer","title":"Reasoning &amp; Final Answer","text":"<p>The agent streams content. When the final answer begins, it typically emits markers:</p> <pre><code>Thought: ...\n&lt;__final_answer__&gt;Final Answer: ...&lt;/__final_answer__&gt;\n</code></pre> <ul> <li>If <code>config.skip_special_tokens=True</code>, these markers are suppressed and only the final text streams.</li> </ul>"},{"location":"api-reference/agents/react-agent/#tool-call-action-format","title":"Tool Call (Action) Format","text":"<p>The LLM must end a step with a JSON Action (no prose after it):</p> <pre><code>Action: {\n  \"tool\": \"tool_name\",\n  \"inputs\": { \"param\": \"value\" },\n  \"final_answer\": false\n}\n</code></pre> <ul> <li><code>tool</code> \u2014 name registered in <code>Toolkit</code>.</li> <li><code>inputs</code> \u2014 must match the tool\u2019s <code>ToolSpec</code> (unknown keys are dropped when <code>allow_input_pruning=True</code>).</li> <li><code>final_answer</code> \u2014 see ToolResponse.final_answer below (this flag expresses the intent; the actual stop condition is ultimately governed by the tool\u2019s returned <code>ToolResponse</code>).</li> </ul> <p>If the Action is missing/invalid, or a tool call fails, the agent will attempt self\u2011repair using history + error messages, up to the configured retry limits.</p>"},{"location":"api-reference/agents/react-agent/#main-methods","title":"Main Methods","text":""},{"location":"api-reference/agents/react-agent/#runuser_query-str-kwargs-generatorstr-none-str","title":"<code>run(user_query: str, **kwargs) -&gt; Generator[str, None, str]</code>","text":"<p>Runs the ReAct loop and streams text chunks (thoughts, tool IO, final answer). The generator\u2019s return value is the final answer string.</p> <pre><code>for chunk in agent.run(\"Summarize the latest design decisions.\", temperature=0.3, max_new_tokens=2000):\n    print(chunk, end=\"\")\n</code></pre> <p>Common kwargs (override config per call):</p> Kwarg Type Description <code>temperature</code> <code>float</code> Sampling temperature for this run (defaults to <code>config.temperature</code>). <code>max_new_tokens</code> <code>int</code> Token cap for this run (defaults to <code>config.max_new_tokens</code>)."},{"location":"api-reference/agents/react-agent/#stop_generation-none","title":"<code>stop_generation() -&gt; None</code>","text":"<p>Signals both the underlying <code>llm</code> and the agent loop to stop as soon as possible (useful for UI Stop buttons).</p> <pre><code>agent.stop_generation()\n</code></pre>"},{"location":"api-reference/agents/react-agent/#update_toolkittoolkit-toolkit-none","title":"<code>update_toolkit(toolkit: Toolkit) -&gt; None</code>","text":"<p>Swap the toolkit at runtime; useful when dynamically adding tools.</p> <pre><code>tk.register_tool(MyNewTool(...))\nagent.update_toolkit(tk)\n</code></pre>"},{"location":"api-reference/agents/react-agent/#tooling-contract","title":"Tooling Contract","text":"<p>Each tool should subclass <code>BaseTool</code> and define a <code>ToolSpec</code>:</p> <pre><code>class MyTool(BaseTool):\n    name = \"my_tool\"\n    description = \"One-line purpose for the LLM.\"\n    spec = ToolSpec(\n        name=name,\n        description=description,\n        inputs=[\n            ToolParam(name=\"path\", ptype=str, required=True, description=\"File path\"),\n            ToolParam(name=\"flag\", ptype=bool, required=False, description=\"Optional flag\"),\n        ],\n        returns=ToolReturn(rtype=str, description=\"Human-readable observation\")\n    )\n\n    def __call__(self, **kwargs) -&gt; ToolResponse:\n        params = self.spec.check_inputs(kwargs)\n        # ... do work ...\n        return ToolResponse(\n            observation=\"Done.\",\n            final_answer=False,\n            extras={\"some_key\": \"value\"}  # becomes ephemeral memory for the next step\n        )\n</code></pre>"},{"location":"api-reference/agents/react-agent/#toolresponsefinal_answer","title":"<code>ToolResponse.final_answer</code>","text":"<ul> <li>Tools return a <code>ToolResponse</code> with <code>final_answer: bool</code>.  </li> <li>When <code>final_answer=True</code>, the agent treats the tool\u2019s observation as the final user\u2011facing answer and stops the loop immediately.  </li> <li>When streaming, the agent typically wraps the final text with <code>&lt;__final_answer__&gt; ... &lt;/__final_answer__&gt;</code> markers; if <code>config.skip_special_tokens=True</code>, it does not emit markers and just streams the text.  </li> <li>Most tools should return <code>final_answer=False</code>. Only mark as final when the tool\u2019s output is already the complete answer the user should see.</li> </ul>"},{"location":"api-reference/agents/react-agent/#passing-extras-between-tools-llminvisible-memory","title":"Passing <code>extras</code> between tools (LLM\u2011invisible memory)","text":"<ul> <li><code>ToolResponse.extras</code> is a dictionary carried by the agent\u2019s ephemeral memory into the very next tool call automatically.  </li> <li>This pass\u2011through does not go through the LLM; it is agent\u2011side only.  </li> <li>You can place non\u2011serializable / rich Python objects in <code>extras</code> (DB connections, compiled regexes, parsed ASTs, pandas objects, etc.) to avoid lossy stringification.  </li> <li>After a tool call completes, the agent injects <code>extras</code> into the input of the following tool (merged with that tool\u2019s Action <code>inputs</code>).  </li> <li>Memory is cleared after each tool call to avoid accidental long\u2011term accumulation. Persist long\u2011term state in your own services or stores if needed.</li> </ul>"},{"location":"api-reference/agents/react-agent/#error-handling-selfrepair","title":"Error Handling &amp; Self\u2011Repair","text":"<ul> <li>Missing/invalid Action \u2192 The agent prompts the LLM to repair the Action (bounded by <code>max_parse_retries</code>).  </li> <li>Tool validation errors (unknown keys, missing requireds) \u2192 Either drop extras (<code>allow_input_pruning=True</code>) or ask the LLM to repair the inputs.  </li> <li>Tool runtime errors \u2192 The agent returns the error text to the LLM and retries with a repaired Action up to <code>max_tool_errors</code>, with backoff.</li> </ul> <p>When retries are exhausted, the agent surfaces the failure as an observation and may still produce a final answer if appropriate.</p>"},{"location":"api-reference/agents/react-agent/#streaming-notes","title":"Streaming Notes","text":"<ul> <li>The agent streams model thoughts and final answers.  </li> <li>Tools may return strings or generators of strings; tool generators are proxied to the caller for live output.  </li> <li>Final\u2011answer markers are emitted unless <code>skip_special_tokens=True</code> (in which case only the raw text streams).</li> </ul>"},{"location":"api-reference/agents/sql-agent/","title":"SQLAgent","text":"<p>Module: <code>neurosurfer.agents.sql_agent</code></p>"},{"location":"api-reference/agents/sql-agent/#overview","title":"Overview","text":"<p><code>SQLAgent</code> specializes the <code>ReActAgent</code> for relational databases. It boots with a SQL-focused <code>Toolkit</code> that knows how to:</p> <ol> <li>Summarize table schemas and cache them locally (<code>SQLSchemaStore</code>)</li> <li>Pick the most relevant tables for a question</li> <li>Generate a SQL statement</li> <li>Execute the statement safely via SQLAlchemy</li> <li>Format the result into natural language</li> </ol> <p>Every step is streamed like the base ReAct agent (thoughts \u2192 action \u2192 observation \u2192 \u2026 \u2192 final). If your app suppresses special tokens, set <code>skip_special_tokens=True</code> in <code>ReActConfig</code> when constructing the agent (see below).</p>"},{"location":"api-reference/agents/sql-agent/#constructor","title":"Constructor","text":""},{"location":"api-reference/agents/sql-agent/#sqlagent__init__","title":"<code>SQLAgent.__init__</code>","text":"<pre><code>SQLAgent(\n    llm: BaseModel,\n    db_uri: str,\n    *,\n    storage_path: str | None = None,\n    sample_rows_in_table_info: int = 3,\n    logger: logging.Logger = logging.getLogger(__name__),\n    verbose: bool = True,\n    config: ReActConfig | None = None,\n    specific_instructions: str | None = None,\n)\n</code></pre> Parameter Type Description <code>llm</code> <code>BaseModel</code> Model used for reasoning and SQL generation. <code>db_uri</code> <code>str</code> SQLAlchemy connection string (e.g. <code>postgresql://user:pass@host/db</code>). <code>storage_path</code> <code>str \\| None</code> Optional location for persisting schema summaries. Defaults to the <code>SQLSchemaStore</code> default under the working directory. <code>sample_rows_in_table_info</code> <code>int</code> Number of example rows to capture when summarizing table schema. <code>logger</code> <code>logging.Logger</code> Logger for status messages. <code>verbose</code> <code>bool</code> When <code>True</code>, prints tool calls and observations as they happen. <code>config</code> <code>ReActConfig</code> | <code>None</code> Advanced configuration (retries, pruning, streaming markers, etc.). Defaults are used when <code>None</code>. <code>specific_instructions</code> <code>str \\| None</code> Optional SQL-specific system addendum. If <code>None</code>, sensible SQL policies are used (discover \u2192 generate \u2192 execute \u2192 format). <p>During initialization the agent:</p> <ul> <li>Creates a SQLAlchemy engine via <code>create_engine(db_uri)</code></li> <li>Instantiates <code>SQLSchemaStore</code> (handles schema discovery + caching)</li> <li>Registers the following tools in its toolkit:</li> <li><code>RelevantTableSchemaFinderLLM</code> (table selection + schema fetch)</li> <li><code>SQLQueryGenerator</code> (LLM prompt \u2192 SQL string)</li> <li><code>SQLExecutor</code> (read-only execution via the engine)</li> <li><code>FinalAnswerFormatter</code> (transforms rows to natural language)</li> <li><code>DBInsightsTool</code> (high\u2011level database summaries and health checks)</li> </ul> <p>Note: If your docs use a different path layout, adjust the links above to match your structure.</p>"},{"location":"api-reference/agents/sql-agent/#usage","title":"Usage","text":"<pre><code>from neurosurfer.agents.sql_agent import SQLAgent\nfrom neurosurfer.agents.react import ReActConfig\nfrom neurosurfer.models.chat_models.openai import OpenAIModel\n\nllm = OpenAIModel(model_name=\"gpt-4o-mini\")\n\nagent = SQLAgent(\n    llm=llm,\n    db_uri=\"sqlite:///examples/chinook.db\",\n    sample_rows_in_table_info=5,\n    verbose=True,\n    config=ReActConfig(skip_special_tokens=False, allow_input_pruning=True, repair_with_llm=True),\n)\n\nfor chunk in agent.run(\"List the top 5 artists by total sales.\"):\n    print(chunk, end=\"\")  # Streams thoughts + tool observations + final answer markers\n</code></pre> <p>If you set <code>skip_special_tokens=True</code> in <code>ReActConfig</code>, the agent will not emit <code>&lt;__final_answer__&gt;</code> markers; only the raw text streams. This is useful if your UI has its own finalization logic.</p>"},{"location":"api-reference/agents/sql-agent/#methods","title":"Methods","text":""},{"location":"api-reference/agents/sql-agent/#runuser_query-str-kwargs-generatorstr-none-str","title":"<code>run(user_query: str, **kwargs) -&gt; Generator[str, None, str]</code>","text":"<p>Delegates to <code>ReActAgent.run</code>. Pass generation kwargs such as <code>temperature</code> or <code>max_new_tokens</code>. The generator yields formatted strings (thoughts, actions, tool observations) and finally returns the final answer string.</p> <p>Runtime context (for example injecting a runtime filter) can be supplied via keyword arguments; they are forwarded to every tool invocation.</p>"},{"location":"api-reference/agents/sql-agent/#register_tooltool-basetool-none","title":"<code>register_tool(tool: BaseTool) -&gt; None</code>","text":"<p>Adds a custom tool to the underlying toolkit and immediately updates the agent to include it in subsequent runs.</p> <pre><code>agent.register_tool(MyAggregationTool(db_engine=agent.db_engine))\n</code></pre>"},{"location":"api-reference/agents/sql-agent/#get_toolkit-toolkit","title":"<code>get_toolkit() -&gt; Toolkit</code>","text":"<p>Returns the preconfigured toolkit if you want to inspect or extend it.</p>"},{"location":"api-reference/agents/sql-agent/#trainsummarize-bool-false-force-bool-false-generator","title":"<code>train(summarize: bool = False, force: bool = False) -&gt; Generator</code>","text":"<p>Convenience wrapper around <code>SQLSchemaStore.train</code>. Use it to pre-compute or refresh schema summaries outside of a chat session.</p> <p><pre><code>for step in agent.train(summarize=True, force=False):\n    print(step)\n</code></pre> Available options depend on <code>SQLSchemaStore</code>; typically <code>summarize=True</code> asks the LLM for narrative summaries of each table, while <code>force=True</code> bypasses existing cache entries.</p>"},{"location":"api-reference/agents/sql-agent/#is_trained-bool","title":"<code>is_trained() -&gt; bool</code>","text":"<p>Returns <code>True</code> when at least one cached schema summary is available.</p>"},{"location":"api-reference/agents/sql-agent/#built-in-sql-tools","title":"Built-in SQL tools","text":"Tool Purpose Typical Inputs Typical Output <code>RelevantTableSchemaFinderLLM</code> Selects relevant tables and returns concise schema context (optionally with sample rows). <code>question: str</code>, optional knobs for limits Schema text / JSON for downstream SQL generation <code>SQLQueryGenerator</code> Produces a dialect-correct SQL query from the question + schema context. <code>question: str</code>, <code>schema: str</code> SQL string <code>SQLExecutor</code> Executes a SQL statement via the configured SQLAlchemy engine. <code>sql: str</code>, optional params Rows (list of dicts/tuples) or error info <code>FinalAnswerFormatter</code> Converts rows into a concise, human-readable answer. <code>rows: Any</code>, <code>question: str</code> Natural-language summary text <code>DBInsightsTool</code> Provides quick DB-wide insights (table counts, schema stats, anomalies). optional scope params Descriptive text / small tables <p>Each tool defines a <code>ToolSpec</code> so the ReAct agent can validate inputs, prune unknown keys (if enabled), and self-repair Actions when needed.</p>"},{"location":"api-reference/agents/sql-agent/#notes-best-practices","title":"Notes &amp; Best Practices","text":"<ul> <li>Read-only by default: <code>SQLExecutor</code> will execute whatever SQL you pass it. For production, use read-only credentials unless write operations are explicitly intended and guarded.  </li> <li>Schema cache location: supply <code>storage_path</code> if you need deterministic cache placement (e.g., containers, CI).  </li> <li>Streaming UI: distinguish tool observations (e.g., <code>Observation:</code> lines) from the final answer markers (unless suppressed).  </li> <li>Domain policy: the agent ships with SQL-specific guidance (discover \u2192 generate \u2192 execute \u2192 format). You can override/extend via <code>specific_instructions</code>.  </li> <li>Extras between tools: tool <code>extras</code> are passed agent-side to the next tool call without going through the LLM. Use this to pass rich Python objects (e.g., compiled queries, parsed schemas) that aren\u2019t easily serializable.  </li> <li>Retries &amp; repair: Action parsing problems and tool failures are automatically repaired with bounded retries (see <code>ReActConfig</code> and <code>RetryPolicy</code>).</li> </ul>"},{"location":"api-reference/agents/sql-agent/#security-considerations","title":"Security Considerations","text":"<ul> <li>SQL injection: The agent itself won\u2019t construct queries unsafely if <code>SQLQueryGenerator</code> is given correct schema context, but validate inputs and consider parameterized SQL in your execution layer.  </li> <li>Privileges: Provide least-privilege DB credentials. Separate read and write roles where possible.  </li> <li>Auditing: Log tool calls and queries in production (subject to privacy constraints).</li> </ul>"},{"location":"api-reference/agents/tools_router_agent/","title":"ToolsRouterAgent","text":"<p>Module: <code>neurosurfer.agents.tools_router</code></p>"},{"location":"api-reference/agents/tools_router_agent/#overview","title":"Overview","text":"<p><code>ToolsRouterAgent</code> is a lightweight, production\u2011ready tool router. It uses an LLM to select exactly one tool from your <code>Toolkit</code>, validates the proposed inputs against the tool\u2019s <code>ToolSpec</code>, and then executes the tool. It supports both streaming and non\u2011streaming outputs, optional input pruning, and bounded retries for both routing and tool execution.</p> <p>Typical flow:</p> <ol> <li>Route: Ask the LLM to emit a strict JSON decision <code>{ \"tool\": \"...\", \"inputs\": { ... } }</code> </li> <li>Validate: Clean/validate inputs against the tool\u2019s schema (<code>ToolSpec</code>)  </li> <li>Execute: Call the tool and proxy its output (stream or text)  </li> <li>Recover: On invalid JSON / missing tool / tool error, retry with backoff (bounded by <code>RouterRetryPolicy</code>).</li> </ol> <p>This router is model\u2011agnostic. It works with OpenAI\u2011style and HF\u2011style clients that implement <code>ask(...)</code> and (for streaming) return OpenAI\u2011like <code>ChatCompletionChunk</code> deltas.</p>"},{"location":"api-reference/agents/tools_router_agent/#constructor","title":"Constructor","text":"<pre><code>ToolsRouterAgent(\n    toolkit: Toolkit,\n    llm: BaseModel,\n    logger: logging.Logger = logging.getLogger(__name__),\n    verbose: bool = False,\n    specific_instructions: str = \"\",\n    config: ToolsRouterConfig | None = None,\n)\n</code></pre> Parameter Type Description <code>toolkit</code> <code>Toolkit</code> Registry containing tool instances and their <code>ToolSpec</code>s. <code>llm</code> <code>BaseModel</code> LLM used for routing and fallback answers. Must implement <code>ask(...)</code>. <code>logger</code> <code>logging.Logger</code> Optional logger used when <code>verbose=True</code> and for error reporting. <code>verbose</code> <code>bool</code> Prints routing decisions, validation messages, and errors when <code>True</code>. <code>specific_instructions</code> <code>str</code> Extra system text appended to the router system prompt (domain policy). <code>config</code> <code>ToolsRouterConfig</code> | <code>None</code> Advanced flags (streaming default, pruning, retries, LLM defaults)."},{"location":"api-reference/agents/tools_router_agent/#configuration","title":"Configuration","text":""},{"location":"api-reference/agents/tools_router_agent/#toolsrouterconfig","title":"<code>ToolsRouterConfig</code>","text":"<pre><code>@dataclass\nclass ToolsRouterConfig:\n    allow_input_pruning: bool = True\n    repair_with_llm: bool = True\n    return_stream_by_default: bool = True\n    retry: RouterRetryPolicy = RouterRetryPolicy()\n    temperature: float = 0.7\n    max_new_tokens: int = 4000\n</code></pre> Parameter Type Description <code>allow_input_pruning</code> <code>bool</code> When <code>True</code>, unknown keys in the routed <code>inputs</code> are dropped before validation. Set <code>False</code> for strict mode. <code>repair_with_llm</code> <code>bool</code> When <code>True</code>, the agent will ask the LLM to re\u2011route or repair inputs after failures (bounded by retry policy). <code>return_stream_by_default</code> <code>bool</code> If <code>True</code>, <code>run(...)</code> streams by default unless <code>stream=False</code> is passed. <code>retry</code> <code>RouterRetryPolicy</code> Controls retry counts and backoff for routing/tool execution. <code>temperature</code> <code>float</code> Default temperature for routing calls (overridable per <code>run(...)</code>). <code>max_new_tokens</code> <code>int</code> Default token cap for routing calls (overridable per <code>run(...)</code>)."},{"location":"api-reference/agents/tools_router_agent/#routerretrypolicy","title":"<code>RouterRetryPolicy</code>","text":"<pre><code>@dataclass\nclass RouterRetryPolicy:\n    max_route_retries: int = 2\n    max_tool_retries: int = 1\n    backoff_sec: float = 0.7\n</code></pre> Parameter Type Description <code>max_route_retries</code> <code>int</code> How many times the router attempts to re\u2011route when JSON is invalid or no tool is chosen. <code>max_tool_retries</code> <code>int</code> How many times to retry tool execution after a tool error (transient failures). <code>backoff_sec</code> <code>float</code> Linear backoff delay between retries (multiplied by attempt index)."},{"location":"api-reference/agents/tools_router_agent/#methods","title":"Methods","text":""},{"location":"api-reference/agents/tools_router_agent/#runuser_query-str-chat_history-listdict-none-none-stream-bool-none-none-temperature-float-none-none-max_new_tokens-int-none-none-kwargs-str-iteratorstr","title":"<code>run(user_query: str, chat_history: list[dict] | None = None, *, stream: bool | None = None, temperature: float | None = None, max_new_tokens: int | None = None, **kwargs) -&gt; str | Iterator[str]</code>","text":"<p>Routes, validates, and executes a tool for the given <code>user_query</code>. Returns a generator of strings when streaming, otherwise a single string.</p> <pre><code>router = ToolsRouterAgent(toolkit=tk, llm=llm, verbose=True)\nfor chunk in router.run(\"Summarize ./README.md with bullets\", stream=True):\n    print(chunk, end=\"\")\n</code></pre> <p>Important kwargs:</p> Arg Type Description <code>stream</code> <code>bool \\| None</code> Whether to stream output. Defaults to <code>config.return_stream_by_default</code>. <code>temperature</code> <code>float \\| None</code> Overrides the routing temperature for this call. <code>max_new_tokens</code> <code>int \\| None</code> Overrides the routing token cap for this call. <code>**kwargs</code> <code>Any</code> Forwarded to the tool call (merged with routed <code>inputs</code>)."},{"location":"api-reference/agents/tools_router_agent/#routing-behavior","title":"Routing behavior","text":"<ul> <li>The router sends the user message and a tool catalog (from <code>Toolkit.get_tools_description()</code>) to the LLM with a strict system prompt.  </li> <li>The model must return one\u2011line JSON with exactly two keys: <code>\"tool\"</code> and <code>\"inputs\"</code>. Example:</li> </ul> <pre><code>{\"tool\":\"markdown_summarizer\",\"inputs\":{\"path\":\"README.md\"}}\n</code></pre> <ul> <li>If JSON parsing fails or <code>\"tool\":\"none\"</code> is returned, the router retries (<code>max_route_retries</code>).  </li> <li>If a usable decision cannot be produced, a helpful fallback message is generated for the user.</li> </ul>"},{"location":"api-reference/agents/tools_router_agent/#input-validation-pruning","title":"Input validation &amp; pruning","text":"<ul> <li>If the chosen tool has a <code>ToolSpec</code>, the router validates inputs.  </li> <li>With <code>allow_input_pruning=True</code>, unknown keys are dropped before validation. Set <code>False</code> for strict behavior (fail fast + attempt repair).</li> </ul>"},{"location":"api-reference/agents/tools_router_agent/#tool-execution","title":"Tool execution","text":"<ul> <li>The router calls the tool and proxies its output.  </li> <li>If the tool returns a <code>ToolResponse</code> with a generator, the router streams those chunks.  </li> <li>If the tool returns a <code>ToolResponse</code> with a plain string / non\u2011stream response, it returns (or yields once) accordingly.  </li> <li>Tool errors are retried up to <code>max_tool_retries</code> with linear backoff.</li> </ul>"},{"location":"api-reference/agents/tools_router_agent/#system-prompt-router","title":"System prompt (router)","text":"<p>The router crafts a compact system instruction that lists available tools and enforces strict JSON. You can append domain policy via <code>specific_instructions</code> (e.g., \u201cprefer read\u2011only operations\u201d or \u201cnever use external network tools\u201d).</p>"},{"location":"api-reference/agents/tools_router_agent/#error-handling-fallbacks","title":"Error handling &amp; fallbacks","text":"<ul> <li>Routing failures: After retries, the agent produces a concise, user\u2011friendly message (without exposing internal errors).  </li> <li>Tool failures: After bounded retries, the agent returns a helpful message indicating that the request couldn\u2019t be completed.  </li> <li>Logging: With <code>verbose=True</code>, routing outputs, parsed decisions, and exceptions are logged for debugging.</li> </ul>"},{"location":"api-reference/agents/tools_router_agent/#example","title":"Example","text":"<pre><code>from neurosurfer.agents.tools_router import ToolsRouterAgent, ToolsRouterConfig, RouterRetryPolicy\nfrom neurosurfer.tools import Toolkit\nfrom neurosurfer.models.chat_models.openai import OpenAIModel\nfrom my_tools import MarkdownSummarizer, GrepTool\n\ntk = Toolkit()\ntk.register_tool(MarkdownSummarizer())\ntk.register_tool(GrepTool())\n\nllm = OpenAIModel(model_name=\"gpt-4o-mini\")\n\nrouter = ToolsRouterAgent(\n    toolkit=tk,\n    llm=llm,\n    verbose=True,\n    config=ToolsRouterConfig(\n        allow_input_pruning=True,\n        repair_with_llm=True,\n        return_stream_by_default=True,\n        retry=RouterRetryPolicy(max_route_retries=2, max_tool_retries=1, backoff_sec=0.7),\n        temperature=0.5,\n        max_new_tokens=2000,\n    ),\n)\n\n# Streaming\nfor token in router.run(\"Summarize the key sections of README.md\", stream=True):\n    print(token, end=\"\")\n\n# Non-stream\ntext = router.run(\"Find 'RAG' mentions in ./docs\", stream=False)\nprint(\"\\n\\nRESULT:\", text)\n</code></pre>"},{"location":"api-reference/agents/tools_router_agent/#best-practices","title":"Best practices","text":"<ul> <li>Keep tool specs accurate: the better your <code>ToolSpec</code>s, the more reliable routing becomes.  </li> <li>Prefer minimal required inputs: tools should require only what\u2019s essential; optional params can be added later by the agent/tool itself.  </li> <li>Use pruning in early development: <code>allow_input_pruning=True</code> is forgiving while tool schemas evolve. For production, consider strict mode.  </li> <li>Add a clarifier tool: if your UX supports it, create an <code>ask_user</code> tool so the router can resolve missing inputs explicitly.  </li> <li>Audit routing: log router output for drift detection; periodically evaluate accuracy on real prompts.</li> </ul>"},{"location":"api-reference/agents/tools_router_agent/#related","title":"Related","text":"<ul> <li><code>Toolkit</code> \u2014 tool registry and descriptions surfaced to the router.  </li> <li><code>ReActAgent</code> \u2014 full reasoning\u2011and\u2011acting loop (choose + observe + iterate).  </li> </ul>"},{"location":"api-reference/database/","title":"Databases","text":"<p>Neurosurfer\u2019s database module provides production-grade utilities for connecting to data backends, inspecting schemas, and powering SQL agents. Today the focus is SQL via SQLAlchemy; the API is designed so additional backends (e.g., NoSQL) can be added without disrupting callers.</p> <ul> <li> <p> SQLDatabase</p> <p>High-level SQLAlchemy wrapper with schema introspection, sample rows, view support, table filters, and metadata caching for fast startup.</p> <p> Documentation</p> </li> <li> <p> SQLSchemaStore</p> <p>Builds a compact, LLM-friendly schema knowledge base from a live database. Stores JSON summaries per table (optionally LLM-generated) for downstream tools/agents.</p> <p> Documentation</p> </li> <li> <p> Future Backends</p> <p>The public contracts are backend-agnostic. More database types (e.g., document stores) can be added under <code>neurosurfer.db.*</code> as the project grows.</p> <p> Contribute</p> </li> </ul>"},{"location":"api-reference/database/#how-the-sql-path-works","title":"How the SQL path works","text":"<ol> <li>Connect with <code>SQLDatabase</code> \u2192 reflect schema (tables/views), cache metadata, and (optionally) sample rows for context.  </li> <li>Summarize with <code>SQLSchemaStore</code> \u2192 persist compact JSON summaries of each table\u2019s purpose + raw schema (optionally via LLM).  </li> <li>Use in tools/agents \u2192 pass stored summaries/schema to built-in SQL tools (e.g., relevant-table finder, query generator, executor) to enable a robust text-to-SQL workflow.</li> </ol> <p>The schema cache prevents repetitive network calls on every run. You control freshness via TTL and <code>force_refresh</code>.</p>"},{"location":"api-reference/database/#quick-start","title":"Quick Start","text":"<pre><code>from neurosurfer.db.sql.sql_database import SQLDatabase\nfrom neurosurfer.db.sql.sql_schema_store import SQLSchemaStore\nfrom neurosurfer.models.chat_models.openai_like import OpenAILikeModel  # example interface\n\n# 1) Inspect a live database\ndb = SQLDatabase(\n    database_uri=\"postgresql://user:pass@localhost:5432/analytics\",\n    include_tables=[\"users\", \"orders\", \"order_items\"],  # optional\n    sample_rows_in_table_info=3,\n    view_support=True,\n    metadata_cache_dir=\"~/.cache/neurosurfer/sqlmeta\",\n    cache_ttl_seconds=86400  # 1 day\n)\nprint(db.get_table_info([\"users\"]))   # CREATE TABLE + sample rows\n\n# 2) Build a schema store (with or without LLM summaries)\nllm = OpenAILikeModel(model=\"gpt-4o-mini\")  # any BaseModel-compatible LLM\nstore = SQLSchemaStore(db_uri=\"postgresql://user:pass@localhost:5432/analytics\", llm=llm, storage_path=\"./\")\nfor _ in store.train(summarize=True, force=True):  # generator yields progress counts\n    pass\n\n# 3) Consume summaries in tools/agents\nprint(\"Tables summarized:\", store.get_tables_count())\nusers = store.get_table_data(\"users\")\n</code></pre>"},{"location":"api-reference/database/#design-goals","title":"Design goals","text":"<ul> <li>Frictionless introspection \u2013 sensible defaults, easy filters, sample rows for realism.  </li> <li>Repeatable performance \u2013 schema caching avoids re-reflection every run.  </li> <li>Agent-first \u2013 JSON summaries that LLMs can reason over without expansive context windows.  </li> <li>Safety \u2013 no credentials in cache filenames; defensive error handling; opt-in view reflection.</li> </ul>"},{"location":"api-reference/database/sql_database/","title":"SQLDatabase","text":"<p>Module: <code>neurosurfer.db.sql.sql_database.SQLDatabase</code></p> <p>A production-grade SQLAlchemy wrapper used by Neurosurfer\u2019s SQL agents/tools. It provides schema introspection, table/view filtering, sample rows, and metadata caching so agents can plan reliably without repeated, slow reflection calls.</p>"},{"location":"api-reference/database/sql_database/#features","title":"Features","text":"<ul> <li>\ud83d\udd0c SQLAlchemy-powered: works with Postgres, MySQL, SQLite, SQL Server (via ODBC), etc.  </li> <li>\ud83e\udded Table/View discovery: include/ignore lists + optional view reflection.  </li> <li>\ud83d\udd0e Schema text: emits <code>CREATE TABLE</code> DDL plus optional indexes and sample rows.  </li> <li>\ud83d\ude80 Metadata cache: reflective metadata is cached to disk; TTL and force refresh supported.  </li> <li>\ud83e\uddea Safe query execution: convenience <code>run(...)</code> pattern (if present in your codebase) and low-level helpers demonstrated below.  </li> <li>\ud83e\uddf0 Utility: <code>build_connection_string(...)</code> for consistent DSNs.</li> </ul>"},{"location":"api-reference/database/sql_database/#initialization","title":"Initialization","text":"<pre><code>from neurosurfer.db.sql.sql_database import SQLDatabase\n\ndb = SQLDatabase(\n    database_uri=\"postgresql://user:pass@localhost:5432/analytics\",\n    schema=None,                         # target schema (optional)\n    ignore_tables=None,                  # e.g., [\"migrations\"]\n    include_tables=None,                 # OR a whitelist, e.g., [\"users\", \"orders\"]\n    sample_rows_in_table_info=3,         # rows per table to include under /* ... */\n    indexes_in_table_info=False,         # include index info in DDL block\n    custom_table_info=None,              # {table_name: \"pre-rendered info string\"}\n    view_support=False,                  # include views in reflection\n    max_string_length=300,               # formatting clamp for sample values\n    lazy_table_reflection=False,         # when False, reflect eagerly\n    metadata_cache_dir=\"~/.cache/neurosurfer/sqlmeta\",\n    force_refresh=False,\n    cache_ttl_seconds=86400,             # expire cache after N seconds (None = never)\n)\n</code></pre>"},{"location":"api-reference/database/sql_database/#parameter-reference","title":"Parameter reference","text":"Name Type Default Description <code>database_uri</code> <code>str | URL</code> \u2014 SQLAlchemy connection string. <code>schema</code> <code>str?</code> <code>None</code> Schema namespace to inspect (dialect-dependent). <code>ignore_tables</code> <code>list[str]?</code> <code>None</code> Exclude these tables from all operations. <code>include_tables</code> <code>list[str]?</code> <code>None</code> Restrict operations to these tables only. Mutually exclusive with <code>ignore_tables</code>. <code>sample_rows_in_table_info</code> <code>int</code> <code>3</code> Number of example rows to include per table in <code>get_table_info()</code>. <code>indexes_in_table_info</code> <code>bool</code> <code>False</code> Include index metadata in <code>get_table_info()</code> output. <code>custom_table_info</code> <code>dict?</code> <code>None</code> Pre-rendered table strings; bypass reflection for those tables. <code>view_support</code> <code>bool</code> <code>False</code> Reflect views in addition to tables. <code>max_string_length</code> <code>int</code> <code>300</code> Clamp cell values when printing sample rows. <code>lazy_table_reflection</code> <code>bool</code> <code>False</code> If <code>False</code>, reflect eagerly (or load from cache). If <code>True</code>, reflect on first use. <code>metadata_cache_dir</code> <code>str | Path?</code> <code>~/.cache/sqlalchemy_metadata</code> Where to store metadata cache files. <code>force_refresh</code> <code>bool</code> <code>False</code> Ignore cache and rebuild metadata now. <code>cache_ttl_seconds</code> <code>int?</code> <code>None</code> If set, reload metadata when cache age exceeds TTL."},{"location":"api-reference/database/sql_database/#common-tasks","title":"Common tasks","text":""},{"location":"api-reference/database/sql_database/#1-list-usable-tables","title":"1) List usable tables","text":"<pre><code>db.get_usable_table_names()\n# honors include/ignore and (optional) view_support\n</code></pre>"},{"location":"api-reference/database/sql_database/#2-get-schema-info-ddl-extras","title":"2) Get schema info (DDL + extras)","text":"<pre><code>print(db.get_table_info())          # all usable tables\nprint(db.get_table_info([\"users\"])) # specific tables\n</code></pre> <p>Output contains <code>CREATE TABLE ...</code> followed by an optional <code>/* ... */</code> block with indexes and <code>N</code> sample rows, e.g.:</p> <pre><code>CREATE TABLE users (\n  id BIGINT PRIMARY KEY,\n  email TEXT NOT NULL,\n  ...\n)\n\n/*\nTable Indexes:\n...\n\n3 rows from users table:\nid  email   ...\n1   alice@example.com   ...\n2   bob@example.com ...\n3   ...\n*/\n</code></pre>"},{"location":"api-reference/database/sql_database/#3-build-dsn-safely","title":"3) Build DSN safely","text":"<pre><code>from neurosurfer.db.sql.sql_database import SQLDatabase\n\ndsn = SQLDatabase.build_connection_string(\n    server=\"db.acme.com\",\n    database=\"analytics\",\n    username=\"svc_app\",\n    password=\"s3cr3t!\",\n    driver=\"mssql+pyodbc\",\n    odbc_driver=\"ODBC Driver 18 for SQL Server\",\n    port=\"1433\",\n)\n</code></pre>"},{"location":"api-reference/database/sql_database/#metadata-caching","title":"Metadata caching","text":"<p>When <code>lazy_table_reflection=False</code>, SQLDatabase attempts to load previously reflected <code>MetaData</code> from a cache file whose name is derived from:</p> <ul> <li>dialect (e.g., <code>postgresql</code>),  </li> <li>database identifier (never includes credentials),  </li> <li>schema, view_support, and the set of usable tables.</li> </ul> <p>You control freshness and behavior with:</p> <ul> <li><code>cache_ttl_seconds</code> \u2014 consider cache stale after this many seconds,  </li> <li><code>force_refresh=True</code> \u2014 ignore cache and rebuild now.</li> </ul> <p>For <code>sqlite:///:memory:</code> the cache is skipped entirely.</p>"},{"location":"api-reference/database/sql_database/#notes-gotchas","title":"Notes &amp; gotchas","text":"<ul> <li>If both <code>include_tables</code> and <code>ignore_tables</code> are set \u2192 a <code>ValueError</code> is raised.  </li> <li>When asking for <code>table_names</code> that don\u2019t exist \u2192 you\u2019ll get a <code>ValueError</code>.  </li> <li>Sample rows use <code>SELECT ... LIMIT N</code>; some dialects may return <code>ProgrammingError</code> for empty tables (handled).  </li> <li>JSON/unknown-typed columns (<code>NullType</code>) are excluded from printed DDL for clarity.</li> </ul>"},{"location":"api-reference/database/sql_schema_store/","title":"SQLSchemaStore","text":"<p>Module: <code>neurosurfer.db.sql.sql_schema_store.SQLSchemaStore</code></p> <p><code>SQLSchemaStore</code> extracts per-table schema strings from a live database and (optionally) asks an LLM to produce concise JSON summaries. It persists these to disk as a single JSON file you can reuse in agents/tools without re-querying the DB on every run.</p>"},{"location":"api-reference/database/sql_schema_store/#what-it-stores","title":"What it stores","text":"<p>Each table is keyed by name and contains at least:</p> <pre><code>{\n  \"table_name\": \"users\",\n  \"summary\": \"What data the table stores and its role in the system.\",\n  \"schema\": \"CREATE TABLE ... /* sample rows ... */\"\n}\n</code></pre> <p>When <code>summarize=True</code>, the <code>summary</code> is generated by your provided LLM according to a strict prompt; otherwise you can store raw schema only (extend as needed).</p>"},{"location":"api-reference/database/sql_schema_store/#initialize","title":"Initialize","text":"<pre><code>from neurosurfer.db.sql.sql_schema_store import SQLSchemaStore\nfrom neurosurfer.models.chat_models.openai_like import OpenAILikeModel  # implements BaseModel\n\nllm = OpenAILikeModel(model=\"gpt-4o-mini\")  # optional if you won't summarize\n\nstore = SQLSchemaStore(\n    db_uri=\"postgresql://user:pass@localhost:5432/analytics\",\n    llm=llm,                         # required if summarize=True\n    sample_rows_in_table_info=3,     # how many rows to include in schema text\n    storage_path=\"./\"                # where to write sql_schema_store_&lt;db&gt;.json\n)\n</code></pre>"},{"location":"api-reference/database/sql_schema_store/#train-extract-and-optionally-summarize","title":"Train (extract and optionally summarize)","text":"<p><code>train(...)</code> is a generator that yields the number of processed tables (for progress bars).</p> <pre><code>for n in store.train(summarize=True, force=True):\n    # e.g., update a progress UI\n    pass\n\nprint(\"Saved to:\", store.storage_path)\nprint(\"Tables summarized:\", store.get_tables_count())\n</code></pre> <ul> <li><code>summarize=False</code> \u2192 store raw schema strings (you can add summaries later).  </li> <li><code>summarize=True</code> \u2192 uses your <code>llm</code> to generate concise JSON summaries per table.  </li> <li><code>force=True</code> \u2192 clears any existing file and retrains all tables.</li> </ul> <p>The built-in prompt enforces a strict JSON response:</p> <pre><code>System: You are a master of SQL...\nUser: Given the schema of {table_name} ... Output this JSON object: {\"table_name\": \"...\", \"summary\": \"...\"}\n</code></pre>"},{"location":"api-reference/database/sql_schema_store/#read-use","title":"Read &amp; use","text":"<pre><code>data = store.get_table_data(\"users\")          # dict with keys: table_name, summary, schema\ncount = store.get_tables_count()              # number of usable tables\ntables = store.db.get_usable_table_names()    # from the underlying SQLDatabase\n</code></pre> <p>These summaries are ideal inputs for relevant-table selection, SQL generation, or DB insights tools. See the built-in tools under Tools \u2192 Built-in Tools \u2192 SQL.</p>"},{"location":"api-reference/database/sql_schema_store/#api-reference","title":"API reference","text":""},{"location":"api-reference/database/sql_schema_store/#__init__db_uri-llmnone-sample_rows_in_table_info3-storage_pathnone-loggernone","title":"<code>__init__(db_uri, llm=None, sample_rows_in_table_info=3, storage_path=None, logger=None)</code>","text":"<ul> <li>Creates an internal <code>SQLDatabase</code> with <code>view_support=True</code> and sets up a JSON file named <code>sql_schema_store_&lt;db&gt;.json</code> beneath <code>storage_path</code> (default <code>./</code>).  </li> <li>If <code>llm</code> is provided, it must satisfy the <code>BaseModel</code> interface with <code>.ask(...)</code> returning an OpenAI-like response dict.</li> </ul>"},{"location":"api-reference/database/sql_schema_store/#trainsummarize-bool-false-force-bool-false-generatorint-none-none","title":"<code>train(summarize: bool = False, force: bool = False) -&gt; Generator[int, None, None]</code>","text":"<ul> <li>Extracts schema strings for all usable tables.  </li> <li>If <code>summarize=True</code>, calls <code>summarize_schema__(table, schema)</code> per table.  </li> <li>Yields the count of processed tables for progress reporting.  </li> <li>Writes the final store to <code>storage_path</code> when complete.</li> </ul>"},{"location":"api-reference/database/sql_schema_store/#get_all_table_schemas-dictstr-str","title":"<code>get_all_table_schemas() -&gt; dict[str, str]</code>","text":"<p>Returns a mapping <code>{table_name: schema_string}</code> by delegating to the underlying <code>SQLDatabase</code> (<code>get_table_info([t])</code>).</p>"},{"location":"api-reference/database/sql_schema_store/#get_table_datatable-str-dict-none","title":"<code>get_table_data(table: str) -&gt; dict | None</code>","text":"<p>Fetches the stored JSON object for a single table.</p>"},{"location":"api-reference/database/sql_schema_store/#save_to_file-load_from_file","title":"<code>save_to_file()</code> / <code>load_from_file()</code>","text":"<p>Persist and restore the store JSON (<code>storage_path</code>).</p>"},{"location":"api-reference/database/sql_schema_store/#get_db_name-str-none","title":"<code>get_db_name() -&gt; str | None</code>","text":"<p>Extracts the DB name from the URI; used to build the default filename.</p>"},{"location":"api-reference/database/sql_schema_store/#get_tables_count-int","title":"<code>get_tables_count() -&gt; int</code>","text":"<p>Returns the number of usable tables from <code>SQLDatabase</code>.</p>"},{"location":"api-reference/database/sql_schema_store/#tips-caveats","title":"Tips &amp; caveats","text":"<ul> <li>Ensure your DB user has metadata privileges (and read access for <code>LIMIT N</code> samples).  </li> <li>You can postprocess or augment summaries (e.g., add row counts, PK/FK lists).  </li> <li>Treat generated JSON carefully in downstream code; validate before use in agents.  </li> <li>For very large schemas, consider chunking or filtering tables to keep context sizes manageable.</li> </ul>"},{"location":"api-reference/models/","title":"Models API","text":"<p>Neurosurfer bundles two model families:</p> <ul> <li>Chat models (<code>neurosurfer.models.chat_models</code>) \u2013 large language models used by agents and the FastAPI server.</li> <li>Embedders (<code>neurosurfer.models.embedders</code>) \u2013 vector generators for RAG pipelines and semantic search.</li> </ul> <p>Both families share consistent Pydantic responses and configuration patterns so you can swap providers without touching business logic.</p>"},{"location":"api-reference/models/#navigation","title":"Navigation","text":"<ul> <li> <p> Chat Models</p> <p>Large language models with OpenAI-compatible responses.</p> <p> Explore chat models</p> </li> <li> <p> Embedders</p> <p>Sentence encoders for semantic search and retrieval.</p> <p> Explore embedders</p> </li> </ul>"},{"location":"api-reference/models/#quick-comparison","title":"Quick comparison","text":""},{"location":"api-reference/models/#chat-models","title":"Chat models","text":"Model Backend Best for <code>OpenAIModel</code> Hosted API OpenAI Cloud, LM Studio, or any OpenAI-compatible gateway <code>TransformersModel</code> Local PyTorch GPU/CPU inference of Hugging Face checkpoints <code>UnslothModel</code> Unsloth CUDA Fast inference for LoRA/QLoRA finetunes <code>LlamaCppModel</code> llama.cpp CPU-first deployments and GGUF quantised weights <code>VLLMModel</code> HTTP client Remote vLLM clusters with OpenAI-compatible APIs"},{"location":"api-reference/models/#embedders","title":"Embedders","text":"Embedder Backend Best for <code>SentenceTransformerEmbedder</code> sentence-transformers High-quality open-source embeddings <code>LlamaCppEmbedder</code> llama.cpp GGUF embedding models without GPU dependencies"},{"location":"api-reference/models/#getting-started","title":"Getting started","text":""},{"location":"api-reference/models/#chat-model-example","title":"Chat model example","text":"<pre><code>from neurosurfer.models.chat_models.openai import OpenAIModel\n\nmodel = OpenAIModel(model_name=\"gpt-4o-mini\")\nreply = model.ask(\"Summarise the project in two sentences.\")\n\nprint(reply.choices[0].message.content)\n</code></pre>"},{"location":"api-reference/models/#embedder-example","title":"Embedder example","text":"<pre><code>from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n\nembedder = SentenceTransformerEmbedder(model_name=\"intfloat/e5-large-v2\")\nvector = embedder.embed(\"Explain retrieval augmented generation.\")\n\nprint(len(vector))  # embedding dimensionality\n</code></pre>"},{"location":"api-reference/models/#see-also","title":"See also","text":"<ul> <li>Agents</li> <li>RAG system</li> </ul>"},{"location":"api-reference/models/chat-models/","title":"Chat Models API","text":"<p>Large language models shipped with Neurosurfer share a unified interface through <code>BaseModel</code>. Choose the backend that matches your deployment constraints\u2014cloud APIs, local GPUs, llama.cpp, or remote vLLM servers.</p>"},{"location":"api-reference/models/chat-models/#supported-backends","title":"Supported backends","text":"Model Runtime Deploy on Ideal for <code>OpenAIModel</code> Hosted API OpenAI Cloud, LM Studio, Ollama, vLLM Production-ready quality with minimal setup <code>TransformersModel</code> PyTorch Local GPU/CPU Running Hugging Face checkpoints directly <code>UnslothModel</code> CUDA (Unsloth) Local GPU Fast inference for LoRA/QLoRA finetunes <code>LlamaCppModel</code> llama.cpp CPU / lightweight GPU GGUF quantised models with tiny footprint"},{"location":"api-reference/models/chat-models/#quick-start-examples","title":"Quick start examples","text":""},{"location":"api-reference/models/chat-models/#1-hosted-api-openai-compatible","title":"1. Hosted API (OpenAI-compatible)","text":"<pre><code>from neurosurfer.models.chat_models.openai import OpenAIModel\n\nmodel = OpenAIModel(model_name=\"gpt-4o-mini\")\ncompletion = model.ask(\"Explain retrieval augmented generation in one sentence.\")\n\nprint(completion.choices[0].message.content)\n</code></pre>"},{"location":"api-reference/models/chat-models/#2-hugging-face-checkpoint-transformers","title":"2. Hugging Face checkpoint (Transformers)","text":"<pre><code>from neurosurfer.models.chat_models.transformers import TransformersModel\n\nmodel = TransformersModel(\n    model_name=\"/weights/Qwen3-4B-unsloth-bnb-4bit\",\n    load_in_4bit=False,  # already quantised\n)\n\nresponse = model.ask(\"List three benefits of local inference.\")\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"api-reference/models/chat-models/#3-llamacpp-gguf","title":"3. llama.cpp GGUF","text":"<pre><code>from neurosurfer.models.chat_models.llamacpp import LlamaCppModel\n\nmodel = LlamaCppModel(\n    model_path=\"/weights/llama-2-7b-chat.Q4_K_M.gguf\",\n    n_ctx=4096,\n    n_threads=8,\n)\n\nanswer = model.ask(\"What are GGUF files?\")\nprint(answer.choices[0].message.content)\n</code></pre>"},{"location":"api-reference/models/chat-models/#4-unsloth-accelerated-inference","title":"4. Unsloth accelerated inference","text":"<pre><code>from neurosurfer.models.chat_models.unsloth import UnslothModel\n\nmodel = UnslothModel(\n    model_name=\"/weights/Qwen3-4B-unsloth-bnb-4bit\",\n    enable_thinking=False,\n)\n\nreply = model.ask(\"Describe Unsloth in two bullet points.\")\nprint(reply.choices[0].message.content)\n</code></pre>"},{"location":"api-reference/models/chat-models/#choosing-a-backend","title":"Choosing a backend","text":"<ul> <li>Use OpenAIModel when you need the highest quality models or want to point at any OpenAI-compatible API.</li> <li>Use TransformersModel when you manage your own Hugging Face checkpoints and have GPU resources.</li> <li>Use UnslothModel for LoRA/QLoRA finetunes optimised with Unsloth\u2019s runtime.</li> <li>Use LlamaCppModel on CPU-first deployments or when you rely on GGUF quantised weights.</li> </ul>"},{"location":"api-reference/models/chat-models/#see-also","title":"See also","text":"<ul> <li>Embedders</li> <li>Agents</li> </ul>"},{"location":"api-reference/models/chat-models/base-model/","title":"BaseModel","text":"<p>Module: <code>neurosurfer.models.chat_models.base</code> Type: Abstract base class</p>"},{"location":"api-reference/models/chat-models/base-model/#overview","title":"Overview","text":"<p><code>BaseModel</code> is the foundation for every chat model shipped with Neurosurfer. It standardises request handling, streaming, token accounting, and OpenAI-compatible responses so that downstream agents can work with any backend without branching.</p>"},{"location":"api-reference/models/chat-models/base-model/#core-capabilities","title":"Core capabilities","text":"<ul> <li>Creates OpenAI-style <code>ChatCompletionResponse</code> / <code>ChatCompletionChunk</code> objects via <code>ask</code></li> <li>Handles streaming vs. non-streaming in a single entry point</li> <li>Tracks prompt/completion token usage (when provided or estimated)</li> <li>Provides helper methods that subclasses can reuse to build deltas, stop chunks, and final responses</li> <li>Exposes <code>stop_generation</code>, <code>reset_stop_signal</code>, and <code>set_stop_words</code> hooks that concrete implementations respond to</li> </ul>"},{"location":"api-reference/models/chat-models/base-model/#constructor","title":"Constructor","text":""},{"location":"api-reference/models/chat-models/base-model/#basemodel__init__","title":"<code>BaseModel.__init__</code>","text":"<pre><code>from neurosurfer.config import config\n\nBaseModel(\n    *,\n    max_seq_length: int = config.base_model.max_seq_length,\n    verbose: bool = config.base_model.verbose,\n    logger: logging.Logger = logging.getLogger(),\n    **kwargs,\n)\n</code></pre> Parameter Type Default Description <code>max_seq_length</code> <code>int</code> <code>config.base_model.max_seq_length</code> Maximum context window supported by the backend. <code>verbose</code> <code>bool</code> <code>config.base_model.verbose</code> When <code>True</code>, subclasses should emit additional debug logs. <code>logger</code> <code>logging.Logger</code> <code>logging.getLogger()</code> Logger instance reused across helper methods. <code>**kwargs</code> any \u2013 Forwarded to subclass initialisation; stored on subclasses as needed. <p>Subclasses must call <code>super().__init__</code> so shared state (<code>model_name</code>, <code>lock</code>, <code>max_seq_length</code>, etc.) is initialised before loading a backend in <code>init_model()</code>.</p> <p>Tip</p> <p><code>config</code> is imported from <code>neurosurfer.config</code>. Please see the configuration section for more details.</p>"},{"location":"api-reference/models/chat-models/base-model/#public-interface","title":"Public interface","text":""},{"location":"api-reference/models/chat-models/base-model/#ask","title":"<code>ask</code>","text":"<pre><code>ask(\n    user_prompt: str,\n    *,\n    system_prompt: str = config.base_model.system_prompt,\n    chat_history: list[dict] = [],\n    temperature: float = config.base_model.temperature,\n    max_new_tokens: int = config.base_model.max_new_tokens,\n    stream: bool = False,\n    **kwargs,\n) -&gt; ChatCompletionResponse | Generator[ChatCompletionChunk, None, None]\n</code></pre> <p>Routes the request to <code>_call</code> (non-streaming) or <code>_stream</code> (streaming) and returns OpenAI-compatible Pydantic models. All subclasses must implement <code>_call</code> and <code>_stream</code>.</p> <p>Usage:</p> <pre><code>response = model.ask(\"Explain RAG in one sentence\")\nprint(response.choices[0].message.content)\n\nfor chunk in model.ask(\"Summarise the docs\", stream=True):\n    delta = chunk.choices[0].delta.content or \"\"\n    print(delta, end=\"\")\n</code></pre>"},{"location":"api-reference/models/chat-models/base-model/#stop_generation","title":"<code>stop_generation</code>","text":"<p>Sets an implementation-defined flag instructing the current generation to stop at the next opportunity. Concrete models should respect this flag inside their generation loops (see the built-in integrations for patterns).</p>"},{"location":"api-reference/models/chat-models/base-model/#reset_stop_signal","title":"<code>reset_stop_signal</code>","text":"<p>Clears any stop signal before a new request runs. Typically called internally by <code>ask</code>.</p>"},{"location":"api-reference/models/chat-models/base-model/#set_stop_words","title":"<code>set_stop_words</code>","text":"<p>Stores a stop word list which helpers such as <code>_transformers_consume_stream</code> use to truncate responses without emitting the terminator tokens.</p>"},{"location":"api-reference/models/chat-models/base-model/#hooks-for-subclass-authors","title":"Hooks for subclass authors","text":"<p>Concrete implementations must provide the following methods:</p> <ul> <li><code>init_model(self) -&gt; None</code>: load the underlying model/client/tokenizer.</li> <li><code>_call(...) -&gt; ChatCompletionResponse</code>: run a blocking generation and return the final response.</li> <li><code>_stream(...) -&gt; Generator[ChatCompletionChunk, None, None]</code>: yield partial chunks followed by a terminal stop chunk.</li> <li><code>stop_generation(self) -&gt; None</code>: honour stop requests from callers.</li> </ul> <p>Optional helpers supplied by <code>BaseModel</code>:</p> <ul> <li><code>_delta_chunk(call_id, model, content)</code> \u2013 build incremental streaming chunks.</li> <li><code>_stop_chunk(call_id, model, finish_reason)</code> \u2013 emit the terminating streaming chunk.</li> <li><code>_final_nonstream_response(call_id, model, content, prompt_tokens, completion_tokens)</code> \u2013 return a complete <code>ChatCompletionResponse</code>.</li> <li><code>_transformers_consume_stream(streamer)</code> \u2013 consume tokens from a <code>TextIteratorStreamer</code>, applying stop words and <code>&lt;think&gt;</code> filtering.</li> </ul>"},{"location":"api-reference/models/chat-models/base-model/#creating-a-custom-model","title":"Creating a custom model","text":"<pre><code>from neurosurfer.models.chat_models.base import BaseModel\nfrom neurosurfer.server.schemas import ChatCompletionResponse\n\nclass CustomModel(BaseModel):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.init_model()\n\n    def init_model(self):\n        self.model = load_backend_somehow()\n        self.tokenizer = load_tokenizer()\n        self.model_name = \"my-custom-model\"\n\n    def _call(self, **kwargs) -&gt; ChatCompletionResponse:\n        # Run the backend, produce text, then build the response\n        content = self._run_backend(**kwargs)\n        return self._final_nonstream_response(\n            call_id=self.call_id,\n            model=self.model_name,\n            content=content,\n            prompt_tokens=0,\n            completion_tokens=0,\n        )\n\n    def _stream(self, **kwargs):\n        for token in self._backend_stream(**kwargs):\n            yield self._delta_chunk(self.call_id, self.model_name, token)\n        yield self._stop_chunk(self.call_id, self.model_name, finish_reason=\"stop\")\n\n    def stop_generation(self):\n        self.backend.cancel_current_request()\n</code></pre>"},{"location":"api-reference/models/chat-models/base-model/#see-also","title":"See also","text":"<ul> <li>OpenAIModel</li> <li>TransformersModel</li> <li>Chat models index</li> </ul> <p>mkdocstrings output is temporarily disabled while import hooks are updated.</p>"},{"location":"api-reference/models/chat-models/llamacpp-model/","title":"LlamaCppModel","text":"<p>Module: <code>neurosurfer.models.chat_models.llamacpp</code> Inherits: <code>BaseModel</code></p>"},{"location":"api-reference/models/chat-models/llamacpp-model/#overview","title":"Overview","text":"<p><code>LlamaCppModel</code> loads GGUF checkpoints through the <code>llama-cpp-python</code> bindings. It enables fast local inference on CPU or GPU with a pure Python interface while retaining the standard Neurosurfer response format.</p>"},{"location":"api-reference/models/chat-models/llamacpp-model/#highlights","title":"Highlights","text":"<ul> <li>Supports both local GGUF files and Hugging Face repositories</li> <li>Works on CPU-only systems or with CUDA/OpenCL acceleration via <code>n_gpu_layers</code></li> <li>Streams tokens incrementally with stop-word enforcement and cancellation support</li> <li>Tracks prompt/completion token usage when provided by llama.cpp</li> </ul>"},{"location":"api-reference/models/chat-models/llamacpp-model/#constructor","title":"Constructor","text":""},{"location":"api-reference/models/chat-models/llamacpp-model/#llamacppmodel__init__","title":"<code>LlamaCppModel.__init__</code>","text":"<pre><code>LlamaCppModel(\n    model_path: str | None = None,\n    *,\n    repo_id: str | None = None,\n    filename: str | None = None,\n    n_ctx: int = 2048,\n    n_threads: int = 4,\n    main_gpu: int = 0,\n    n_gpu_layers: int = -1,\n    max_seq_length: int = 2048,\n    stop_words: list[str] | None = None,\n    verbose: bool = False,\n    logger: logging.Logger = logging.getLogger(),\n    **kwargs,\n)\n</code></pre> Parameter Type Description <code>model_path</code> <code>str \\| None</code> Path to a local <code>.gguf</code> file. Required unless <code>repo_id</code> + <code>filename</code> is provided. <code>repo_id</code> <code>str \\| None</code> Hugging Face repo id for <code>Llama.from_pretrained</code>. <code>filename</code> <code>str \\| None</code> Specific filename within the repo (usually the GGUF file). <code>n_ctx</code> <code>int</code> Context window size passed to llama.cpp (<code>max_seq_length</code> mirrors this). <code>n_threads</code> <code>int</code> CPU worker threads. <code>main_gpu</code> <code>int</code> Primary GPU index when offloading layers. <code>n_gpu_layers</code> <code>int</code> Number of layers to place on GPU (<code>-1</code> = as many as possible). <code>stop_words</code> <code>list[str] \\| None</code> Optional list of stop words enforced client-side. <code>verbose</code> <code>bool</code> Enable verbose llama.cpp logging. <code>logger</code> <code>logging.Logger</code> Logger used for wrapper messages. <p>If <code>model_path</code> ends with <code>.gguf</code>, the file is loaded directly. Otherwise supply <code>repo_id</code> and <code>filename</code> to download and cache automatically.</p>"},{"location":"api-reference/models/chat-models/llamacpp-model/#usage","title":"Usage","text":""},{"location":"api-reference/models/chat-models/llamacpp-model/#basic-completion","title":"Basic completion","text":"<pre><code>from neurosurfer.models.chat_models.llamacpp import LlamaCppModel\n\nmodel = LlamaCppModel(\n    model_path=\"/weights/llama-2-7b-chat.Q4_K_M.gguf\",\n    n_ctx=4096,\n    n_threads=8,\n)\n\nresponse = model.ask(\"Explain retrieval-augmented generation.\")\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"api-reference/models/chat-models/llamacpp-model/#streaming-with-gpu-acceleration","title":"Streaming with GPU acceleration","text":"<pre><code>gpu_model = LlamaCppModel(\n    model_path=\"/weights/Qwen2.5-7B-Instruct-Q3_K_L.gguf\",\n    n_ctx=4096,\n    n_gpu_layers=40,  # offload as many layers as possible\n)\n\nstream = gpu_model.ask(\"List three pros of llama.cpp.\", stream=True)\nfor chunk in stream:\n    delta = chunk.choices[0].delta.content or \"\"\n    print(delta, end=\"\")\n</code></pre>"},{"location":"api-reference/models/chat-models/llamacpp-model/#loading-from-hugging-face","title":"Loading from Hugging Face","text":"<pre><code>hf_model = LlamaCppModel(\n    repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n    filename=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n    n_ctx=4096,\n)\n</code></pre>"},{"location":"api-reference/models/chat-models/llamacpp-model/#tips","title":"Tips","text":"<ul> <li>Stop words: <code>model.set_stop_words([\"Observation:\", \"\\nFinal answer:\"])</code> prevents the response from emitting tool markers.</li> <li>Cancellation: Call <code>model.stop_generation()</code> while streaming to interrupt long generations.</li> <li>GPU selection: <code>main_gpu</code> selects the device; use <code>CUDA_VISIBLE_DEVICES</code> when running inside multi-GPU nodes.</li> <li>Thread count: On CPU-only setups tune <code>n_threads</code> to match available cores for optimal throughput.</li> </ul>"},{"location":"api-reference/models/chat-models/llamacpp-model/#related-models","title":"Related models","text":"<ul> <li><code>TransformersModel</code> \u2013 load PyTorch checkpoints directly</li> <li><code>UnslothModel</code> \u2013 GPU-optimised inference via Unsloth</li> </ul> <p>mkdocstrings output is temporarily disabled while import hooks are updated.</p>"},{"location":"api-reference/models/chat-models/openai-model/","title":"OpenAIModel","text":"<p>Module: <code>neurosurfer.models.chat_models.openai</code> Inherits: <code>BaseModel</code></p>"},{"location":"api-reference/models/chat-models/openai-model/#overview","title":"Overview","text":"<p><code>OpenAIModel</code> connects Neurosurfer to any OpenAI-compatible chat endpoint. It ships with sensible defaults for OpenAI Cloud but also works with LM Studio, vLLM, Ollama OpenAI bridges, and other self-hosted gateways. Responses are normalised to OpenAI's <code>ChatCompletionResponse</code>/<code>ChatCompletionChunk</code> schema so agents can consume them transparently.</p>"},{"location":"api-reference/models/chat-models/openai-model/#highlights","title":"Highlights","text":"<ul> <li>Supports both non-streaming and streaming chat completions</li> <li>Accepts <code>stop_words</code> for client-side truncation without leaking the sequence</li> <li>Optional <code>strip_reasoning</code> helper to hide <code>&lt;think&gt;</code> or <code>&lt;analysis&gt;</code> blocks returned by newer models</li> <li>Attempts to load a tokenizer to estimate token usage when the server omits usage fields</li> </ul>"},{"location":"api-reference/models/chat-models/openai-model/#constructor","title":"Constructor","text":""},{"location":"api-reference/models/chat-models/openai-model/#openaimodel__init__","title":"<code>OpenAIModel.__init__</code>","text":"<pre><code>from neurosurfer.config import config\n\nOpenAIModel(\n    model_name: str = \"gpt-4o-mini\",\n    *,\n    base_url: str | None = None,\n    api_key: str | None = os.getenv(\"OPENAI_API_KEY\"),\n    timeout: float | None = 120.0,\n    stop_words: list[str] | None = config.base_model.stop_words,\n    strip_reasoning: bool = config.base_model.strip_reasoning,\n    max_seq_length: int = config.base_model.max_seq_length,\n    verbose: bool = config.base_model.verbose,\n    logger: logging.Logger = logging.getLogger(),\n    **kwargs,\n)\n</code></pre> Parameter Type Default Description <code>model_name</code> <code>str</code> <code>\"gpt-4o-mini\"</code> Target deployment on the OpenAI-compatible endpoint. <code>base_url</code> <code>str \\| None</code> <code>None</code> Override API base URL (e.g. <code>http://localhost:8000/v1</code>). Leave <code>None</code> for OpenAI Cloud. <code>api_key</code> <code>str \\| None</code> <code>OPENAI_API_KEY</code> env var Bearer token used by the OpenAI SDK. Can be omitted for proxies that ignore authentication. <code>timeout</code> <code>float \\| None</code> <code>120.0</code> Maximum seconds to wait for a response. <code>stop_words</code> <code>list[str] \\| None</code> <code>config.base_model.stop_words</code> Optional stop sequence list checked on the client side. <code>strip_reasoning</code> <code>bool</code> <code>config.base_model.strip_reasoning</code> If <code>True</code>, remove reasoning/thinking blocks before returning content. <code>max_seq_length</code> <code>int</code> <code>config.base_model.max_seq_length</code> Advertised context window, forwarded to <code>BaseModel</code>. <code>verbose</code> <code>bool</code> <code>config.base_model.verbose</code> Emit detailed logs during initialisation and errors. <code>logger</code> <code>logging.Logger</code> <code>logging.getLogger()</code> Logger instance reused across operations. <p>The constructor immediately calls <code>init_model()</code> which instantiates the OpenAI client and tries to load a Hugging Face tokenizer matching <code>model_name</code> to estimate token usage when the server response omits it.</p> <p>Tip</p> <p><code>config</code> is imported from <code>neurosurfer.config</code>. Please see the configuration section for more details.</p>"},{"location":"api-reference/models/chat-models/openai-model/#usage","title":"Usage","text":""},{"location":"api-reference/models/chat-models/openai-model/#non-streaming-completion","title":"Non-streaming completion","text":"<pre><code>from neurosurfer.models.chat_models.openai import OpenAIModel\n\nmodel = OpenAIModel(model_name=\"gpt-4o-mini\")\nresponse = model.ask(\"Explain retrieval-augmented generation in one paragraph.\")\n\nprint(response.choices[0].message.content)\nprint(response.usage.total_tokens)\n</code></pre>"},{"location":"api-reference/models/chat-models/openai-model/#streaming-completion","title":"Streaming completion","text":"<pre><code>stream = model.ask(\n    \"List three benefits of streaming responses.\",\n    system_prompt=\"Keep answers concise.\",\n    stream=True,\n    temperature=0.4,\n)\n\nfor chunk in stream:\n    delta = chunk.choices[0].delta.content or \"\"\n    print(delta, end=\"\", flush=True)\n</code></pre>"},{"location":"api-reference/models/chat-models/openai-model/#working-with-stop-words-and-reasoning-blocks","title":"Working with stop words and reasoning blocks","text":"<pre><code>model.set_stop_words([\"\\nObservation:\"])\n\n# Hides &lt;think&gt;...&lt;/think&gt; and similar reasoning tags\nmodel.strip_reasoning = True\n\ncompletion = model.ask(\"Return thought+answer with explicit markers.\")\nprint(completion.choices[0].message.content)\n</code></pre> <p>Allowed passthrough parameters (forwarded directly to the OpenAI API) include <code>top_p</code>, <code>presence_penalty</code>, <code>frequency_penalty</code>, <code>logit_bias</code>, <code>user</code>, <code>n</code>, <code>response_format</code>, <code>seed</code>. Provide them as keyword arguments in the <code>ask</code> call.</p>"},{"location":"api-reference/models/chat-models/openai-model/#error-handling","title":"Error handling","text":"<p>API errors from <code>openai</code> (connection issues, rate limits, API status errors) are caught and returned as textual messages inside the response body so that your application can surface them without crashing. Enable <code>verbose=True</code> to emit detailed stack traces to logs.</p>"},{"location":"api-reference/models/chat-models/openai-model/#environment-variables","title":"Environment variables","text":"<ul> <li><code>OPENAI_API_KEY</code> \u2013 default API key used when <code>api_key</code> is not supplied.</li> <li>Set <code>OPENAI_BASE_URL</code> alongside <code>base_url</code> in infrastructure tooling if you proxy requests.</li> </ul> <p>Example <code>.env</code> snippet:</p> <pre><code>OPENAI_API_KEY=sk-your-key\nOPENAI_MODEL_NAME=gpt-4o-mini\n</code></pre>"},{"location":"api-reference/models/chat-models/openai-model/#related-apis","title":"Related APIs","text":"<ul> <li><code>BaseModel</code> \u2013 shared helpers and lifecycle hooks</li> <li><code>TransformersModel</code> \u2013 local Hugging Face backend</li> </ul> <p>mkdocstrings output is temporarily disabled while import hooks are updated.</p>"},{"location":"api-reference/models/chat-models/transformers-model/","title":"TransformersModel","text":"<p>Module: <code>neurosurfer.models.chat_models.transformers</code> Inherits: <code>BaseModel</code></p>"},{"location":"api-reference/models/chat-models/transformers-model/#overview","title":"Overview","text":"<p><code>TransformersModel</code> wraps any Hugging Face causal language model for local inference. It handles device placement, optional 4-bit quantisation via <code>bitsandbytes</code>, stop word filtering, and streaming support through <code>TextIteratorStreamer</code>.</p>"},{"location":"api-reference/models/chat-models/transformers-model/#highlights","title":"Highlights","text":"<ul> <li>Works with local checkpoints or remote Hub ids (<code>trust_remote_code=True</code>)</li> <li>Auto-selects GPU (<code>cuda</code>) when available, otherwise falls back to CPU</li> <li>Detects pre-quantised models to avoid double-quantising</li> <li>Supports <code>&lt;think&gt;</code> suppression for Qwen/Qwen2 style models when <code>enable_thinking=False</code></li> <li>Exposes graceful cancellation through <code>stop_generation</code></li> </ul>"},{"location":"api-reference/models/chat-models/transformers-model/#constructor","title":"Constructor","text":""},{"location":"api-reference/models/chat-models/transformers-model/#transformersmodel__init__","title":"<code>TransformersModel.__init__</code>","text":"<pre><code>from neurosurfer.config import config\n\nTransformersModel(\n    model_name: str = \"openai/gpt-oss-20b\",\n    *,\n    max_seq_length: int = config.base_model.max_seq_length,\n    load_in_4bit: bool = config.base_model.load_in_4bit,\n    enable_thinking: bool = config.base_model.enable_thinking,\n    stop_words: list[str] | None = config.base_model.stop_words,\n    verbose: bool = config.base_model.verbose,\n    logger: logging.Logger = logging.getLogger(),\n    **kwargs,\n)\n</code></pre> Parameter Type Default Description <code>model_name</code> <code>str</code> <code>\"openai/gpt-oss-20b\"</code> Hugging Face id or local path passed to <code>from_pretrained</code>. <code>max_seq_length</code> <code>int</code> <code>config.base_model.max_seq_length</code> Logical context window (recorded on the instance). <code>load_in_4bit</code> <code>bool</code> <code>config.base_model.load_in_4bit</code> Enable 4-bit quantisation via <code>BitsAndBytesConfig</code> if the model is not already quantised. <code>enable_thinking</code> <code>bool</code> <code>config.base_model.enable_thinking</code> When <code>False</code>, <code>&lt;think&gt;</code> sections are stripped and Qwen-style prompts append <code>/nothink</code>. <code>stop_words</code> <code>list[str] \\| None</code> <code>config.base_model.stop_words</code> Optional stop sequence list enforced client-side. <code>verbose</code> <code>bool</code> <code>config.base_model.verbose</code> Emit additional logging during model load and streaming. <code>logger</code> <code>logging.Logger</code> <code>logging.getLogger()</code> Logger shared with helper methods. <code>**kwargs</code> any \u2013 Forwarded to <code>AutoModelForCausalLM.generate</code>. <p>During initialisation:</p> <ol> <li>Device (<code>cuda</code> vs <code>cpu</code>) and dtype are selected automatically.</li> <li><code>AutoTokenizer.from_pretrained</code> and <code>AutoModelForCausalLM.from_pretrained</code> are called with <code>trust_remote_code=True</code>.</li> <li>If <code>load_in_4bit</code> is <code>True</code>, a <code>BitsAndBytesConfig</code> is attached unless the checkpoint already contains <code>quantization_config</code>.</li> </ol> <p>Tip</p> <p><code>config</code> is imported from <code>neurosurfer.config</code>. Please see the configuration section for more details.</p>"},{"location":"api-reference/models/chat-models/transformers-model/#usage","title":"Usage","text":""},{"location":"api-reference/models/chat-models/transformers-model/#basic-completion","title":"Basic completion","text":"<pre><code>from neurosurfer.models.chat_models.transformers import TransformersModel\n\nmodel = TransformersModel(\n    model_name=\"/home/nomi/workspace/Model_Weights/Qwen3-4B-unsloth-bnb-4bit\",\n    load_in_4bit=False,  # already quantised\n)\n\nreply = model.ask(\"Summarise the features of Neurosurfer in two sentences.\")\nprint(reply.choices[0].message.content)\n</code></pre>"},{"location":"api-reference/models/chat-models/transformers-model/#streaming-completion","title":"Streaming completion","text":"<pre><code>stream = model.ask(\n    \"Provide three bullet points about vector databases.\",\n    system_prompt=\"Answer as a helpful engineer.\",\n    stream=True,\n    temperature=0.3,\n)\n\nfor chunk in stream:\n    delta = chunk.choices[0].delta.content or \"\"\n    print(delta, end=\"\")\n</code></pre>"},{"location":"api-reference/models/chat-models/transformers-model/#cancelling-generation","title":"Cancelling generation","text":"<pre><code>stream = model.ask(\"Write a very long story about transformers.\", stream=True)\n\nfor chunk in stream:\n    if should_cancel_now():\n        model.stop_generation()\n        break\n    print(chunk.choices[0].delta.content, end=\"\")\n</code></pre>"},{"location":"api-reference/models/chat-models/transformers-model/#tips","title":"Tips","text":"<ul> <li>Quantised checkpoints: For weights already saved in 4-bit/8-bit format, leave <code>load_in_4bit=False</code>. The loader inspects <code>config.json</code> to avoid double quantisation.</li> <li>Thinking tags: Qwen/Qwen2 models output <code>&lt;think&gt;...&lt;/think&gt;</code> content. Set <code>enable_thinking=False</code> (default) to strip it; set <code>True</code> to keep the reasoning trace.</li> <li>Custom kwargs: Pass <code>top_p</code>, <code>top_k</code>, <code>repetition_penalty</code>, etc. directly to <code>ask</code> and they propagate to <code>generate</code>.</li> <li>Stop words: Use <code>model.set_stop_words([\"Observation:\", \"\\nResult:\"])</code> to truncate outputs before those markers.</li> </ul>"},{"location":"api-reference/models/chat-models/transformers-model/#related-models","title":"Related models","text":"<ul> <li><code>UnslothModel</code> \u2013 similar API built on Unsloth's <code>FastLanguageModel</code></li> <li><code>LlamaCppModel</code> \u2013 run GGUF weights via llama.cpp</li> </ul> <p>mkdocstrings output is temporarily disabled while import hooks are updated.</p>"},{"location":"api-reference/models/chat-models/unsloth-model/","title":"UnslothModel","text":"<p>Module: <code>neurosurfer.models.chat_models.unsloth</code> Inherits: <code>BaseModel</code></p>"},{"location":"api-reference/models/chat-models/unsloth-model/#overview","title":"Overview","text":"<p><code>UnslothModel</code> integrates Unsloth's <code>FastLanguageModel</code> runtime, giving you accelerated inference for LoRA/QLoRA checkpoints without rewriting application code. It mirrors the <code>TransformersModel</code> interface but leans on Unsloth's optimisations for NVIDIA GPUs.</p>"},{"location":"api-reference/models/chat-models/unsloth-model/#highlights","title":"Highlights","text":"<ul> <li>Optimised CUDA kernels with optional 4-bit/8-bit quantisation</li> <li>Thread-safe streaming with stop signal support</li> <li>Optional <code>&lt;think&gt;</code> suppression for Qwen-style models</li> <li>Compatible with checkpoints produced by the Unsloth finetuning workflow</li> </ul>"},{"location":"api-reference/models/chat-models/unsloth-model/#constructor","title":"Constructor","text":""},{"location":"api-reference/models/chat-models/unsloth-model/#unslothmodel__init__","title":"<code>UnslothModel.__init__</code>","text":"<pre><code>from neurosurfer.config import config\n\nUnslothModel(\n    model_name: str,\n    *,\n    max_seq_length: int = config.base_model.max_seq_length,\n    load_in_4bit: bool = config.base_model.load_in_4bit,\n    load_in_8bit: bool = False,\n    full_finetuning: bool = config.base_model.full_finetuning,\n    enable_thinking: bool = config.base_model.enable_thinking,\n    stop_words: list[str] | None = config.base_model.stop_words,\n    verbose: bool = config.base_model.verbose,\n    logger: logging.Logger = logging.getLogger(),\n    **kwargs,\n)\n</code></pre> Parameter Type Default Description <code>model_name</code> <code>str</code> \u2013 Local path or Hugging Face id recognised by <code>FastLanguageModel</code>. <code>max_seq_length</code> <code>int</code> <code>config.base_model.max_seq_length</code> Context window passed to Unsloth; must align with how the model was trained. <code>load_in_4bit</code> <code>bool</code> <code>config.base_model.load_in_4bit</code> Load weights in 4-bit mode for memory savings. <code>load_in_8bit</code> <code>bool</code> <code>False</code> Load weights in 8-bit mode instead of 4-bit. <code>full_finetuning</code> <code>bool</code> <code>config.base_model.full_finetuning</code> Enable full-parameter finetuning mode (not needed for inference). <code>enable_thinking</code> <code>bool</code> <code>config.base_model.enable_thinking</code> Keep <code>&lt;think&gt;</code> reasoning traces instead of stripping them. <code>stop_words</code> <code>list[str] or None</code> <code>config.base_model.stop_words</code> Optional stop sequence list enforced client-side. <code>verbose</code> <code>bool</code> <code>config.base_model.verbose</code> Emit additional logs from the wrapper. <code>logger</code> <code>logging.Logger</code> <code>logging.getLogger()</code> Logger shared across helper methods. <p>Additional keyword arguments are forwarded to <code>FastLanguageModel.from_pretrained</code>.</p> <p>Tip</p> <p><code>config</code> is imported from <code>neurosurfer.config</code>. Please see the configuration section for more details.</p>"},{"location":"api-reference/models/chat-models/unsloth-model/#usage","title":"Usage","text":""},{"location":"api-reference/models/chat-models/unsloth-model/#non-streaming-reply","title":"Non-streaming reply","text":"<pre><code>from neurosurfer.models.chat_models.unsloth import UnslothModel\n\nmodel = UnslothModel(\n    model_name=\"/weights/Qwen2.5-7B-Instruct-bnb-4bit\",\n    enable_thinking=False,\n)\n\nresponse = model.ask(\"Summarise the README in two bullet points.\")\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"api-reference/models/chat-models/unsloth-model/#streaming-with-stop-control","title":"Streaming with stop control","text":"<pre><code>stream = model.ask(\n    \"List three benefits of Unsloth.\",\n    stream=True,\n    temperature=0.6,\n)\n\nfor chunk in stream:\n    delta = chunk.choices[0].delta.content or \"\"\n    print(delta, end=\"\")\n    if \"&lt;END&gt;\" in delta:\n        model.stop_generation()\n</code></pre>"},{"location":"api-reference/models/chat-models/unsloth-model/#updating-stop-words-at-runtime","title":"Updating stop words at runtime","text":"<pre><code>model.set_stop_words([\"\\nObservation:\"])\nreply = model.ask(\"Respond using the format 'Answer: ...' and 'Observation: ...'.\")\nprint(reply.choices[0].message.content)\n</code></pre>"},{"location":"api-reference/models/chat-models/unsloth-model/#notes","title":"Notes","text":"<ul> <li>Unsloth currently targets CUDA; running on CPU is not supported.</li> <li>The wrapper uses a background thread plus <code>TextIteratorStreamer</code> to support streaming and stop signals\u2014call <code>stop_generation()</code> to cancel long generations.</li> <li>Token counts are approximated via the tokenizer when possible; failures fall back to a word-count heuristic.</li> </ul>"},{"location":"api-reference/models/chat-models/unsloth-model/#related-models","title":"Related models","text":"<ul> <li><code>TransformersModel</code> \u2013 direct Hugging Face integration</li> <li><code>LlamaCppModel</code> \u2013 run GGUF weights via llama.cpp</li> </ul> <p>mkdocstrings output is temporarily disabled while import hooks are updated.</p>"},{"location":"api-reference/models/embedders/","title":"Embedders API","text":"<p>Embedding backends turn text into dense vectors used by retrieval, clustering, and semantic search components.</p>"},{"location":"api-reference/models/embedders/#available-embedders","title":"Available embedders","text":"Embedder Backend Ideal for <code>SentenceTransformerEmbedder</code> <code>sentence-transformers</code> High-quality multilingual embeddings with optional 8-bit quantisation <code>LlamaCppEmbedder</code> <code>llama-cpp-python</code> Deployments that already rely on GGUF models and need fully offline embeddings"},{"location":"api-reference/models/embedders/#quick-start","title":"Quick start","text":""},{"location":"api-reference/models/embedders/#sentencetransformer","title":"SentenceTransformer","text":"<pre><code>from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n\nembedder = SentenceTransformerEmbedder(model_name=\"intfloat/e5-large-v2\")\n\nquery_vector = embedder.embed(\"What is retrieval augmented generation?\")\ndoc_vectors = embedder.embed([\n    \"Neurosurfer exposes an OpenAI-compatible API.\",\n    \"Agents can run on top of multiple model backends.\",\n])\n</code></pre>"},{"location":"api-reference/models/embedders/#llamacpp","title":"LlamaCpp","text":"<pre><code>from neurosurfer.models.embedders.llamacpp import LlamaCppEmbedder\n\nconfig = {\n    \"model_path\": \"/weights/nomic-embed-text-v1.5.Q4_K_M.gguf\",\n    \"n_threads\": 8,\n    \"embedding\": True,\n}\n\nembedder = LlamaCppEmbedder(config=config)\nvector = embedder.embed(\"Offline embeddings with llama.cpp are convenient.\")\n</code></pre>"},{"location":"api-reference/models/embedders/#choosing-an-embedder","title":"Choosing an embedder","text":"<ul> <li>Pick SentenceTransformerEmbedder when you prioritise accuracy and have access to the Hugging Face ecosystem.</li> <li>Pick LlamaCppEmbedder when you need an entirely local stack and already distribute GGUF models alongside chat backends.</li> </ul>"},{"location":"api-reference/models/embedders/#see-also","title":"See also","text":"<ul> <li>BaseEmbedder</li> <li>RAG system</li> <li>Vector stores</li> <li>Chat models</li> </ul>"},{"location":"api-reference/models/embedders/base-embedder/","title":"BaseEmbedder","text":"<p>Module: <code>neurosurfer.models.embedders.base</code> Type: Abstract base class</p>"},{"location":"api-reference/models/embedders/base-embedder/#overview","title":"Overview","text":"<p><code>BaseEmbedder</code> defines the contract for embedding backends in Neurosurfer. All embedder implementations inherit from this class so they can be passed interchangeably to RAG pipelines, vector stores, or custom components.</p>"},{"location":"api-reference/models/embedders/base-embedder/#responsibilities","title":"Responsibilities","text":"<ul> <li>Manage a shared <code>logger</code> instance</li> <li>Load any backend-specific model inside the subclass constructor</li> <li>Implement a single <code>embed(query, **kwargs)</code> method that accepts either one string or a list of strings and returns vectors as Python lists</li> </ul>"},{"location":"api-reference/models/embedders/base-embedder/#creating-a-custom-embedder","title":"Creating a custom embedder","text":"<pre><code>from typing import List, Union\nfrom neurosurfer.models.embedders.base import BaseEmbedder\n\nclass MyEmbedder(BaseEmbedder):\n    def __init__(self):\n        super().__init__()\n        self.model = load_something()\n\n    def embed(self, query: Union[str, List[str]], **kwargs) -&gt; Union[List[float], List[List[float]]]:\n        if isinstance(query, list):\n            return [self._encode(text) for text in query]\n        return self._encode(query)\n\n    def _encode(self, text: str) -&gt; List[float]:\n        return self.model.encode(text).tolist()\n</code></pre> <p>Return plain Python lists so downstream code can serialise or cast to <code>numpy</code>/<code>torch</code> as needed.</p>"},{"location":"api-reference/models/embedders/base-embedder/#see-also","title":"See also","text":"<ul> <li><code>SentenceTransformerEmbedder</code></li> <li><code>LlamaCppEmbedder</code></li> </ul> <p>mkdocstrings output is temporarily disabled while import hooks are updated.</p>"},{"location":"api-reference/models/embedders/llamacpp-embedder/","title":"LlamaCppEmbedder","text":"<p>Module: <code>neurosurfer.models.embedders.llamacpp</code> Inherits: <code>BaseEmbedder</code></p>"},{"location":"api-reference/models/embedders/llamacpp-embedder/#overview","title":"Overview","text":"<p><code>LlamaCppEmbedder</code> produces embeddings using the <code>llama-cpp-python</code> bindings. It is ideal when you already host GGUF models for chat and want embeddings from the same runtime without depending on external APIs.</p>"},{"location":"api-reference/models/embedders/llamacpp-embedder/#constructor","title":"Constructor","text":""},{"location":"api-reference/models/embedders/llamacpp-embedder/#llamacppembedder__init__","title":"<code>LlamaCppEmbedder.__init__</code>","text":"<pre><code>LlamaCppEmbedder(\n    config: dict,\n    *,\n    logger: logging.Logger | None = None,\n)\n</code></pre> <p>The <code>config</code> dictionary is passed directly to <code>llama_cpp.Llama</code>. Typical keys include:</p> <pre><code>config = {\n    \"model_path\": \"/weights/nomic-embed-text-v1.5.Q4_K_M.gguf\",\n    \"n_threads\": 8,\n    \"n_gpu_layers\": 0,\n    \"main_gpu\": 0,\n    \"embedding\": True,  # required\n}\n</code></pre> <p><code>embedding=True</code> must be set so llama.cpp exposes the embedding endpoint. All other llama.cpp parameters (batch size, rope scaling, etc.) are also supported.</p>"},{"location":"api-reference/models/embedders/llamacpp-embedder/#embedding-api","title":"Embedding API","text":"<pre><code>from neurosurfer.models.embedders.llamacpp import LlamaCppEmbedder\n\nembedder = LlamaCppEmbedder(config=config)\n\nvector = embedder.embed(\"What is retrieval augmented generation?\")\nprint(len(vector))\n\nvectors = embedder.embed([\n    \"Neurosurfer ships a FastAPI server.\",\n    \"Agents reuse OpenAI-compatible responses.\",\n])\nprint(len(vectors), len(vectors[0]))\n</code></pre>"},{"location":"api-reference/models/embedders/llamacpp-embedder/#signature","title":"Signature","text":"<pre><code>embed(\n    query: str | list[str],\n    *,\n    normalize_embeddings: bool = True,\n) -&gt; list[float] | list[list[float]]\n</code></pre> <ul> <li>When <code>normalize_embeddings=True</code>, each vector is L2-normalised.</li> <li>Passing a list returns a list of vectors; no separate <code>embed_batch</code> method is needed.</li> </ul>"},{"location":"api-reference/models/embedders/llamacpp-embedder/#tips","title":"Tips","text":"<ul> <li>GGUF embedding checkpoints such as <code>nomic-embed-text-v1.5</code> are readily available from Hugging Face (<code>TheBloke</code> maintains many quantised variants).</li> <li>Increase <code>n_threads</code> for CPU-bound workloads; when GPU acceleration is available, set <code>n_gpu_layers</code> to offload part of the encoder.</li> <li>The wrapper returns native Python lists. Convert to <code>numpy</code>/<code>torch</code> as needed: <code>np.array(embedder.embed(texts))</code>.</li> </ul>"},{"location":"api-reference/models/embedders/llamacpp-embedder/#related-embedders","title":"Related embedders","text":"<ul> <li><code>SentenceTransformerEmbedder</code> \u2013 higher-quality, transformer-based embeddings.</li> </ul> <p>mkdocstrings output is temporarily disabled while import hooks are updated.</p>"},{"location":"api-reference/models/embedders/sentence-transformer/","title":"SentenceTransformerEmbedder","text":"<p>Module: <code>neurosurfer.models.embedders.sentence_transformer</code> Inherits: <code>BaseEmbedder</code></p>"},{"location":"api-reference/models/embedders/sentence-transformer/#overview","title":"Overview","text":"<p>Wraps the <code>sentence-transformers</code> library to generate dense vectors for retrieval, clustering, or semantic search. The embedder optionally loads models with 8-bit quantisation using <code>BitsAndBytesConfig</code> for reduced memory consumption.</p>"},{"location":"api-reference/models/embedders/sentence-transformer/#constructor","title":"Constructor","text":""},{"location":"api-reference/models/embedders/sentence-transformer/#sentencetransformerembedder__init__","title":"<code>SentenceTransformerEmbedder.__init__</code>","text":"<pre><code>SentenceTransformerEmbedder(\n    model_name: str,\n    *,\n    logger: logging.Logger | None = None,\n    quantized: bool = True,\n)\n</code></pre> Parameter Type Default Description <code>model_name</code> <code>str</code> \u2013 Hugging Face id or local path recognised by <code>sentence-transformers</code>. <code>logger</code> <code>logging.Logger \\| None</code> <code>None</code> Custom logger; falls back to a module-level logger. <code>quantized</code> <code>bool</code> <code>True</code> When <code>True</code>, wraps the Transformer with <code>BitsAndBytesConfig(load_in_8bit=True)</code> to cut memory usage. <p>When <code>quantized=True</code>, the embedder builds a custom pipeline consisting of a quantised transformer block and a pooling layer. Otherwise it loads the plain <code>SentenceTransformer</code>.</p>"},{"location":"api-reference/models/embedders/sentence-transformer/#embedding-text","title":"Embedding text","text":"<pre><code>from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n\nembedder = SentenceTransformerEmbedder(model_name=\"intfloat/e5-large-v2\")\n\n# Single text \u2192 list[float]\nquery_vector = embedder.embed(\"What is retrieval augmented generation?\")\n\n# Multiple texts \u2192 list[list[float]]\ndoc_vectors = embedder.embed([\n    \"Neurosurfer ships a FastAPI server.\",\n    \"Agents rely on OpenAI-style responses.\",\n])\n</code></pre>"},{"location":"api-reference/models/embedders/sentence-transformer/#method-signature","title":"Method signature","text":"<pre><code>embed(\n    query: str | list[str],\n    *,\n    convert_to_tensor: bool = False,\n    normalize_embeddings: bool = True,\n) -&gt; list[float] | list[list[float]]\n</code></pre> <ul> <li>Set <code>convert_to_tensor=True</code> to receive <code>torch.Tensor</code> objects (useful when chaining with PyTorch operations).</li> <li>Disable <code>normalize_embeddings</code> if you need raw vectors instead of unit-normalised outputs. By default it normalises to make cosine similarity equivalent to dot product.</li> </ul>"},{"location":"api-reference/models/embedders/sentence-transformer/#tips","title":"Tips","text":"<ul> <li>Quantised models run efficiently on commodity GPUs; disable quantisation for CPU-only environments or when you require the original precision.</li> <li>Sentence-transformer checkpoints sometimes require authentication (e.g. private models). Run <code>huggingface-cli login</code> if necessary.</li> <li>Combine with <code>neurosurfer.vectorstores.ChromaVectorStore</code> or other vector stores through the shared <code>BaseEmbedder</code> API.</li> </ul>"},{"location":"api-reference/models/embedders/sentence-transformer/#related-embedders","title":"Related embedders","text":"<ul> <li><code>LlamaCppEmbedder</code> \u2013 build embeddings with llama.cpp-based GGUF models.</li> </ul> <p>mkdocstrings output is temporarily disabled while import hooks are updated.</p>"},{"location":"api-reference/rag/","title":"RAG System API","text":"<p>Neurosurfer\u2019s Retrieval-Augmented Generation (RAG) system turns heterogeneous sources (files, directories, raw text, URLs, ZIPs) into chunk-level embeddings that your LLM can ground on during inference. It is modular, safe, and extensible:</p> <ul> <li>FileReader \u2013 normalizes inputs by detecting type/extension and returning UTF-8 text for supported formats; skips binaries and common build/VCS artifacts.</li> <li>Chunker \u2013 converts long text into context-preserving chunks using structure-aware strategies (e.g., Python AST, Markdown headers, JSON objects) with configurable line/character fallbacks and overlap.</li> <li>RAGIngestor \u2013 orchestrates: queue \u2192 chunk (parallel) \u2192 dedupe (content hash) \u2192 batch-embed \u2192 upsert to vector store, preserving metadata (filename, url, source_type, content_hash) and supporting progress callbacks &amp; cancellation.</li> </ul>"},{"location":"api-reference/rag/#components","title":"Components","text":"<ul> <li> <p> Chunker</p> <p>Split text into context\u2011preserving chunks (AST/structure\u2011aware where possible)</p> <p> Documentation</p> </li> <li> <p> FileReader</p> <p>Read various file formats into normalized UTF\u20118 text</p> <p> Documentation</p> </li> <li> <p> RAGIngestor</p> <p>Queue sources \u2192 chunk \u2192 batch\u2011embed \u2192 dedupe \u2192 persist to vector store</p> <p> Documentation</p> </li> </ul>"},{"location":"api-reference/rag/#minimal-usage-examples","title":"Minimal Usage Examples","text":""},{"location":"api-reference/rag/#chunker","title":"Chunker","text":"<pre><code>from neurosurfer.rag.chunker import Chunker\n\ntext = \"\"\"# Title\n\nParagraph one.\n\n## Section\nMore text here.\"\"\"\n\nchunks = Chunker().chunk(text, file_path=\"notes.md\")\nprint(len(chunks), \"chunks\")  # e.g., 2\u20135 depending on config\n</code></pre>"},{"location":"api-reference/rag/#filereader","title":"FileReader","text":"<pre><code>from neurosurfer.rag.filereader import FileReader\n\nreader = FileReader()\ntxt = reader.read(\"./document.pdf\")   # returns extracted text (or \"\")\nprint(txt[:200])\n</code></pre>"},{"location":"api-reference/rag/#ragingestor","title":"RAGIngestor","text":"<pre><code>from neurosurfer.rag.ingestor import RAGIngestor\nfrom neurosurfer.rag.chunker import Chunker\nfrom neurosurfer.rag.filereader import FileReader\nfrom neurosurfer.vectorstores.chroma import ChromaVectorStore\nfrom neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n\nembedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\nvectorstore = ChromaVectorStore(collection_name=\"docs\")\n\ningestor = RAGIngestor(embedder=embedder, vector_store=vectorstore, chunker=Chunker(), file_reader=FileReader())\ningestor.add_files([\"./document.pdf\", \"./notes.md\"]).build()\n\n# Quick retrieval smoke test\nfor doc, score in ingestor.search(\"ingestion pipeline\", top_k=3):\n    print(f\"{score:.3f} | {doc.metadata.get('filename', doc.id)}\")\n</code></pre>"},{"location":"api-reference/rag/chunker/","title":"Chunker","text":"<p>Module: <code>neurosurfer.rag.chunker</code></p> <p>A production\u2011grade, extensible document chunker purpose\u2011built for RAG (Retrieval\u2011Augmented Generation) systems. It supports AST\u2011aware Python chunking, structure\u2011aware JS/TS chunking, header\u2011aware Markdown chunking, JSON object/array chunking, and robust fallbacks (line\u2011based for code\u2011like, char\u2011based for prose). It also includes custom handler registration, a router for dynamic strategy selection, comment\u2011aware filtering, prompt\u2011block stripping, and safety caps to prevent pathological outputs from custom logic.</p>"},{"location":"api-reference/rag/chunker/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Strategy registry by file extension (built\u2011in + custom)  </li> <li>Built\u2011ins: <code>.py</code>, <code>.js</code>, <code>.ts</code>, <code>.tsx</code>, <code>.jsx</code>, <code>.json</code>, <code>.md</code>, <code>.txt</code></li> <li>AST\u2011aware Python chunking: groups imports / defs / classes (with decorators) and then windows large blocks</li> <li>JS/TS structure\u2011aware chunking: coarse segmentation by <code>function</code>/<code>class</code> followed by cleanup</li> <li>Markdown/README header\u2011aware chunking: windows by headings/max lines</li> <li>JSON chunking: per top\u2011level object/array element with size caps; falls back to char windows if needed</li> <li>Fallback heuristics: <code>\"auto\"</code> detection prefers line\u2011based windows for code\u2011like text, char\u2011based for prose</li> <li>Comment\u2011aware filtering: collapses long comment blocks, skips fully\u2011commented chunks</li> <li>Prompt\u2011block filtering (triple\u2011quoted strings that look like prompts), reducing retrieval poisoning</li> <li>Custom handler system: register callable handlers by name and bind them to extensions or a router</li> <li>Safety limits: cap chunk counts and total output characters from custom handlers</li> <li>Blacklist skip: ignore common binary/config/infra paths early</li> <li>Optional logging hook</li> </ul>"},{"location":"api-reference/rag/chunker/#configuration-chunkerconfig","title":"Configuration (<code>ChunkerConfig</code>)","text":"<p><code>ChunkerConfig</code> centralizes chunk sizes, overlaps, fallbacks, and safety limits.</p> Name Type Default Description <code>fallback_chunk_size</code> <code>int</code> <code>25</code> Line-based window size (lines) for generic code splitting and Python/JS sub\u2011chunking. <code>overlap_lines</code> <code>int</code> <code>3</code> Line overlap between consecutive windows (context retention). <code>max_chunk_lines</code> <code>int</code> <code>1000</code> Hard cap on lines per chunk to avoid giant blocks. <code>comment_block_threshold</code> <code>int</code> <code>4</code> Consecutive comment\u2011only lines at/above this threshold form a \u201ccomment block\u201d that can be dropped. <code>char_chunk_size</code> <code>int</code> <code>1000</code> Char-based window size (chars) for prose/unknown text. <code>char_overlap</code> <code>int</code> <code>150</code> Char overlap between consecutive char windows. <code>readme_max_lines</code> <code>int</code> <code>30</code> Max lines per Markdown/README chunk. <code>json_chunk_size</code> <code>int</code> <code>1000</code> Max chars per JSON sub\u2011chunk when pretty\u2011printing. <code>fallback_mode</code> <code>str</code> <code>\"char\"</code> What to do when no strategy: <code>\"char\"</code>, <code>\"line\"</code>, or <code>\"auto\"</code> (code\u2011like \u2192 line; else char). <code>max_returned_chunks</code> <code>int</code> <code>500</code> Hard limit on number of chunks returned by a custom handler (post\u2011sanitize). <code>max_total_output_chars</code> <code>int</code> <code>1_000_000</code> Hard limit on total characters returned by a custom handler (post\u2011sanitize). <code>min_chunk_non_ws_chars</code> <code>int</code> <code>1</code> Drop chunks that have fewer than this many non\u2011whitespace characters. <p>Example</p> <pre><code>from neurosurfer.rag.chunker import Chunker, ChunkerConfig\n\nconfig = ChunkerConfig(\n    fallback_chunk_size=30,\n    overlap_lines=5,\n    char_chunk_size=1000,\n    comment_block_threshold=4,\n    fallback_mode=\"auto\"\n)\nchunker = Chunker(config)\n</code></pre>"},{"location":"api-reference/rag/chunker/#blacklist-skip-rules","title":"Blacklist / Skip Rules","text":"<p><code>Chunker</code> avoids ingesting common non\u2011text or infra files via <code>_should_skip_file(file_path)</code> which tests against these regular\u2011expression patterns (examples):  </p> <pre><code>*.lock, .env*, .git*, node_modules, __pycache__, .DS_Store, Thumbs.db,\n*.png, *.jpg, *.svg, *.ico, *.zip, *.tar.gz, *.mp4, *.mp3,\n/dist/, /build/, .idea, .vscode, .eslintrc, .prettierrc, .editorconfig,\n.gitignore, /LICENSE, /CODEOWNERS, /CONTRIBUTING.md, /CHANGELOG.md\n</code></pre> <p>If a path matches any blacklist regex, <code>chunk(...)</code> returns <code>[]</code> immediately.</p>"},{"location":"api-reference/rag/chunker/#public-api","title":"Public API","text":""},{"location":"api-reference/rag/chunker/#class-chunkerconfig-chunkerconfig-chunkerconfig","title":"<code>class Chunker(config: ChunkerConfig = ChunkerConfig())</code>","text":""},{"location":"api-reference/rag/chunker/#builtin-strategy-registration","title":"Built\u2011in strategy registration","text":"<ul> <li><code>.register(exts: List[str], fn: StrategyFn) -&gt; None</code>   Map file extensions to a strategy function: <code>fn(text: str, file_path: Optional[str]) -&gt; List[str]</code>   Built\u2011ins are pre\u2011registered for:  </li> <li><code>.py</code> \u2192 <code>_chunk_python</code> (AST\u2011aware + line windows)  </li> <li><code>.js</code>, <code>.ts</code>, <code>.tsx</code>, <code>.jsx</code> \u2192 <code>_chunk_javascript</code> </li> <li><code>.json</code> \u2192 <code>_chunk_json</code> </li> <li><code>.md</code>, <code>.txt</code> \u2192 <code>_chunk_readme</code></li> </ul>"},{"location":"api-reference/rag/chunker/#custom-handler-registry","title":"Custom handler registry","text":"<ul> <li> <p><code>.register_custom(name: str, handler: CustomChunkHandler) -&gt; None</code>   Registers a named handler. The <code>CustomChunkHandler</code> signature is: <code>handler(text: str, *, file_path: Optional[str] = None, config: Optional[ChunkerConfig] = None) -&gt; List[str]</code></p> </li> <li> <p><code>.unregister_custom(name: str) -&gt; None</code>   Removes the handler and any extension mappings pointing to it.</p> </li> <li> <p><code>.list_custom_handlers() -&gt; List[str]</code>   Names of all registered custom handlers.</p> </li> <li> <p><code>.use_custom_for_ext(exts: List[str], handler_name: str) -&gt; None</code>   Route specific extensions to a named custom handler.</p> </li> <li> <p><code>.clear_custom_for_ext(exts: List[str]) -&gt; None</code>   Remove previous extension mappings.</p> </li> <li> <p><code>.set_router(router: Optional[Callable[[Optional[str], str], Optional[str]]]) -&gt; None</code>   Install a router invoked as <code>router(file_path, text) -&gt; handler_name | None</code>. If it returns the name of a registered handler, that handler is used.</p> </li> <li> <p><code>.list_ext_mappings() -&gt; List[Tuple[str, str]]</code>   Returns current extension \u2192 custom handler mappings.</p> </li> </ul>"},{"location":"api-reference/rag/chunker/#logging-optional","title":"Logging (optional)","text":"<ul> <li><code>.set_logger(logger_fn: Callable[[str], None]) -&gt; None</code>   Attach a logger callback. Internal helpers route info/warn/error strings to this function.</li> </ul>"},{"location":"api-reference/rag/chunker/#main-entry-point","title":"Main entry point","text":"<ul> <li><code>.chunk(text: str, *, source_id: str | None = None, file_path: str | None = None, k: int = 40, custom: str | CustomChunkHandler | None = None) -&gt; List[str]</code></li> </ul> <p>Priority order used by <code>chunk(...)</code>:</p> <ol> <li>Explicit <code>custom</code> handler (string name or callable)  </li> <li>Router result (registered handler name)  </li> <li>Extension mapping (registered handler name)  </li> <li>Built\u2011in strategy for the extension  </li> <li>Heuristic fallback: line windows if <code>_looks_like_code(text)</code> else char windows</li> </ol> <p>Additional rules:</p> <ul> <li>If <code>file_path</code> matches the blacklist, returns <code>[]</code>.</li> <li>If <code>text</code> word count <code>&lt;= k</code>, returns the whole text as a single chunk (if it meets <code>min_chunk_non_ws_chars</code>).</li> </ul>"},{"location":"api-reference/rag/chunker/#strategy-details","title":"Strategy Details","text":""},{"location":"api-reference/rag/chunker/#python-_chunk_python","title":"Python (<code>_chunk_python</code>)","text":"<ul> <li>Strips prompt\u2011like triple\u2011quoted blocks first (<code>_filter_prompt_like_blocks</code>).</li> <li>Parses AST; for each top\u2011level node (imports, defs/classes incl. decorators), collects its line range.</li> <li>Sub\u2011chunks large blocks by line windows with cleanup: <code>_split_into_chunks()</code> \u2192 <code>clean_chunk_lines()</code></li> <li>Skips fully\u2011commented windows.</li> <li>Appends any remaining lines (non\u2011AST or trailing comments) via the same windowing logic.</li> <li>Fallback to <code>_line_windows</code> on parse failure.</li> </ul>"},{"location":"api-reference/rag/chunker/#javascripttypescript-_chunk_javascript","title":"JavaScript/TypeScript (<code>_chunk_javascript</code>)","text":"<ul> <li>Coarse split by <code>function ... {</code> or <code>class ... {</code> occurrences.</li> <li>Cleans each segment via <code>_clean_lines</code> and skips fully\u2011commented blocks.</li> <li>Falls back to <code>_line_windows</code> if no structural matches found.</li> </ul>"},{"location":"api-reference/rag/chunker/#json-_chunk_json","title":"JSON (<code>_chunk_json</code>)","text":"<ul> <li>Attempts <code>json.loads(text)</code>:</li> <li>dict: chunk each <code>{\"key\": value}</code> pretty\u2011printed, truncating by <code>json_chunk_size</code>.</li> <li>list: chunk each element pretty\u2011printed, capped by <code>json_chunk_size</code>.</li> <li>On parse error, falls back to <code>_char_windows(text, json_chunk_size, json_chunk_size * 0.2)</code>.</li> </ul>"},{"location":"api-reference/rag/chunker/#markdownreadme-_chunk_readme","title":"Markdown/README (<code>_chunk_readme</code>)","text":"<ul> <li>Windows by headings and <code>readme_max_lines</code>, ensuring manageable slices for embeddings.</li> </ul>"},{"location":"api-reference/rag/chunker/#fallbacks","title":"Fallbacks","text":"<ul> <li> <p><code>_line_windows(text, window=fallback_chunk_size)</code>   Removes empty lines, applies line overlap (<code>overlap_lines</code>), cleans long comment blocks, skips fully\u2011commented windows.</p> </li> <li> <p><code>_char_windows(text, size=char_chunk_size, overlap=char_overlap)</code>   Sliding char windows with overlap; empties removed inside each chunk.</p> </li> </ul>"},{"location":"api-reference/rag/chunker/#heuristics-filters","title":"Heuristics &amp; Filters","text":"<ul> <li><code>_looks_like_code(text)</code> \u2013 detects code\u2011likeness with braces/semicolons/keywords/indent patterns &amp; short lines ratio.</li> <li><code>_is_comment_line(line)</code> and <code>_is_fully_commented(lines)</code> \u2013 used to skip comment\u2011only chunks.</li> <li><code>_clean_lines(chunk_lines, comment_block_threshold)</code> \u2013 collapses long comment blocks.</li> <li><code>_is_prompt_like(text)</code> + <code>_filter_prompt_like_blocks(code)</code> \u2013 removes triple\u2011quoted strings that look like LLM prompts.</li> </ul>"},{"location":"api-reference/rag/chunker/#safety-sanitization-custom-handlers","title":"Safety &amp; Sanitization (custom handlers)","text":"<ul> <li><code>_sanitize_chunks(chunks)</code> enforces:  </li> <li><code>max_returned_chunks</code> and <code>max_total_output_chars</code> </li> <li>drop entries shorter than <code>min_chunk_non_ws_chars</code> </li> <li>trim trailing newlines</li> </ul>"},{"location":"api-reference/rag/chunker/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/rag/chunker/#baseline-usage","title":"Baseline usage","text":"<pre><code>from pathlib import Path\nfrom neurosurfer.rag.chunker import Chunker, ChunkerConfig\n\nchunker = Chunker(ChunkerConfig(fallback_mode=\"auto\"))\n\npy_text = Path(\"app/main.py\").read_text()\npy_chunks = chunker.chunk(py_text, file_path=\"app/main.py\")\n\nmd_text = Path(\"README.md\").read_text()\nmd_chunks = chunker.chunk(md_text, file_path=\"README.md\")\n\nraw = \"A lot of prose...\"\ntxt_chunks = chunker.chunk(raw, file_path=\"notes.unknown\")  # auto \u2192 char windows\n</code></pre>"},{"location":"api-reference/rag/chunker/#register-a-custom-handler-by-name","title":"Register a custom handler (by name)","text":"<pre><code>from typing import List, Optional\nfrom neurosurfer.rag.chunker import Chunker, ChunkerConfig\n\ndef my_handler(text: str, *, file_path: Optional[str] = None, config: Optional[ChunkerConfig] = None) -&gt; List[str]:\n    # Example: split on double newlines and clamp ~1200 chars\n    segs = [s.strip() for s in text.split(\"\\n\\n\") if s.strip()]\n    out, buf = [], \"\"\n    for s in segs:\n        if len(buf) + len(s) + 2 &lt;= 1200:\n            buf = f\"{buf}\\n\\n{s}\".strip()\n        else:\n            if buf: out.append(buf)\n            buf = s\n    if buf: out.append(buf)\n    return out\n\nchunker = Chunker()\nchunker.register_custom(\"my_handler\", my_handler)\nchunker.use_custom_for_ext([\".rst\", \".tex\"], \"my_handler\")\n\nrst_text = open(\"paper.rst\").read()\nchunks = chunker.chunk(rst_text, file_path=\"paper.rst\")\n</code></pre>"},{"location":"api-reference/rag/chunker/#use-an-adhoc-callable-for-a-single-call","title":"Use an ad\u2011hoc callable for a single call","text":"<pre><code>def once(text: str, *, file_path=None, config=None):\n    return [p for p in text.split(\"\\n\\n\") if p.strip()]\n\nchunks = chunker.chunk(big_text, custom=once)\n</code></pre>"},{"location":"api-reference/rag/chunker/#install-a-router-to-decide-dynamically","title":"Install a router to decide dynamically","text":"<pre><code>def router(file_path, text):\n    if file_path and file_path.endswith(\".proto\"):\n        return \"proto_chunks\"   # must be registered via register_custom\n    if \"BEGIN:VCARD\" in text:\n        return \"vcard_chunks\"\n    return None\n\nchunker.set_router(router)\n</code></pre>"},{"location":"api-reference/rag/chunker/#attach-a-logger","title":"Attach a logger","text":"<pre><code>chunker.set_logger(lambda msg: print(f\"[chunker] {msg}\"))\n</code></pre>"},{"location":"api-reference/rag/chunker/#production-notes-best-practices","title":"Production Notes &amp; Best Practices","text":"<ul> <li>Prefer <code>fallback_mode=\"auto\"</code> in mixed repos; it chooses line windows for code\u2011like content.</li> <li>Keep <code>overlap_lines</code> low (2\u20135) to reduce duplication in code while preserving local context.</li> <li>For long prose, start with <code>char_chunk_size ~ 800\u20131200</code> and <code>char_overlap ~ 100\u2013200</code>.</li> <li>Treat JSON as structured: chunking at top\u2011level keys/elements often boosts retrieval precision.</li> <li>When adding custom handlers, rely on <code>_sanitize_chunks</code> (already applied) and consider your own local guards.</li> <li>The blacklist is conservative; extend/override upstream if needed before crawling a repo.</li> <li>Chunk after text normalization (decode, HTML \u2192 text, etc.) to keep windowing deterministic.</li> </ul>"},{"location":"api-reference/rag/filereader/","title":"FileReader","text":"<p>Module: <code>neurosurfer.rag.filereader</code></p> <p>A unified, production\u2011grade file \u2192 text loader for RAG pipelines. <code>FileReader</code> auto\u2011detects the file type by extension and applies the appropriate extractor to return clean UTF\u20118 text that\u2019s ready for downstream chunking (Chunker) and ingestion (RAG Ingestor). It is defensive by design: optional dependencies are handled gracefully and errors are returned as descriptive strings instead of crashing your pipeline.</p>"},{"location":"api-reference/rag/filereader/#overview","title":"Overview","text":"<p><code>FileReader</code> exposes a single high\u2011level method, <code>read(path)</code>, which dispatches to format\u2011specific readers. It supports documents, data files, presentations, code/config/log formats, HTML pages, and more. When a format is not explicitly supported, it falls back to plain\u2011text reading with UTF\u20118 (and <code>errors=\"ignore\"</code>).</p>"},{"location":"api-reference/rag/filereader/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Auto\u2011detection by extension via <code>supported_types</code> mapping.</li> <li>Broad format coverage out of the box (see table below).</li> <li>Graceful degradation when optional libraries are missing (returns helpful messages).</li> <li>Consistent plaintext output suitable for embedding + retrieval.</li> <li>Zero surprises: reader methods never raise; you get text (or an \u201cError reading \u2026\u201d string).</li> </ul>"},{"location":"api-reference/rag/filereader/#supported-formats-readers","title":"Supported Formats &amp; Readers","text":"Category Extensions Reader method Dependencies PDF <code>.pdf</code> <code>_read_pdf</code> <code>fitz</code> (PyMuPDF) HTML <code>.html</code>, <code>.htm</code> <code>_read_html</code> <code>bs4</code> (BeautifulSoup) DOCX <code>.docx</code> <code>_read_docx</code> <code>python-docx</code> CSV / TSV <code>.csv</code>, <code>.tsv</code> <code>_read_csv</code> <code>pandas</code> Excel <code>.xls</code>, <code>.xlsx</code> <code>_read_excel</code> <code>pandas</code> YAML <code>.yaml</code>, <code>.yml</code> <code>_read_yaml</code> <code>pyyaml</code> (optional) XML <code>.xml</code> <code>_read_xml</code> <code>xml.etree.ElementTree</code> (stdlib) PPTX <code>.pptx</code> <code>_read_pptx</code> <code>python-pptx</code> (optional) Plain\u2011text family <code>.txt</code>, <code>.md</code>, <code>.rtf</code>, <code>.doc</code>, <code>.odt</code>, <code>.json</code>, <code>.ppt</code>, <code>.py</code>, <code>.ipynb</code>, <code>.java</code>, <code>.js</code>, <code>.ts</code>, <code>.jsx</code>, <code>.tsx</code>, <code>.cpp</code>, <code>.c</code>, <code>.h</code>, <code>.cs</code>, <code>.go</code>, <code>.rb</code>, <code>.rs</code>, <code>.php</code>, <code>.swift</code>, <code>.kt</code>, <code>.sh</code>, <code>.bat</code>, <code>.ps1</code>, <code>.scala</code>, <code>.lua</code>, <code>.r</code>, <code>.env</code>, <code>.ini</code>, <code>.toml</code>, <code>.cfg</code>, <code>.conf</code>, <code>.properties</code>, <code>.log</code>, <code>.tex</code>, <code>.srt</code>, <code>.vtt</code> <code>_read_txt</code> none <p>Anything not in <code>supported_types</code> also falls back to <code>_read_txt</code> (UTF\u20118, <code>errors=\"ignore\"</code>).</p>"},{"location":"api-reference/rag/filereader/#dependencies","title":"Dependencies","text":"<ul> <li>Required core libs (used if format encountered): </li> <li><code>fitz</code> (PyMuPDF) for PDF  </li> <li><code>docx</code> (python\u2011docx) for DOCX  </li> <li><code>pandas</code> for CSV/TSV/Excel  </li> <li><code>bs4</code> (BeautifulSoup) for HTML</li> <li>Optional: </li> <li><code>pyyaml</code> for YAML (<code>yaml.safe_load</code>)  </li> <li><code>python-pptx</code> for PPTX  </li> <li><code>xml.etree.ElementTree</code> is from the standard library</li> </ul> <p>When an optional dependency is missing, the corresponding reader returns a clear message (e.g., <code>\"python-pptx not installed\"</code>).</p>"},{"location":"api-reference/rag/filereader/#public-api","title":"Public API","text":""},{"location":"api-reference/rag/filereader/#class-filereader","title":"<code>class FileReader</code>","text":""},{"location":"api-reference/rag/filereader/#attributes","title":"Attributes","text":"<ul> <li><code>supported_types: dict[str, Callable]</code> \u2013 mapping from extension (lowercase, with dot) to the concrete reader method.</li> </ul>"},{"location":"api-reference/rag/filereader/#methods","title":"Methods","text":"<ul> <li><code>read(file_path: str) -&gt; str</code>   Auto\u2011detects by extension and dispatches to a concrete <code>_read_*</code> method. If no handler is registered, uses <code>_read_txt</code>. Never raises; errors are returned as readable strings.</li> </ul>"},{"location":"api-reference/rag/filereader/#formatspecific-readers","title":"Format\u2011specific readers","text":"<ul> <li><code>_read_pdf(path: str) -&gt; str</code>   Page\u2011wise text extraction using <code>fitz</code>. On error returns <code>\"Error reading PDF: &lt;message&gt;\"</code>.</li> <li><code>_read_txt(path: str) -&gt; str</code>   Reads UTF\u20118 with <code>errors=\"ignore\"</code>. On error returns <code>\"Error reading TXT: &lt;message&gt;\"</code>.</li> <li><code>_read_html(path: str) -&gt; str</code>   Parses with <code>BeautifulSoup(..., \"html.parser\")</code> and returns visible text via <code>.get_text()</code>. On error returns <code>\"Error reading HTML: &lt;message&gt;\"</code>.</li> <li><code>_read_docx(path: str) -&gt; str</code>   Iterates <code>doc.paragraphs</code>, joins with newlines. On error returns <code>\"Error reading DOCX: &lt;message&gt;\"</code>.</li> <li><code>_read_excel(path: str) -&gt; str</code>   Uses <code>pandas.read_excel(..., sheet_name=None)</code> to load all sheets; renders with <code>.astype(str).to_string(index=False)</code> and sheet headers. On error returns <code>\"Error reading Excel: &lt;message&gt;\"</code>.</li> <li><code>_read_csv(path: str) -&gt; str</code>   Uses <code>pandas.read_csv</code>, stringifies and returns <code>.to_string(index=False)</code>. On error returns <code>\"Error reading CSV/TSV: &lt;message&gt;\"</code>.</li> <li><code>_read_yaml(path: str) -&gt; str</code>   Uses <code>yaml.safe_load</code> if <code>pyyaml</code> is available; otherwise returns <code>\"PyYAML not installed\"</code>. On error returns <code>\"Error reading YAML: &lt;message&gt;\"</code>.</li> <li><code>_read_xml(path: str) -&gt; str</code>   Uses <code>xml.etree.ElementTree.parse(...).getroot()</code> and <code>ET.tostring(..., encoding=\"unicode\")</code>. If unavailable, returns <code>\"XML parser not available\"</code>. On error returns <code>\"Error reading XML: &lt;message&gt;\"</code>.</li> <li><code>_read_pptx(path: str) -&gt; str</code>   Uses <code>Presentation(path)</code>; concatenates <code>shape.text</code> for all shapes across slides. If library is missing, returns <code>\"python-pptx not installed\"</code>. On error returns <code>\"Error reading PPTX: &lt;message&gt;\"</code>.</li> </ul>"},{"location":"api-reference/rag/filereader/#behavior-error-model","title":"Behavior &amp; Error Model","text":"<ul> <li>Non\u2011throwing: all readers catch exceptions and return <code>\"Error reading &lt;FORMAT&gt;: &lt;message&gt;\"</code> to prevent ingestion crashes. You may choose to skip these records upstream.</li> <li>Encoding: text reading uses UTF\u20118 with <code>errors=\"ignore\"</code> to maximize robustness.</li> <li>Best\u2011effort structure: dataframes, DOCX paragraphs, and PPTX shape texts are stringified in a predictable, readable way.</li> </ul>"},{"location":"api-reference/rag/filereader/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/rag/filereader/#basic","title":"Basic","text":"<pre><code>from neurosurfer.rag.filereader import FileReader\n\nreader = FileReader()\n\npdf_text = reader.read(\"report.pdf\")\nexcel_text = reader.read(\"dataset.xlsx\")\ncode_text = reader.read(\"script.py\")\nhtml_text = reader.read(\"page.html\")\n</code></pre>"},{"location":"api-reference/rag/filereader/#with-chunker-ingestor","title":"With Chunker &amp; Ingestor","text":"<pre><code>from neurosurfer.rag.filereader import FileReader\nfrom neurosurfer.rag.chunker import Chunker\nfrom neurosurfer.rag.ingestor import RAGIngestor\nfrom neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\nfrom neurosurfer.vectorstores.chroma import ChromaDB\n\nreader = FileReader()\nchunker = Chunker()\ningestor = RAGIngestor(\n    embedder=SentenceTransformerEmbedder(\"all-MiniLM-L6-v2\"),\n    vector_store=ChromaDB(collection=\"neurosurfer\")\n)\n\ntext = reader.read(\"README.md\")\nchunks = chunker.chunk(text, file_path=\"README.md\")\n# or: ingestor.add_files([\"README.md\"]).build()\n</code></pre>"},{"location":"api-reference/rag/filereader/#handling-errors","title":"Handling Errors","text":"<pre><code>txt = reader.read(\"possibly_corrupt.pdf\")\nif txt.startswith(\"Error reading\"):\n    # log and skip\n    pass\n</code></pre>"},{"location":"api-reference/rag/filereader/#extension-mapping-reference-supported_types","title":"Extension Mapping Reference (<code>supported_types</code>)","text":"<p>Below is the canonical mapping initialized in <code>__init__</code> (extensions are lowercase). You can inspect it at runtime:</p> <pre><code>reader = FileReader()\nprint(sorted(reader.supported_types.keys()))\n</code></pre> <p>Registered as structured readers: <code>.pdf</code>, <code>.html</code>, <code>.htm</code>, <code>.docx</code>, <code>.csv</code>, <code>.tsv</code>, <code>.xls</code>, <code>.xlsx</code>, <code>.xml</code>, <code>.yaml</code>, <code>.yml</code>, <code>.pptx</code> Registered to read as plain\u2011text: <code>.txt</code>, <code>.md</code>, <code>.rtf</code>, <code>.doc</code>, <code>.odt</code>, <code>.json</code>, <code>.ppt</code>, <code>.py</code>, <code>.ipynb</code>, <code>.java</code>, <code>.js</code>, <code>.ts</code>, <code>.jsx</code>, <code>.tsx</code>, <code>.cpp</code>, <code>.c</code>, <code>.h</code>, <code>.cs</code>, <code>.go</code>, <code>.rb</code>, <code>.rs</code>, <code>.php</code>, <code>.swift</code>, <code>.kt</code>, <code>.sh</code>, <code>.bat</code>, <code>.ps1</code>, <code>.scala</code>, <code>.lua</code>, <code>.r</code>, <code>.env</code>, <code>.ini</code>, <code>.toml</code>, <code>.cfg</code>, <code>.conf</code>, <code>.properties</code>, <code>.log</code>, <code>.tex</code>, <code>.srt</code>, <code>.vtt</code> Anything else \u2192 <code>_read_txt</code> (fallback)</p>"},{"location":"api-reference/rag/filereader/#production-notes-best-practices","title":"Production Notes &amp; Best Practices","text":"<ul> <li>HTML: <code>get_text()</code> strips tags; if you need DOM\u2011aware extraction (tables, links), post\u2011process the HTML separately and feed structured text to the Chunker.</li> <li>PDF: text extraction quality varies; consider adding an OCR fallback at a higher layer for scanned PDFs.</li> <li>Excel/CSV: very wide tables can produce long lines\u2014rely on the Chunker\u2019s char windows to split into manageable pieces.</li> <li>YAML/XML: readers return a serialized string; for semantic RAG over structured data, you may want to pre\u2011normalize into key/value lines.</li> <li>Error strings: treat them as loggable noise\u2014skip during ingestion rather than embedding error messages.</li> <li>Encoding: if a file is known to be non\u2011UTF\u20118, preconvert to UTF\u20118 before invoking <code>read()</code> for best results.</li> </ul>"},{"location":"api-reference/rag/ingestor/","title":"RAG Ingestor","text":"<p>Module: <code>neurosurfer.rag.ingestor</code> Depends on: - Reader: <code>FileReader</code> - Chunking: <code>Chunker</code> - Embeddings: BaseEmbedder (Embedders) - Vector DB: BaseVectorDB (Vector Stores)</p> <p>A production-grade end-to-end ingestion pipeline for RAG systems. <code>RAGIngestor</code> reads sources (files, folders, URLs, ZIPs, raw text), chunks them, embeds in batches, performs content-hash deduplication, and indexes into a vector store\u2014with progress callbacks, cancellation, and parallelism built in.</p>"},{"location":"api-reference/rag/ingestor/#features-flow","title":"Features &amp; Flow","text":"<p><code>RAGIngestor</code> is a full-stack ingestion engine: it accepts many source types (files, directories, raw text, URLs via your fetcher, Git folders, ZIPs), runs parallel chunking (configurable <code>max_workers</code>), performs batch embedding (tuned by <code>batch_size</code> with optional vector normalization), and enforces content deduplication (SHA-256 over chunk text). Throughout the run it emits structured progress events, supports cooperative cancellation (<code>threading.Event</code>), preserves metadata (e.g., filename/URL/ZIP + your custom fields), and handles archives with zip-slip\u2013safe extraction and defensive error handling.</p>"},{"location":"api-reference/rag/ingestor/#typical-flow","title":"Typical flow:","text":"<ol> <li>Queue sources with <code>add_files</code>, <code>add_directory</code>, <code>add_texts</code>, <code>add_urls</code>, <code>add_git_folder</code>, or <code>add_zipfile</code>.</li> <li>Build with <code>build()</code> which internally:</li> <li>Reads content via FileReader </li> <li>Chunks text via Chunker </li> <li>Deduplicates on SHA-256 of chunk text  </li> <li>Embeds using your BaseEmbedder </li> <li>Indexes into your BaseVectorDB</li> <li>Optionally verify the index with <code>embed_query</code> and <code>search</code>.</li> </ol>"},{"location":"api-reference/rag/ingestor/#constructor","title":"Constructor","text":"<pre><code>from neurosurfer.rag.ingestor import RAGIngestor\nfrom neurosurfer.rag.filereader import FileReader\nfrom neurosurfer.rag.chunker import Chunker\nfrom neurosurfer.models.embedders import BaseEmbedder\nfrom neurosurfer.vectorstores.base import BaseVectorDB\n\ningestor = RAGIngestor(\n    embedder: BaseEmbedder,                 # required\n    vector_store: BaseVectorDB,             # required\n    file_reader: FileReader | None = None,  # default FileReader()\n    chunker: Chunker | None = None,         # default Chunker()\n    logger: logging.Logger | None = None,\n    progress_cb: callable | None = None,    # def cb(d: dict) -&gt; None\n    cancel_event: threading.Event | None = None,\n    batch_size: int = 64,\n    max_workers: int = max(4, os.cpu_count() or 4),\n    deduplicate: bool = True,\n    normalize_embeddings: bool = True,\n    default_metadata: dict | None = None,\n    tmp_dir: str | None = None,             # defaults to \"./tmp\"\n)\n</code></pre>"},{"location":"api-reference/rag/ingestor/#parameters","title":"Parameters","text":"Name Type Default Description <code>embedder</code> <code>BaseEmbedder</code> \u2014 Embedding backend used for <code>embed(texts, normalize_embeddings=...)</code>. <code>vector_store</code> <code>BaseVectorDB</code> \u2014 Vector DB with <code>add_documents(docs)</code> and <code>similarity_search(vec, top_k)</code>. <code>file_reader</code> <code>FileReader</code> <code>FileReader()</code> File \u2192 text loader. See FileReader. <code>chunker</code> <code>Chunker</code> <code>Chunker()</code> Text chunker. See Chunker. <code>logger</code> <code>logging.Logger</code> module logger Target for exceptions/warnings. <code>progress_cb</code> <code>Callable[[dict], None]</code> <code>None</code> Receives progress events (see Progress Events). <code>cancel_event</code> <code>threading.Event</code> new event When set, long steps return early with <code>\"status\": \"cancelled\"</code>. <code>batch_size</code> <code>int</code> <code>64</code> Texts per embed call. Tune to GPU/CPU throughput. <code>max_workers</code> <code>int</code> <code>max(4, os.cpu_count() or 4)</code> Thread workers for concurrent chunking. <code>deduplicate</code> <code>bool</code> <code>True</code> Drop duplicate chunk texts (by SHA-256). <code>normalize_embeddings</code> <code>bool</code> <code>True</code> Unit-normalize vectors before indexing (cosine friendly). <code>default_metadata</code> <code>dict</code> <code>{}</code> Base metadata merged into every queued record. <code>tmp_dir</code> <code>str</code> <code>\"./tmp\"</code> Working directory used for ZIP extraction, etc."},{"location":"api-reference/rag/ingestor/#input-queueing-methods","title":"Input Queueing Methods","text":"<p>All queueing methods return <code>self</code> to enable builder-style chaining. Internal queue shape: <code>self._queue: List[Tuple[source_id: str, text: str, metadata: dict]]</code></p>"},{"location":"api-reference/rag/ingestor/#add_filespaths-include_extssupported_file_types-rootnone-extra_metadatanone","title":"<code>add_files(paths, include_exts=supported_file_types, *, root=None, extra_metadata=None)</code>","text":"<p>Reads individual files by extension and enqueues them.</p> <ul> <li><code>paths</code>: <code>Sequence[str | Path]</code></li> <li><code>include_exts</code>: set of allowed extensions (defaults to <code>rag.constants.supported_file_types</code>)</li> <li><code>root</code>: optional root to compute relative <code>filename</code> metadata</li> <li><code>extra_metadata</code>: merged with <code>default_metadata</code></li> </ul> <p>Metadata added: <code>{\"filename\": &lt;relative-or-absolute&gt;, \"source_type\": \"file\"}</code></p> <pre><code>ingestor.add_files([\"/data/README.md\", \"/data/notes.txt\"], root=\"/data\")\n</code></pre>"},{"location":"api-reference/rag/ingestor/#add_directorydirectory-include_extssupported_file_types-exclude_dirsnone-extra_metadatanone","title":"<code>add_directory(directory, *, include_exts=supported_file_types, exclude_dirs=None, extra_metadata=None)</code>","text":"<p>Recursively walks a directory, applying <code>include_exts</code> and excluding common junk (default excludes: <code>.git</code>, <code>__pycache__</code>, <code>.venv</code>, <code>node_modules</code>, <code>dist</code>, <code>build</code>).</p> <p>Metadata added: <code>{\"filename\": &lt;relative-path&gt;, \"source_type\": \"file\"}</code></p> <pre><code>ingestor.add_directory(\"./docs\")\n</code></pre>"},{"location":"api-reference/rag/ingestor/#add_textstexts-base_idtext-metadatasnone","title":"<code>add_texts(texts, *, base_id=\"text\", metadatas=None)</code>","text":"<p>Enqueue raw text strings (already in memory).</p> <ul> <li><code>texts</code>: sequence of strings</li> <li><code>base_id</code>: used to form <code>source_id</code> as <code>\"{base_id}:{i}\"</code></li> <li><code>metadatas</code>: optional list of dicts; aligned with <code>texts</code></li> </ul> <p>Metadata added: <code>{\"source_type\": \"raw_text\", \"ordinal\": i}</code></p> <pre><code>ingestor.add_texts(\n    [\"First text\", \"Second text\"],\n    metadatas=[{\"tag\": \"a\"}, {\"tag\": \"b\"}]\n)\n</code></pre>"},{"location":"api-reference/rag/ingestor/#add_urlsurls-fetchernone-extra_metadatanone","title":"<code>add_urls(urls, fetcher=None, extra_metadata=None)</code>","text":"<p>Add URLs using a user-provided <code>fetcher(url) -&gt; str | None</code> that returns clean text. (This keeps the ingestor offline-friendly and testable.)</p> <p>Metadata added: <code>{\"url\": url, \"source_type\": \"url\"}</code></p> <pre><code>import requests\nfrom bs4 import BeautifulSoup\n\ndef fetcher(url: str) -&gt; str | None:\n    r = requests.get(url, timeout=20)\n    soup = BeautifulSoup(r.text, \"html.parser\")\n    return soup.get_text(separator=\"\\n\").strip()\n\ningestor.add_urls([\"https://example.com\"], fetcher=fetcher)\n</code></pre>"},{"location":"api-reference/rag/ingestor/#add_git_folderrepo_root-include_extsnone-extra_metadatanone","title":"<code>add_git_folder(repo_root, *, include_exts=None, extra_metadata=None)</code>","text":"<p>Index an already-cloned repository folder using the code-friendly exclude set from <code>rag.constants.exclude_dirs_in_code</code>. Internally delegates to <code>add_directory(...)</code>.</p> <pre><code>ingestor.add_git_folder(\"/path/to/repo\")\n</code></pre>"},{"location":"api-reference/rag/ingestor/#add_zipfilezip_path-include_extssupported_file_types-exclude_dirsexclude_dirs_in_code-extra_metadatanone","title":"<code>add_zipfile(zip_path, *, include_exts=supported_file_types, exclude_dirs=exclude_dirs_in_code, extra_metadata=None)</code>","text":"<p>Safely extracts a single <code>.zip</code> file into a temporary directory under <code>tmp_dir</code>, indexes it via <code>add_directory</code>, then deletes the temp directory (context manager). Protected against zip-slip.</p> <p>Metadata added (in addition to others): <code>{\"source_zip\": \"&lt;zipname&gt;\"}</code></p> <pre><code>ingestor.add_zipfile(\"archive.zip\", extra_metadata={\"source\": \"upload\"})\n</code></pre>"},{"location":"api-reference/rag/ingestor/#build-ingest","title":"Build / Ingest","text":""},{"location":"api-reference/rag/ingestor/#build-dict","title":"<code>build() -&gt; dict</code>","text":"<p>Runs the full pipeline for all queued inputs:</p> <ol> <li>Chunk (concurrent): convert <code>(source_id, text, metadata)</code> \u2192 <code>(source_id, chunk_text, metadata)</code> </li> <li>Uses <code>Chunker</code>.  </li> <li> <p>Thread-pooled via <code>ThreadPoolExecutor(max_workers=self.max_workers)</code>.</p> </li> <li> <p>Deduplicate: drop duplicate <code>chunk_text</code> by SHA-256 hash, stored in <code>self._seen_hashes</code> (if <code>deduplicate=True</code>).  </p> </li> <li> <p>Adds <code>\"content_hash\"</code> to chunk metadata.</p> </li> <li> <p>Embed (batched): call <code>embedder.embed(texts, normalize_embeddings=self.normalize_embeddings)</code> in batches of <code>batch_size</code>.</p> </li> <li> <p>Index: Create <code>Doc</code> records and call <code>vector_store.add_documents(docs)</code>.</p> </li> <li> <p>Done: return stats dict (see Return below).</p> </li> </ol>"},{"location":"api-reference/rag/ingestor/#return","title":"Return","text":"<p><pre><code>{\n  \"status\": \"ok\",\n  \"sources\": &lt;int&gt;,\n  \"chunks\": &lt;int&gt;,\n  \"unique_chunks\": &lt;int&gt;,\n  \"added\": &lt;int&gt;,\n  \"finished_at\": &lt;unix_ts_float&gt;\n}\n</code></pre> If cancelled mid-run: <code>{\"status\": \"cancelled\", ...}</code> with partial counts.</p>"},{"location":"api-reference/rag/ingestor/#document-ids","title":"Document IDs","text":"<p>For each chunk, a stable <code>doc_id</code> is computed as: <pre><code>doc_id = f\"{content_hash[:16]}:{sha256(source_id)[:8]}\"\n</code></pre> This makes duplicates (same text) naturally collide while preserving source identity component.</p>"},{"location":"api-reference/rag/ingestor/#progress-events","title":"Progress Events","text":"<p>If <code>progress_cb</code> is provided, it receives dicts during <code>build()</code> like:</p> <pre><code>{\"stage\": \"start\",    \"queued_sources\": 42}\n{\"stage\": \"chunking\", \"completed_sources\": 10, \"total_sources\": 42, \"progress_pct\": 23}\n{\"stage\": \"dedupe\",   \"before\": 1200, \"after\": 985}\n{\"stage\": \"embedding\",\"embedded\": 256, \"total\": 985}\n{\"stage\": \"done\",     \"added\": 985}\n</code></pre> <p>Use this to update UIs, logs, or metrics dashboards.</p>"},{"location":"api-reference/rag/ingestor/#cancellation","title":"Cancellation","text":"<p>Set <code>cancel_event</code> (a <code>threading.Event</code>) from another thread to cooperatively stop the ingestion. The ingestor checks it between major steps and inside the embed loop; on detection it returns a <code>\"status\": \"cancelled\"</code> summary payload.</p> <pre><code>from threading import Event\nstop = Event()\n\ningestor = RAGIngestor(..., cancel_event=stop)\n# In another thread:\nstop.set()\n</code></pre>"},{"location":"api-reference/rag/ingestor/#quick-start-end-to-end","title":"Quick Start (End-to-End)","text":"<pre><code>from threading import Event\nfrom neurosurfer.rag.ingestor import RAGIngestor\nfrom neurosurfer.rag.filereader import FileReader\nfrom neurosurfer.rag.chunker import Chunker\nfrom neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\nfrom neurosurfer.vectorstores.chroma import ChromaDB\n\nstop = Event()\n\ndef progress(p):\n    print(p)\n\ningestor = RAGIngestor(\n    embedder=SentenceTransformerEmbedder(\"all-MiniLM-L6-v2\"),\n    vector_store=ChromaDB(collection=\"neurosurfer\"),\n    file_reader=FileReader(),\n    chunker=Chunker(),\n    progress_cb=progress,\n    cancel_event=stop,\n    batch_size=64,\n    max_workers=4,\n    deduplicate=True,\n    normalize_embeddings=True,\n    default_metadata={\"project\": \"Neurosurfer\"}\n)\n\n(ingestor\n    .add_directory(\"./docs\")\n    .add_files([\"README.md\"])\n    .add_texts([\"Custom note\"], metadatas=[{\"source\": \"manual\"}])\n)\n\nstats = ingestor.build()\nprint(\"Build stats:\", stats)\n\n# Smoke test:\nprint(ingestor.search(\"How do I run the server?\", top_k=3))\n</code></pre>"},{"location":"api-reference/rag/ingestor/#vector-store-expectations","title":"Vector Store Expectations","text":"<p><code>BaseVectorDB</code> must provide:</p> <ul> <li> <p><code>add_documents(docs: list[Doc]) -&gt; None</code>   Where <code>Doc</code> has: <code>id: str</code>, <code>text: str</code>, <code>embedding: list[float] | None</code>, <code>metadata: dict</code>.</p> </li> <li> <p><code>similarity_search(query_vec: list[float], top_k: int = 5) -&gt; list[tuple[Doc, float]]</code>   Returns <code>(Doc, score)</code> pairs (higher score = more similar).</p> </li> </ul> <p>See Vector Stores.</p>"},{"location":"api-reference/rag/ingestor/#embedder-expectations","title":"Embedder Expectations","text":"<p><code>BaseEmbedder</code> must provide:</p> <ul> <li><code>embed(texts: list[str], normalize_embeddings: bool = True) -&gt; list[list[float]]</code></li> </ul> <p>If <code>normalize_embeddings=True</code>, return unit vectors (cosine similarity friendly). See Embedders.</p>"},{"location":"api-reference/rag/ingestor/#performance-tuning-tips","title":"Performance Tuning &amp; Tips","text":"<ul> <li>Batch size: start with <code>64</code> for CPU sentence transformers; raise for GPUs until you saturate memory/throughput.</li> <li>Workers: chunking is CPU-bound but cheap; <code>max_workers=4\u201316</code> works well for large corpora.</li> <li>Dedup: keep enabled; it can reduce storage and query noise significantly for codebases &amp; docs.</li> <li>Metadata discipline: include <code>filename</code>, <code>url</code>, <code>source_zip</code>, <code>commit</code>, <code>page</code>/<code>sheet</code> where available.</li> <li>Normalization: keep <code>normalize_embeddings=True</code> if your store uses cosine distance.</li> <li>Backpressure: vector stores sometimes perform better with smaller bulks; split <code>docs_to_add</code> if needed.</li> </ul>"},{"location":"api-reference/rag/ingestor/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>My build crashes while embedding \u2192 Wrap/inspect <code>embedder.embed</code>; verify VRAM/num threads; check <code>batch_size</code>.</li> <li>I see many duplicate chunks \u2192 Confirm <code>deduplicate=True</code>; ensure preprocessing (e.g., <code>Chunker</code> filters) is enabled.</li> <li>ZIP ingestion missed files \u2192 Check <code>include_exts</code> and <code>exclude_dirs</code>; verify <code>supported_file_types</code> in <code>rag.constants</code>.</li> <li>Slow PDF extraction \u2192 Consider pre-processing/cleaning PDFs, or adding an OCR path outside this module.</li> <li>Error strings embedded \u2192 <code>FileReader</code> returns <code>\"Error reading ...\"</code> text; filter these upstream (skip on <code>\"Error reading\"</code> prefix).</li> </ul>"},{"location":"api-reference/rag/ingestor/#reference-helper-functions","title":"Reference: Helper Functions","text":"<ul> <li><code>sha256_text(s: str) -&gt; str</code> \u2014 UTF\u20118 SHA\u2011256 hex digest for dedupe &amp; IDs.  </li> <li><code>now_ts() -&gt; float</code> \u2014 <code>time.time()</code> convenience for <code>finished_at</code>.</li> </ul>"},{"location":"api-reference/rag/ingestor/#internals-for-contributors","title":"Internals (for contributors)","text":"<ul> <li>Queue structure: <code>self._queue: List[Tuple[str, str, dict]]</code></li> <li>Dedup state: <code>self._seen_hashes: set[str]</code></li> <li>Temp workspace: <code>self.tmp_dir</code> used by <code>add_zipfile</code> (auto-created)</li> <li>Safe ZIP extraction mitigates zip-slip by validating paths against the temp root before writing.</li> </ul>"},{"location":"api-reference/tools/","title":"Tools","text":"<p>Neurosurfer\u2019s tooling system lets agents perform real actions\u2014query data, retrieve knowledge, call internal services\u2014through a consistent, validated contract. You can register custom tools or use built-in tools that ship with Neurosurfer. Each tool declares a spec (inputs, returns, when-to-use) and returns a structured <code>ToolResponse</code> so agents can plan, validate, and compose multi-step workflows safely.</p> <ul> <li> <p> BaseTool</p> <p>The abstract contract every tool must implement. Enforces a class-level <code>spec</code> and a <code>__call__(...) -&gt; ToolResponse</code> implementation.</p> <p> Documentation</p> </li> <li> <p> Toolkit</p> <p>Validates, registers, and describes tools for agents. Prevents duplicates and renders Markdown descriptions via <code>get_tools_description()</code>.</p> <p> Documentation</p> </li> <li> <p> ToolSpec</p> <p>The schema for a tool (<code>name</code>, <code>description</code>, <code>when_to_use</code>, <code>inputs</code>, <code>returns</code>) with strict runtime validation via <code>check_inputs(...)</code>.</p> <p> Documentation</p> </li> <li> <p> Custom Tools</p> <p>Your domain-specific actions (internal APIs, dashboards, analytics). Subclass <code>BaseTool</code>, declare a <code>ToolSpec</code>, return a <code>ToolResponse</code>.</p> <p> Quick Start</p> </li> <li> <p> Built-in Tools</p> <p>Ready-to-use tools maintained by Neurosurfer (retrieval helpers, data utilities, and more). Same contract as your own tools.</p> <p> Catalog</p> </li> </ul>"},{"location":"api-reference/tools/#core-building-blocks","title":"Core Building Blocks","text":""},{"location":"api-reference/tools/#basetool-execution-contract","title":"BaseTool (execution contract)","text":"<ul> <li>Subclass and define a <code>spec: ToolSpec</code> on the class.</li> <li>Implement <code>__call__(...) -&gt; ToolResponse</code>.</li> <li>Use <code>ToolResponse.extras</code> to pass state between tools (e.g., IDs, cursors, intermediate artifacts).</li> </ul>"},{"location":"api-reference/tools/#toolspec-validation-documentation","title":"ToolSpec (validation &amp; documentation)","text":"<ul> <li>Declare inputs with precise types (<code>string</code>, <code>number</code>, <code>integer</code>, <code>boolean</code>, <code>array</code>, <code>object</code>).</li> <li>Provide when-to-use guidance so agents can select the right tool.</li> <li>Enforce correctness at runtime with <code>spec.check_inputs(raw_dict)</code>.</li> </ul>"},{"location":"api-reference/tools/#toolkit-registry-discovery","title":"Toolkit (registry &amp; discovery)","text":"<ul> <li><code>register_tool(tool)</code> with type checks and duplicate prevention.</li> <li><code>get_tools_description()</code> for agent prompts / <code>/help</code> output.</li> <li>Provide <code>registry[name]</code> lookup to execute tools directly.</li> </ul>"},{"location":"api-reference/tools/#quick-start-custom-tool","title":"Quick Start (Custom Tool)","text":"<pre><code>from neurosurfer.tools.base_tool import BaseTool, ToolResponse\nfrom neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\nfrom neurosurfer.tools.toolkit import Toolkit\n\n# 1) Define a custom tool by extending BaseTool\nclass Calculator(BaseTool):\n    spec = ToolSpec(\n        name=\"calculator\",\n        description=\"Performs basic arithmetic on two numbers.\",\n        when_to_use=\"When you need a quick numeric computation.\",\n        inputs=[\n            ToolParam(name=\"op\", type=\"string\", description=\"add|sub|mul|div\"),\n            ToolParam(name=\"a\", type=\"number\", description=\"Left operand\"),\n            ToolParam(name=\"b\", type=\"number\", description=\"Right operand\"),\n        ],\n        returns=ToolReturn(type=\"number\", description=\"Computation result\"),\n    )\n    def __call__(self, *, op: str, a: float, b: float, **_) -&gt; ToolResponse:\n        ops = {\"add\": a+b, \"sub\": a-b, \"mul\": a*b, \"div\": a/b if b else float('inf')}\n        return ToolResponse(final_answer=True, observation=str(ops.get(op, 'NaN')))\n\n# 2) Register it in a toolkit\ntoolkit = Toolkit()\ntoolkit.register_tool(Calculator())\n\n# 3) Let agents discover &amp; invoke\nprint(toolkit.get_tools_description())                         # Markdown summary for prompts/docs\nvalidated = toolkit.registry[\"calculator\"].spec.check_inputs(\n    {\"op\": \"mul\", \"a\": 6, \"b\": 7}\n)\nresult = toolkit.registry[\"calculator\"](**validated)           # -&gt; ToolResponse\nprint(\"Answer:\", result.observation)                           # \"42\"\n</code></pre>"},{"location":"api-reference/tools/#how-tools-fit-into-agents","title":"How Tools Fit Into Agents","text":"<ol> <li>Discovery \u2014 The agent reads <code>Toolkit.get_tools_description()</code> and selects candidate tools.  </li> <li>Validation \u2014 The agent validates inputs with <code>ToolSpec.check_inputs(...)</code>.  </li> <li>Invocation \u2014 The agent calls <code>tool(**validated_inputs, **runtime_ctx)</code> and gets a <code>ToolResponse</code>.  </li> <li>Control Flow \u2014 If <code>final_answer=True</code>, the agent stops. Otherwise, it may chain more tools using <code>extras</code> for context.  </li> <li>Observability \u2014 Tools should log responsibly and return meaningful <code>observation</code> text (or a streaming generator) for a great UX.</li> </ol>"},{"location":"api-reference/tools/#design-guidelines","title":"Design Guidelines","text":"<ul> <li>Be explicit \u2014 precise names and descriptions improve agent planning.</li> <li>Validate strictly \u2014 reject extra unknown inputs; type-check everything.</li> <li>Small, composable tools \u2014 easier to plan, test, and swap.</li> <li>Stream when it helps \u2014 long results or progressive tasks benefit from generator output in <code>ToolResponse.observation</code>.</li> <li>Document side effects \u2014 specify in <code>description</code>/<code>when_to_use</code> if a tool writes to disk, calls external services, or mutates state.</li> <li>Version carefully \u2014 if you change inputs/returns, consider versioning the tool name (<code>\u2026_v2</code>).</li> </ul>"},{"location":"api-reference/tools/#next-steps","title":"Next Steps","text":"<ul> <li>Contracts: BaseTool \u2022 ToolSpec \u2022 Toolkit </li> <li>Catalog: Built-in Tools </li> <li>Extend: add your own tools and register them in <code>Toolkit</code> to grow agent capabilities.</li> </ul>"},{"location":"api-reference/tools/base-tool/","title":"Base Tool","text":"<p>Module: <code>neurosurfer.tools.base_tool</code> Works with: <code>ToolSpec</code>, <code>Toolkit</code> (docs to follow)</p> <p><code>BaseTool</code> defines the contract every tool must follow in Neurosurfer. Tools encapsulate side-effectful or domain-specific actions (e.g., SQL execution, RAG queries, report generation) behind a stable interface so agents can discover, validate, and invoke them safely. Each tool declares a <code>ToolSpec</code> used for input validation, automatic documentation, and agent reasoning.</p>"},{"location":"api-reference/tools/base-tool/#what-this-module-provides","title":"What this module provides","text":"<ul> <li><code>ToolResponse</code> \u2014 a structured return type for tool executions (supports streaming).</li> <li><code>BaseTool</code> \u2014 an abstract base class that all tools must inherit from; enforces a <code>spec</code> and a <code>__call__</code> implementation.</li> </ul>"},{"location":"api-reference/tools/base-tool/#toolresponse","title":"<code>ToolResponse</code>","text":"<p>A lightweight dataclass describing a tool\u2019s result. It supports final answers, intermediate observations, and streaming (via Python generators).</p>"},{"location":"api-reference/tools/base-tool/#fields","title":"Fields","text":"Field Type Required Description <code>final_answer</code> <code>bool</code> \u2713 If <code>True</code>, the agent should treat <code>observation</code> as the final answer and stop tool use. <code>observation</code> <code>str \\| Generator[Any, None, None]</code> \u2713 The tool\u2019s output. Use a string for single-shot responses or a generator to stream tokens/lines/chunks. <code>extras</code> <code>dict</code> Arbitrary metadata to persist in agent memory and pass to subsequent tool calls (e.g., IDs, cursors, diagnostics)."},{"location":"api-reference/tools/base-tool/#examples","title":"Examples","text":"<p>Single-shot response</p> <pre><code>from neurosurfer.tools.base_tool import ToolResponse\n\nToolResponse(\n    final_answer=False,\n    observation=\"Found 3 matching rows in `users` table\",\n    extras={\"row_ids\": [1, 2, 3]}\n)\n</code></pre> <p>Streaming response</p> <pre><code>from typing import Generator\nfrom neurosurfer.tools.base_tool import ToolResponse\n\ndef _stream_lines(lines) -&gt; Generator[str, None, None]:\n    for ln in lines:\n        yield ln  # agent will stream these to the user\n\nToolResponse(\n    final_answer=True,\n    observation=_stream_lines([\"Step 1...\", \"Step 2...\", \"Done.\"]),\n)\n</code></pre> <p>Agents should detect generator outputs and stream them to the user/UI. If your tool streams, set <code>final_answer=True</code> only when it truly concludes the task.</p>"},{"location":"api-reference/tools/base-tool/#basetool","title":"<code>BaseTool</code>","text":"<p><code>BaseTool</code> enforces common behavior across all tools:</p> <ul> <li>A declared <code>spec</code> of type <code>ToolSpec</code> (validated at init).</li> <li>A concrete <code>__call__(...) -&gt; ToolResponse</code> implementation.</li> <li>A consistent invocation model (LLM/tooling context is passed via <code>**kwargs</code>).</li> </ul>"},{"location":"api-reference/tools/base-tool/#required-attribute","title":"Required attribute","text":"<ul> <li><code>spec: ToolSpec</code> \u2014 must be defined on the subclass class body. It describes:</li> <li><code>name</code>: unique tool identifier (e.g., <code>\"sql_query\"</code>)</li> <li><code>description</code>: what the tool does</li> <li><code>when_to_use</code>: guidance for agents</li> <li><code>inputs</code>: list of <code>ToolParam</code> (name, type, description, required)</li> <li><code>returns</code>: a <code>ToolReturn</code> (type + description)</li> </ul> <p>See full spec schema in <code>ToolSpec</code>.</p>"},{"location":"api-reference/tools/base-tool/#lifecycle","title":"Lifecycle","text":"<pre><code>def __init__(self) -&gt; None:\n    # Ensures subclass has a valid spec.\n    if not hasattr(self, \"spec\") or not isinstance(self.spec, ToolSpec):\n        raise TypeError(\"YourTool must define a ToolSpec 'spec'.\")\n    self.spec.validate()\n</code></pre> <p>If validation fails (e.g., unknown param type, duplicate param names), a <code>ValueError</code> is raised from <code>ToolSpec.validate()</code>.</p>"},{"location":"api-reference/tools/base-tool/#required-method","title":"Required method","text":"<pre><code>@abstractmethod\ndef __call__(self, *args: Any, **kwargs: Any) -&gt; ToolResponse:\n    ...\n</code></pre> <ul> <li>Inputs: The agent validates and packages inputs per <code>spec.inputs</code> and calls your tool as <code>tool(**validated_inputs, **runtime_ctx)</code>.  </li> <li>Runtime context (in <code>**kwargs</code>) may include objects like <code>llm</code>, <code>db_engine</code>, <code>embedder</code>, <code>vector_store</code>, etc., injected by the calling agent. Your tool should not assume their presence\u2014check and fail gracefully.</li> <li>Return: Always a <code>ToolResponse</code> (single-shot or streaming).</li> </ul>"},{"location":"api-reference/tools/base-tool/#implementing-a-tool-practical-examples","title":"Implementing a tool (practical examples)","text":""},{"location":"api-reference/tools/base-tool/#minimal-single-shot-tool","title":"Minimal single-shot tool","text":"<pre><code>from neurosurfer.tools.base_tool import BaseTool, ToolResponse\nfrom neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n\nclass EchoTool(BaseTool):\n    spec = ToolSpec(\n        name=\"echo\",\n        description=\"Echoes the input string back to the caller.\",\n        when_to_use=\"When you need to return the same text verbatim.\",\n        inputs=[\n            ToolParam(name=\"text\", type=\"string\", description=\"Text to echo\", required=True),\n        ],\n        returns=ToolReturn(type=\"string\", description=\"The echoed text\"),\n    )\n\n    def __call__(self, *, text: str, **kwargs) -&gt; ToolResponse:\n        return ToolResponse(final_answer=True, observation=text)\n</code></pre>"},{"location":"api-reference/tools/base-tool/#tool-that-uses-injected-runtime-context","title":"Tool that uses injected runtime context","text":"<pre><code>class SQLQueryTool(BaseTool):\n    spec = ToolSpec(\n        name=\"sql_query\",\n        description=\"Execute a SQL query and return rows as JSON.\",\n        when_to_use=\"When you need structured data from the database.\",\n        inputs=[\n            ToolParam(name=\"query\", type=\"string\", description=\"The SQL query to run\", required=True),\n            ToolParam(name=\"limit\", type=\"integer\", description=\"Max rows to return\", required=False),\n        ],\n        returns=ToolReturn(type=\"object\", description=\"Query results as a JSON object\"),\n    )\n\n    def __call__(self, *, query: str, limit: int = 100, **kwargs) -&gt; ToolResponse:\n        db = kwargs.get(\"db_engine\")\n        if db is None:\n            return ToolResponse(final_answer=True, observation=\"DB engine not available.\")\n        rows = db.execute(query).fetchmany(size=limit)\n        return ToolResponse(final_answer=False, observation={\"rows\": [dict(r) for r in rows]})\n</code></pre>"},{"location":"api-reference/tools/base-tool/#streaming-tool","title":"Streaming tool","text":"<pre><code>from typing import Generator, Iterable\n\nclass StreamLinesTool(BaseTool):\n    spec = ToolSpec(\n        name=\"stream_lines\",\n        description=\"Stream lines back to the caller (demo).\",\n        when_to_use=\"When producing incremental output is better for UX.\",\n        inputs=[ToolParam(name=\"lines\", type=\"array\", description=\"List of lines to stream\", required=True)],\n        returns=ToolReturn(type=\"string\", description=\"A stream of lines\"),\n    )\n\n    def __call__(self, *, lines: Iterable[str], **kwargs) -&gt; ToolResponse:\n        def _gen() -&gt; Generator[str, None, None]:\n            for ln in lines:\n                yield ln\n        return ToolResponse(final_answer=True, observation=_gen())\n</code></pre>"},{"location":"api-reference/tools/base-tool/#agenttool-invocation-model","title":"Agent/tool invocation model","text":"<ol> <li>Agent chooses a tool based on its <code>ToolSpec</code> (<code>when_to_use</code>, <code>inputs</code>, <code>returns</code>).</li> <li>Agent validates candidate inputs via <code>ToolSpec.check_inputs(raw)</code>.</li> <li>Agent calls <code>tool(**validated_inputs, **runtime_ctx)</code>.</li> <li>Tool returns <code>ToolResponse</code>:</li> <li><code>final_answer=True</code> \u2192 agent halts tool-use and presents the observation.</li> <li><code>final_answer=False</code> \u2192 agent may call more tools using <code>extras</code> if present.</li> <li>If <code>observation</code> is a generator, agent streams results to the user/UI.</li> </ol>"},{"location":"api-reference/tools/base-tool/#best-practices","title":"Best practices","text":"<ul> <li>Keep specs precise: Names/types/descriptions drive agent decision-making and validation.</li> <li>Use <code>extras</code> sparingly: Include only what\u2019s useful for follow-on calls (IDs, cursors, references).</li> <li>Fail soft: If a dependency (e.g., DB) isn\u2019t provided, return a helpful message rather than raising.</li> <li>Be deterministic: Idempotent tools make planning easier; document any side effects in <code>description</code>/<code>when_to_use</code>.</li> <li>Stream when valuable: Large or long-running results benefit from generator-based <code>observation</code>.</li> <li>Security: Validate/escape user inputs before executing commands/queries; document constraints in the spec.</li> </ul>"},{"location":"api-reference/tools/base-tool/#reference","title":"Reference","text":"<ul> <li>Source: <code>neurosurfer/tools/base_tool.py</code> </li> <li>Related: <code>ToolSpec</code> \u2022 <code>Toolkit</code></li> </ul>"},{"location":"api-reference/tools/tool-spec/","title":"ToolSpec","text":"<p>Module: <code>neurosurfer.tools.tool_spec</code> Pairs with: <code>BaseTool</code> \u2022 <code>Toolkit</code></p> <p><code>ToolSpec</code> defines the contract for a tool: its name, capabilities, inputs, and return type. Agents rely on this metadata to decide when to use a tool, how to validate inputs, and how to interpret outputs. The spec is also used to auto-generate documentation and enforce runtime validation.</p>"},{"location":"api-reference/tools/tool-spec/#data-model","title":"Data Model","text":""},{"location":"api-reference/tools/tool-spec/#supported-types","title":"Supported Types","text":"<p>The spec uses a constrained set of primitive types for validation:</p> <ul> <li><code>string</code>, <code>integer</code>, <code>number</code>, <code>boolean</code>, <code>array</code>, <code>object</code></li> </ul> <p>Internally, the validator also accepts <code>str</code> as an alias for <code>string</code>.</p>"},{"location":"api-reference/tools/tool-spec/#toolparam-input-parameter","title":"<code>ToolParam</code> \u2014 input parameter","text":"Field Type Required Description <code>name</code> <code>str</code> \u2713 Parameter name used as the keyword in calls. <code>type</code> <code>str</code> \u2713 One of the Supported Types above. <code>description</code> <code>str</code> \u2713 Human\u2011readable description for agents/users. <code>required</code> <code>bool</code> If <code>True</code>, must be present in inputs. Defaults to <code>True</code>. <p>Example</p> <pre><code>from neurosurfer.tools.tool_spec import ToolParam\nToolParam(name=\"query\", type=\"string\", description=\"SQL to execute\", required=True)\n</code></pre>"},{"location":"api-reference/tools/tool-spec/#toolreturn-return-value","title":"<code>ToolReturn</code> \u2014 return value","text":"Field Type Required Description <code>type</code> <code>str</code> \u2713 One of the Supported Types above. <code>description</code> <code>str</code> \u2713 What the tool returns (shape/semantics). <p>Example</p> <pre><code>from neurosurfer.tools.tool_spec import ToolReturn\nToolReturn(type=\"object\", description=\"Query results as a JSON object\")\n</code></pre>"},{"location":"api-reference/tools/tool-spec/#toolspec-full-specification","title":"<code>ToolSpec</code> \u2014 full specification","text":"Field Type Required Description <code>name</code> <code>str</code> \u2713 Unique tool identifier (e.g., <code>sql_query</code>). <code>description</code> <code>str</code> \u2713 Brief description of what the tool does. <code>when_to_use</code> <code>str</code> \u2713 Guidance to the agent about appropriate usage. <code>inputs</code> <code>list[ToolParam]</code> \u2713 Parameter specs (unique names). <code>returns</code> <code>ToolReturn</code> \u2713 Return value spec. <p>Example</p> <pre><code>from neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n\nspec = ToolSpec(\n    name=\"calculator\",\n    description=\"Performs arithmetic operations\",\n    when_to_use=\"Use when you must compute a numeric result\",\n    inputs=[\n        ToolParam(name=\"operation\", type=\"string\", description=\"add|sub|mul|div\", required=True),\n        ToolParam(name=\"a\", type=\"number\", description=\"Left operand\", required=True),\n        ToolParam(name=\"b\", type=\"number\", description=\"Right operand\", required=True),\n    ],\n    returns=ToolReturn(type=\"number\", description=\"Result of the operation\"),\n)\nspec.validate()\n</code></pre>"},{"location":"api-reference/tools/tool-spec/#validation","title":"Validation","text":""},{"location":"api-reference/tools/tool-spec/#validate-none","title":"<code>validate() -&gt; None</code>","text":"<p>Ensures a spec is well-formed. It checks:</p> <ul> <li>Presence of <code>name</code>, <code>description</code>, <code>when_to_use</code></li> <li>At least one input param</li> <li>Supported types for each param and for the return value</li> <li>Uniqueness of parameter names</li> </ul> <p>Failure modes: raises <code>ValueError</code> with a descriptive message (e.g., \u201c<code>calculator.b</code> has unsupported type 'float'` or \u201cduplicate input 'query'\u201d).</p>"},{"location":"api-reference/tools/tool-spec/#check_inputsraw-dict-dict","title":"<code>check_inputs(raw: dict) -&gt; dict</code>","text":"<p>Validates and sanitizes runtime inputs coming from an agent/LLM/user:</p> <ol> <li>Requireds: all <code>required=True</code> params must be present.  </li> <li>No extras: keys must match the declared param names exactly.  </li> <li>Type checks: values must satisfy the declared param <code>type</code> using strict predicates:  </li> <li><code>integer</code>: <code>isinstance(v, int) and not isinstance(v, bool)</code> </li> <li><code>number</code>: <code>isinstance(v, (int, float)) and not isinstance(v, bool)</code> </li> <li><code>array</code>: <code>isinstance(v, list)</code> </li> <li><code>object</code>: <code>isinstance(v, dict)</code> </li> <li>etc.</li> </ol> <p>Failure modes: raises <code>ValueError</code> for missing requireds, unexpected keys, or type mismatches. Success: returns the validated <code>raw</code> dict.</p>"},{"location":"api-reference/tools/tool-spec/#to_json-dict","title":"<code>to_json() -&gt; dict</code>","text":"<p>Returns a JSON\u2011serializable dictionary representation of the spec (useful for introspection or emitting tool descriptions to a client).</p>"},{"location":"api-reference/tools/tool-spec/#usage-with-basetool","title":"Usage with <code>BaseTool</code>","text":"<p>Every tool must declare a <code>spec: ToolSpec</code> on the class and will be validated in <code>BaseTool</code><code>.__init__</code>. At runtime, agents can call:</p> <pre><code>validated = tool.spec.check_inputs(raw_inputs)\nresult = tool(**validated, **runtime_ctx)\n</code></pre> <p>See the full tool lifecycle in <code>BaseTool</code>.</p>"},{"location":"api-reference/tools/tool-spec/#examples","title":"Examples","text":""},{"location":"api-reference/tools/tool-spec/#minimal-echo-tool","title":"Minimal echo tool","text":"<pre><code>from neurosurfer.tools.base_tool import BaseTool, ToolResponse\nfrom neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n\nclass EchoTool(BaseTool):\n    spec = ToolSpec(\n        name=\"echo\",\n        description=\"Echoes text back to the caller.\",\n        when_to_use=\"Use for basic echo or smoke testing\",\n        inputs=[ToolParam(name=\"text\", type=\"string\", description=\"Text to echo\", required=True)],\n        returns=ToolReturn(type=\"string\", description=\"The echoed text\"),\n    )\n    def __call__(self, *, text: str, **kwargs) -&gt; ToolResponse:\n        return ToolResponse(final_answer=True, observation=text)\n</code></pre>"},{"location":"api-reference/tools/tool-spec/#strict-input-validation","title":"Strict input validation","text":"<pre><code># Raises ValueError: missing required 'text', or wrong types, or extras.\nEchoTool().spec.check_inputs({\"text\": 123})\n</code></pre>"},{"location":"api-reference/tools/tool-spec/#best-practices","title":"Best Practices","text":"<ul> <li>Be explicit: clear names and descriptions improve agent planning.</li> <li>Keep inputs minimal: fewer, well-typed parameters \u2192 better validation and UX.</li> <li>Describe side effects in the <code>description</code>/<code>when_to_use</code> if applicable (e.g., \u201cwrites to DB\u201d).</li> <li>Version breaking changes in tool names if you must change inputs/returns radically.</li> </ul>"},{"location":"api-reference/tools/tool-spec/#reference","title":"Reference","text":"<ul> <li>Source: <code>neurosurfer/tools/tool_spec.py</code> </li> <li>Related: <code>BaseTool</code> \u2022 <code>Toolkit</code></li> </ul>"},{"location":"api-reference/tools/toolkit/","title":"Toolkit","text":"<p>Module: <code>neurosurfer.tools.toolkit</code> Works with: <code>BaseTool</code> \u2022 <code>ToolSpec</code></p> <p><code>Toolkit</code> is the registry and manager for tools in Neurosurfer. It validates and registers tools, keeps their specs, and produces markdown descriptions agents can consume to understand capabilities, inputs, and return types.</p>"},{"location":"api-reference/tools/toolkit/#responsibilities","title":"Responsibilities","text":"<ul> <li>Register tools (type-checked, spec-validated)</li> <li>Prevent duplicates by unique tool name</li> <li>Expose the registry for direct access</li> <li>Render human/LLM-friendly descriptions (<code>get_tools_description()</code>)</li> </ul>"},{"location":"api-reference/tools/toolkit/#data-attributes","title":"Data &amp; Attributes","text":"Attribute Type Description <code>logger</code> <code>logging.Logger</code> Target for info/warnings about registrations. <code>registry</code> <code>dict[str, BaseTool]</code> Tool name \u2192 tool instance. <code>specs</code> <code>dict[str, ToolSpec]</code> Tool name \u2192 <code>ToolSpec</code>."},{"location":"api-reference/tools/toolkit/#api","title":"API","text":""},{"location":"api-reference/tools/toolkit/#__init__-tools-listbasetool-logger-logginglogger-none-logginggetlogger__name__","title":"<code>__init__( tools: List[BaseTool] = [], logger: logging.Logger | None = logging.getLogger(__name__))</code>","text":"<p>Creates a toolkit with the provided tools. <code>tools</code> is optional and must be a list of <code>BaseTool</code> instances. If <code>logger</code> is omitted, the module logger is used.</p>"},{"location":"api-reference/tools/toolkit/#register_tooltool-basetool-none","title":"<code>register_tool(tool: BaseTool) -&gt; None</code>","text":"<p>Registers a tool. Enforces:</p> <ul> <li><code>tool</code> must be an instance of <code>BaseTool</code></li> <li>Tool name (<code>tool.spec.name</code>) must be unique within the registry</li> </ul> <p>On success: the tool is available in <code>registry[name]</code> and <code>specs[name]</code>. On failure: raises - <code>TypeError</code> if <code>tool</code> is not a <code>BaseTool</code> - <code>ValueError</code> if a tool with the same name is already registered</p> <p>Example</p> <pre><code>from neurosurfer.tools.toolkit import Toolkit\nfrom neurosurfer.tools.base_tool import BaseTool, ToolResponse\nfrom neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n\nclass HelloTool(BaseTool):\n    spec = ToolSpec(\n        name=\"hello\",\n        description=\"Greets a person\",\n        when_to_use=\"When you need a greeting\",\n        inputs=[ToolParam(name=\"name\", type=\"string\", description=\"Person's name\")],\n        returns=ToolReturn(type=\"string\", description=\"A greeting\"),\n    )\n    def __call__(self, *, name: str, **kwargs) -&gt; ToolResponse:\n        return ToolResponse(final_answer=True, observation=f\"Hello, {name}!\")\n\ntoolkit = Toolkit()\ntoolkit.register_tool(HelloTool())\nassert \"hello\" in toolkit.registry\n</code></pre>"},{"location":"api-reference/tools/toolkit/#get_tools_description-str","title":"<code>get_tools_description() -&gt; str</code>","text":"<p>Returns a markdown string describing each registered tool in a consistent format:</p> <pre><code>### `tool_name`\n&lt;description&gt;\n**When to use**: &lt;when_to_use&gt;\n**Inputs**:\n- `param`: &lt;type&gt; (required|optional) \u2014 &lt;description&gt;\n**Returns**: &lt;type&gt; \u2014 &lt;description&gt;\n</code></pre> <p>This is useful for: - Agent prompts (tooling instructions) - User-facing docs or <code>/help</code> output - Debugging tool registration</p> <p>Example</p> <pre><code>md = toolkit.get_tools_description()\nprint(md)\n</code></pre>"},{"location":"api-reference/tools/toolkit/#putting-it-together-agent-flow","title":"Putting it together (Agent flow)","text":"<ol> <li>Build a <code>Toolkit</code> and register all tools.</li> <li>Provide <code>toolkit.get_tools_description()</code> to the agent at planning time.</li> <li>When the agent selects a tool:</li> <li>Validate inputs using <code>tool.spec.check_inputs(raw)</code></li> <li>Call the tool: <code>tool(**validated, **runtime_ctx)</code> (see <code>BaseTool</code>)</li> <li>Consume the <code>ToolResponse</code> and decide whether to stop or continue based on <code>final_answer</code>.</li> </ol>"},{"location":"api-reference/tools/toolkit/#best-practices","title":"Best Practices","text":"<ul> <li>Unique names: treat <code>spec.name</code> as a stable API surface (version if breaking changes occur).</li> <li>Log registrations at startup so you can audit available tools quickly.</li> <li>Small, focused tools compose better than monoliths.</li> <li>Surface constraints in the spec (<code>when_to_use</code>, param descriptions).</li> </ul>"},{"location":"api-reference/tools/toolkit/#reference","title":"Reference","text":"<ul> <li>Source: <code>neurosurfer/tools/toolkit.py</code> </li> <li>Related: <code>BaseTool</code> \u2022 <code>ToolSpec</code></li> </ul>"},{"location":"api-reference/tools/builtin-tools/","title":"Built-in Tools","text":"<p>Neurosurfer ships with a growing set of ready-to-use tools built on the same Tooling API as your custom tools. Drop them into a <code>Toolkit</code>, compose them in agents, and replace them with your own implementations when needed.</p> <ul> <li> <p> SQL Tools</p> <p>Composable building blocks for natural\u2011language analytics and database understanding: pick relevant tables, generate queries, execute them, and present results.</p> <p> Explore SQL Tools</p> </li> </ul> <p>Tip: Use <code>Toolkit.get_tools_description()</code> to emit a human/LLM\u2011friendly summary of everything you registered.</p>"},{"location":"api-reference/tools/builtin-tools/sql/","title":"SQL Tools","text":"<p>A suite of composable tools for text\u2011to\u2011SQL workflows and database insights. Use them end\u2011to\u2011end or pick individual blocks that fit your agent.</p> <ul> <li> <p> Relevant Table Schema Finder (LLM)</p> <p>Selects the most relevant tables based on summaries and returns a schema context to guide query generation.</p> <p> Documentation</p> </li> <li> <p> SQL Query Generator</p> <p>Produces a single, syntactically valid T\u2011SQL query from a refined question and provided schema context.</p> <p> Documentation</p> </li> <li> <p> SQL Executor</p> <p>Executes the generated SQL with SQLAlchemy and returns rows as dictionaries for downstream formatting.</p> <p> Documentation</p> </li> <li> <p> Final Answer Formatter</p> <p>Converts raw rows into a clear, user\u2011friendly narrative and/or markdown table suitable for UI display.</p> <p> Documentation</p> </li> <li> <p> DB Insights Tool</p> <p>Answers conceptual questions about schema design and relationships using table metadata (no query execution).</p> <p> Documentation</p> </li> </ul>"},{"location":"api-reference/tools/builtin-tools/sql/#flow-typical","title":"Flow (typical)","text":"<ol> <li>Relevant Table Schema Finder (LLM) \u2192 build focused schema context  </li> <li>SQL Query Generator \u2192 produce a valid query  </li> <li>SQL Executor \u2192 run the query and collect rows  </li> <li>Final Answer Formatter \u2192 present the result to the user (Use DB Insights Tool anytime you need architectural/relationship explanations.)</li> </ol> <p>All SQL tools are standard <code>BaseTool</code> subclasses. Register them in a <code>Toolkit</code>, validate inputs with <code>ToolSpec.check_inputs()</code>, and chain state via <code>ToolResponse.extras</code> (e.g., <code>schema_context</code>, <code>db_results</code>).</p>"},{"location":"api-reference/tools/builtin-tools/sql/db_insights_tool/","title":"Database Insights Tool","text":"<p>Module: <code>neurosurfer.tools.sql.db_insights_tool.DBInsightsTool</code> Pairs with: <code>BaseTool</code> \u2022 <code>ToolSpec</code> \u2022 <code>Toolkit</code></p>"},{"location":"api-reference/tools/builtin-tools/sql/db_insights_tool/#overview","title":"Overview","text":"<p><code>DBInsightsTool</code> answers high-level, conceptual questions about your database using table summaries/metadata (not by executing SQL). It\u2019s ideal for explaining the purpose of the database, the roles of tables, and relationships that may have architectural or security implications.</p> <p>It builds a prompt from your schema store and queries your LLM to produce a structured, concise narrative answer.</p>"},{"location":"api-reference/tools/builtin-tools/sql/db_insights_tool/#when-to-use","title":"When to Use","text":"<p>Use this tool when you need: - Conceptual insights about the overall database design. - Explanations about how entities relate (e.g., departments \u2194 roles \u2194 access control). - Architectural/security observations that cannot be answered by running a SQL query.</p> <p>Not for: returning data rows or executing queries (use <code>SQLExecutor</code> for that).</p>"},{"location":"api-reference/tools/builtin-tools/sql/db_insights_tool/#spec-inputs-returns","title":"Spec (Inputs &amp; Returns)","text":"Field Type Required Description <code>query</code> <code>str</code> \u2713 A natural-language question about the database's structure, design, or semantics. <p>Returns: <code>str</code> \u2014 A natural-language explanation synthesizing table metadata and relationships.</p> <p>Note: <code>ToolSpec</code> here declares types as <code>str</code> (alias of <code>string</code> supported by the validator).</p>"},{"location":"api-reference/tools/builtin-tools/sql/db_insights_tool/#runtime-dependencies-config","title":"Runtime Dependencies &amp; Config","text":"<ul> <li>Constructor: <code>DBInsightsTool(llm: BaseModel, sql_schema_store: SQLSchemaStore, logger: logging.Logger | None = None)</code></li> <li>Uses: <code>sql_schema_store.store: Dict[str, {{summary: str, ...}}]</code></li> <li>Prompt: Built from <code>DATABASE_INSIGHT_PROMPT</code>:</li> <li><code>system_prompt</code>: senior DB architect persona</li> <li><code>user_prompt</code>: injects your <code>query</code> and aggregated table summaries</li> <li>LLM call: <code>llm.ask(system_prompt, user_prompt, temperature=0.7, max_new_tokens=3000, stream=True)</code></li> <li>Streaming: <code>True</code> by default \u2192 <code>observation</code> may be a generator.</li> <li><code>final_answer</code>: Defaults to <code>True</code> (can be overridden via <code>kwargs[\"final_answer\"]</code>).</li> </ul>"},{"location":"api-reference/tools/builtin-tools/sql/db_insights_tool/#behavior","title":"Behavior","text":"<ol> <li>Collects table summaries via <code>get_tables_summaries__()</code>.</li> <li>Renders system and user prompts.</li> <li>Calls <code>llm.ask(...)</code> and returns a <code>ToolResponse</code> with <code>final_answer=True</code> by default (streaming enabled).</li> </ol> <p>Special token: <code>\" [__DATABASE_INSIGHT__] \"</code> is available on the instance if you need to tag content for downstream parsing.</p>"},{"location":"api-reference/tools/builtin-tools/sql/db_insights_tool/#usage","title":"Usage","text":"<pre><code>tool = DBInsightsTool(llm=chat_llm, sql_schema_store=schema_store)\nresp = tool(query=\"How do departments relate to approvals?\")\n# resp.observation may be a streaming generator (depending on llm.ask)\n</code></pre>"},{"location":"api-reference/tools/builtin-tools/sql/db_insights_tool/#error-handling-notes","title":"Error Handling &amp; Notes","text":"<ul> <li>If the schema store lacks summaries, answers may be limited; the prompt asks the LLM to say \u201cdon\u2019t know\u201d where appropriate.</li> <li>This tool does not execute SQL and does not inspect live data.</li> <li>Ensure <code>sql_schema_store.store</code> contains <code>{{table_name: {{'summary': '...'}}}}</code> for best results.</li> </ul>"},{"location":"api-reference/tools/builtin-tools/sql/final_answer_formatter/","title":"Final Answer Formatter","text":"<p>Module: <code>neurosurfer.tools.sql.final_answer_formatter.FinalAnswerFormatter</code> Pairs with: <code>BaseTool</code> \u2022 <code>ToolSpec</code> \u2022 <code>Toolkit</code></p>"},{"location":"api-reference/tools/builtin-tools/sql/final_answer_formatter/#overview","title":"Overview","text":"<p><code>FinalAnswerFormatter</code> turns raw SQL results into a user-friendly explanation or markdown table, suitable as the final answer to present to end users. It leverages an LLM to produce clear, concise output while avoiding unnecessary SQL jargon.</p>"},{"location":"api-reference/tools/builtin-tools/sql/final_answer_formatter/#when-to-use","title":"When to Use","text":"<ul> <li>After <code>SQLExecutor</code> has run a query and produced <code>db_results</code>.  </li> <li>You want a polished response (narrative, table, or mixed) for the final user-facing message.</li> </ul> <p>Not for: generating or executing SQL. For generation, see <code>SQLQueryGenerator</code>.</p>"},{"location":"api-reference/tools/builtin-tools/sql/final_answer_formatter/#spec-inputs-returns","title":"Spec (Inputs &amp; Returns)","text":"Field Type Required Description <code>user_query</code> <code>string</code> \u2713 The original user question (for context). <p>Requires (via <code>kwargs</code>): <code>db_results: list[dict]</code> \u2014 typically passed from <code>SQLExecutor</code> as <code>extras[\"db_results\"]</code>.</p> <p>Returns: <code>string</code> \u2014 Natural language summary and/or markdown table.</p>"},{"location":"api-reference/tools/builtin-tools/sql/final_answer_formatter/#runtime-dependencies-config","title":"Runtime Dependencies &amp; Config","text":"<ul> <li>Constructor: <code>FinalAnswerFormatter(llm: BaseModel, logger: logging.Logger | None = None)</code></li> <li>Prompt: <code>RESULTS_PRESENTATION_PROMPT</code> (system + user)</li> <li>Streaming: <code>True</code></li> <li>Temperature: <code>0.7</code></li> <li>Max tokens: <code>8000</code></li> </ul> <p>Preprocessing: First 10 rows are formatted into a markdown table for the LLM (headers inferred from the first row\u2019s keys).</p>"},{"location":"api-reference/tools/builtin-tools/sql/final_answer_formatter/#behavior","title":"Behavior","text":"<ol> <li>Validates <code>db_results</code> from <code>kwargs</code>; if missing/empty, returns <code>ToolResponse(final_answer=False, observation=\"No results found...\")</code>.</li> <li>Builds a concise table preview (up to 10 rows).</li> <li>Calls <code>llm.ask(...)</code> to produce the final explanation/table and returns <code>ToolResponse(final_answer=True, observation=...)</code>.</li> </ol>"},{"location":"api-reference/tools/builtin-tools/sql/final_answer_formatter/#usage","title":"Usage","text":"<pre><code># After SQLExecutor\nfmt = FinalAnswerFormatter(llm=chat_llm)\nresp = fmt(user_query=\"Top 5 active users by posts?\", db_results=db_results)\n# resp.final_answer == True; resp.observation is the final message (may stream)\n</code></pre>"},{"location":"api-reference/tools/builtin-tools/sql/final_answer_formatter/#error-handling-notes","title":"Error Handling &amp; Notes","text":"<ul> <li>If <code>db_results</code> is empty or missing, the tool returns a non-final observation so the agent can branch (e.g., ask for a refined query).</li> <li>The tool streams by default; ensure your agent can stream generators from <code>ToolResponse.observation</code>.</li> </ul>"},{"location":"api-reference/tools/builtin-tools/sql/relevant_table_schema_retriever/","title":"Relevant Table Schema Finder (LLM)","text":"<p>Module: <code>neurosurfer.tools.sql.relevant_table_schema_retriever.RelevantTableSchemaFinderLLM</code> Pairs with: <code>BaseTool</code> \u2022 <code>ToolSpec</code> \u2022 <code>Toolkit</code></p>"},{"location":"api-reference/tools/builtin-tools/sql/relevant_table_schema_retriever/#overview","title":"Overview","text":"<p><code>RelevantTableSchemaFinderLLM</code> uses an LLM to select the most relevant tables for a user\u2019s question based on table summaries, then retrieves the corresponding schemas from your schema store. It\u2019s typically used early in a workflow before generating SQL.</p>"},{"location":"api-reference/tools/builtin-tools/sql/relevant_table_schema_retriever/#when-to-use","title":"When to Use","text":"<ul> <li>You have many tables and need to narrow down which are relevant to the question.</li> <li>You want to assemble a schema context to pass to <code>SQLQueryGenerator</code>.</li> </ul>"},{"location":"api-reference/tools/builtin-tools/sql/relevant_table_schema_retriever/#spec-inputs-returns","title":"Spec (Inputs &amp; Returns)","text":"Field Type Required Description <code>query</code> <code>string</code> \u2713 Natural-language user question. <p>Returns: <code>string</code> \u2014 A human-readable message listing selected tables. Extras: <code>schema_context: string</code> \u2014 The formatted schemas of selected tables for downstream tools.</p>"},{"location":"api-reference/tools/builtin-tools/sql/relevant_table_schema_retriever/#runtime-dependencies-config","title":"Runtime Dependencies &amp; Config","text":"<ul> <li>Constructor: <code>RelevantTableSchemaFinderLLM(llm: BaseModel, sql_schema_store: SQLSchemaStore, logger: logging.Logger | None = None)</code></li> <li>Prompt: <code>RELEVENT_TABLES_PROMPT</code> (note: result must be a valid Python list literal)</li> <li>Top K: <code>top_k = 6</code> (upper bound; LLM can return fewer)</li> <li>Token trimming: uses <code>RAGRetrieverAgent._trim_context_by_token_limit(...)</code> to fit summaries and adjust <code>max_new_tokens</code>.</li> <li>LLM call: <code>stream=False</code> (expects a one-shot list literal)</li> <li>Post-processing: <code>eval(...)</code> is applied to parse the returned list of table names.</li> </ul> <p>\u26a0\ufe0f Security note: Since <code>eval</code> is used on the LLM response, ensure your LLM and prompts are trusted/controlled. The system prompt strictly instructs a Python list literal only\u2014no text or newlines.</p>"},{"location":"api-reference/tools/builtin-tools/sql/relevant_table_schema_retriever/#behavior","title":"Behavior","text":"<ol> <li>Collects table summaries: <code>\"Table: &lt;name&gt;\\nSummary: &lt;summary&gt;\\n\\n\"</code>.</li> <li>Trims context to token budget if needed.</li> <li>Calls LLM; expects output like <code>['Users', 'Orders']</code>.</li> <li>Builds a message and schema context by fetching schemas via <code>sql_schema_store.get_table_data(name)</code>.</li> <li>Returns <code>ToolResponse(final_answer=False, observation=&lt;message&gt;, extras={{\"schema_context\": ...}})</code>.</li> </ol>"},{"location":"api-reference/tools/builtin-tools/sql/relevant_table_schema_retriever/#usage","title":"Usage","text":"<pre><code>finder = RelevantTableSchemaFinderLLM(llm=chat_llm, sql_schema_store=schema_store)\nresp = finder(query=\"Show monthly revenue by region in 2024\")\nschema_ctx = resp.extras[\"schema_context\"]  # pass to SQLQueryGenerator\n</code></pre>"},{"location":"api-reference/tools/builtin-tools/sql/relevant_table_schema_retriever/#error-handling-notes","title":"Error Handling &amp; Notes","text":"<ul> <li>If the LLM returns an invalid list, <code>eval</code> may fail; guard at the agent layer if necessary.</li> <li>Ensure <code>sql_schema_store</code> contains both <code>summary</code> and <code>schema</code> for each table used.</li> <li>Special token available: <code>\" [__RELEVANT_TABLES__] \"</code> (if you tag content downstream).</li> </ul>"},{"location":"api-reference/tools/builtin-tools/sql/sql_executor/","title":"SQL Executor","text":"<p>Module: <code>neurosurfer.tools.sql.sql_executor.SQLExecutor</code> Pairs with: <code>BaseTool</code> \u2022 <code>ToolSpec</code> \u2022 <code>Toolkit</code></p>"},{"location":"api-reference/tools/builtin-tools/sql/sql_executor/#overview","title":"Overview","text":"<p><code>SQLExecutor</code> runs a raw SQL query string using a provided SQLAlchemy engine and returns rows as a list of dictionaries. It is typically used after you\u2019ve generated SQL via <code>SQLQueryGenerator</code>.</p>"},{"location":"api-reference/tools/builtin-tools/sql/sql_executor/#when-to-use","title":"When to Use","text":"<ul> <li>You have a finalized SQL query and need to execute it against a live database.</li> <li>You want raw rows for further processing or for final formatting via <code>FinalAnswerFormatter</code>.</li> </ul>"},{"location":"api-reference/tools/builtin-tools/sql/sql_executor/#spec-inputs-returns","title":"Spec (Inputs &amp; Returns)","text":"Field Type Required Description <code>sql_query</code> <code>string</code> \u2713 The SQL to execute. <p>Returns: <code>list</code> \u2014 Each item is a <code>dict</code> representing one result row. Extras: <code>db_results: list[dict]</code> \u2014 returned in <code>ToolResponse.extras</code> for downstream tools.</p>"},{"location":"api-reference/tools/builtin-tools/sql/sql_executor/#runtime-dependencies-config","title":"Runtime Dependencies &amp; Config","text":"<ul> <li>Constructor: <code>SQLExecutor(db_engine: sqlalchemy.Engine, logger: logging.Logger | None = None)</code></li> <li>Executes: <code>connection.execute(sqlalchemy.text(query))</code></li> <li>Marshalling: zips <code>result.keys()</code> with each row to produce <code>dict</code> rows.</li> <li>Observation message: human-friendly (\u201cfetched results\u201d / \u201cno results\u201d).</li> </ul> <p>Special token: <code>\" [__SQL_EXECUTOR__] \"</code> is available on the instance for tagging, if desired.</p>"},{"location":"api-reference/tools/builtin-tools/sql/sql_executor/#behavior","title":"Behavior","text":"<ol> <li>Executes the SQL using the configured <code>db_engine</code>.</li> <li>Builds <code>db_results</code> (list of dict rows).</li> <li>Returns <code>ToolResponse(final_answer=False, observation=&lt;message&gt;, extras={{\"db_results\": db_results}})</code>.</li> </ol> <p>On exception: logs an error and returns <code>ToolResponse(final_answer=False, observation=[{{\"error\": \"&lt;message&gt;\"}}])</code>.</p>"},{"location":"api-reference/tools/builtin-tools/sql/sql_executor/#usage","title":"Usage","text":"<pre><code>executor = SQLExecutor(db_engine=engine)\nresp = executor(sql_query=\"SELECT TOP 5 id, name FROM users ORDER BY created_at DESC\")\nrows = resp.extras.get(\"db_results\", [])\n</code></pre>"},{"location":"api-reference/tools/builtin-tools/sql/sql_executor/#security-notes","title":"Security &amp; Notes","text":"<ul> <li>Pass parameterized queries where possible to avoid injection (<code>sqlalchemy.text</code> with bindparams).</li> <li>Ensure the engine user has appropriate read-only permissions for analysis scenarios.</li> </ul>"},{"location":"api-reference/tools/builtin-tools/sql/sql_query_generator/","title":"SQL Query Generator","text":"<p>Module: <code>neurosurfer.tools.sql.sql_query_generator.SQLQueryGenerator</code> Pairs with: <code>BaseTool</code> \u2022 <code>ToolSpec</code> \u2022 <code>Toolkit</code></p>"},{"location":"api-reference/tools/builtin-tools/sql/sql_query_generator/#overview","title":"Overview","text":"<p><code>SQLQueryGenerator</code> uses an LLM to produce a syntactically valid T\u2011SQL query from a refined natural-language request and a schema context. It\u2019s designed to incorporate joins, filters, and aggregations, and to avoid dangerous or ambiguous SQL patterns.</p>"},{"location":"api-reference/tools/builtin-tools/sql/sql_query_generator/#when-to-use","title":"When to Use","text":"<ul> <li>After you\u2019ve identified relevant tables and assembled schema context (e.g., using <code>RelevantTableSchemaFinderLLM</code>).</li> <li>When you need an executable query for <code>SQLExecutor</code>.</li> </ul>"},{"location":"api-reference/tools/builtin-tools/sql/sql_query_generator/#spec-inputs-returns","title":"Spec (Inputs &amp; Returns)","text":"Field Type Required Description <code>query</code> <code>string</code> \u2713 A refined question describing exactly what the SQL should do. <p>Requires (via <code>kwargs</code>): <code>schema_context: string</code> \u2014 The schema text for relevant tables.</p> <p>Returns: <code>string</code> \u2014 A single T\u2011SQL query string (<code>extras[\"sql_query\"]</code> mirrors it).</p>"},{"location":"api-reference/tools/builtin-tools/sql/sql_query_generator/#runtime-dependencies-config","title":"Runtime Dependencies &amp; Config","text":"<ul> <li>Constructor: <code>SQLQueryGenerator(llm: BaseModel | None = None, logger: logging.Logger | None = None)</code></li> <li>Prompt: <code>SQL_QUERY_GENERATION_PROMPT</code> (strict rules)</li> <li>Avoid <code>*</code>; prefer <code>TOP n</code>; <code>LIKE '%value%'</code> for text matches; <code>=</code> for numeric/date</li> <li>BIT columns: <code>1/0</code> for TRUE/FALSE</li> <li>Output only the query \u2014 no explanations</li> <li>LLM call: <code>stream=False</code>, <code>max_new_tokens=2000</code>, <code>temperature=0.7</code></li> <li>Post-process: removes code fences/backticks.</li> </ul> <p>Special token: <code>\" [__SQL_QUERY__] \"</code> if you need to tag content.</p>"},{"location":"api-reference/tools/builtin-tools/sql/sql_query_generator/#behavior","title":"Behavior","text":"<ol> <li>Renders system &amp; user prompts using the given <code>schema_context</code> and <code>query</code>.</li> <li>Calls the LLM and extracts <code>response[\"choices\"][0][\"message\"][\"content\"]</code>.</li> <li>Strips code fences, returns <code>ToolResponse(final_answer=False, observation=sql_query, extras={{\"sql_query\": sql_query}})</code>.</li> </ol>"},{"location":"api-reference/tools/builtin-tools/sql/sql_query_generator/#usage","title":"Usage","text":"<pre><code>gen = SQLQueryGenerator(llm=chat_llm)\nsql = gen(query=\"Top 10 customers by total 2024 spend\", schema_context=schema_ctx).extras[\"sql_query\"]\n</code></pre>"},{"location":"api-reference/tools/builtin-tools/sql/sql_query_generator/#best-practices-safety","title":"Best Practices &amp; Safety","text":"<ul> <li>Refine the question before calling this tool; vague prompts produce vague SQL.</li> <li>If a previous attempt failed (missing columns, invalid filters), revise the input and try again (the tool\u2019s prompt explicitly instructs this behavior).</li> <li>Keep generation read-only; validate queries before execution in production.</li> </ul>"},{"location":"api-reference/vectorstores/","title":"Vector Stores","text":"<p>Neurosurfer\u2019s VectorDB layer provides a unified interface for storing and retrieving embeddings across different backends (e.g., Chroma, in-memory). Implementations share the same contract so you can swap backends without changing app code.</p> <ul> <li> <p> Base Concepts</p> <p>Core contracts and data structures. Start here to understand the abstraction used by all backends.</p> <p> <code>BaseVectorDB</code> <code>Doc</code></p> </li> <li> <p> Chroma</p> <p>Persistent, production-ready store using <code>chromadb</code> with <code>PersistentClient</code>, metadata filtering, and similarity search.</p> <p> Documentation</p> </li> <li> <p> In-Memory</p> <p>Lightweight baseline store ideal for tests, demos, and small prototypes. Implements cosine similarity over Python lists.</p> <p> Documentation</p> </li> </ul>"},{"location":"api-reference/vectorstores/#quick-example","title":"\u26a1 Quick Example","text":"<pre><code>from neurosurfer.vectorstores import ChromaVectorStore\nfrom neurosurfer.vectorstores.base import Doc\nfrom neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n\n# Create vector store and embedder\nvectorstore = ChromaVectorStore(\n    collection_name=\"my_docs\",\n    persist_directory=\"./chroma_db\"\n)\nembedder = SentenceTransformerEmbedder(model_name=\"intfloat/e5-large-v2\")\n\n# Create and add documents\ndocs = [\n    Doc(id=\"1\", text=\"Document 1\", \n        embedding=embedder.embed(\"Document 1\"),\n        metadata={\"source\": \"doc1.txt\"}),\n    Doc(id=\"2\", text=\"Document 2\", \n        embedding=embedder.embed(\"Document 2\"),\n        metadata={\"source\": \"doc2.txt\"})\n]\nvectorstore.add_documents(docs)\n\n# Search (requires embedding the query)\nquery_embedding = embedder.embed(\"query\")\nresults = vectorstore.similarity_search(query_embedding, top_k=5)\n\nfor doc, score in results:\n    print(f\"[{score:.3f}] {doc.text}\")\n</code></pre> <p>Tip: When using the ingestion pipeline, see RAG Ingestor for batching, deduplication, and automatic ID strategy. The vector store API is intentionally minimal to keep backends interchangeable.</p>"},{"location":"api-reference/vectorstores/base-vectordb/","title":"BaseVectorDB","text":"<p>Module: <code>neurosurfer.vectorstores.base</code></p> <p>Type: Abstract Base Class</p>"},{"location":"api-reference/vectorstores/base-vectordb/#overview","title":"Overview","text":"<p>The BaseVectorDB is the abstract base class for all vector database implementations in Neurosurfer.</p> <p>Required Abstract Methods:</p> <ul> <li><code>add_documents(docs: List[Doc])</code> - Add documents with embeddings</li> <li><code>similarity_search(query_embedding, top_k, metadata_filter, similarity_threshold)</code> - Search for similar documents</li> <li><code>count()</code> - Get document count</li> <li><code>list_all_documents(metadata_filter)</code> - Retrieve all documents</li> <li><code>delete_documents(ids)</code> - Delete specific documents</li> <li><code>delete_collection()</code> - Delete the collection</li> <li><code>clear_collection()</code> - Clear all documents</li> </ul>"},{"location":"api-reference/vectorstores/base-vectordb/#doc-dataclass","title":"Doc Dataclass","text":"<p>Documents are represented using the <code>Doc</code> dataclass:</p> <pre><code>@dataclass\nclass Doc:\n    id: str\n    text: str\n    embedding: Optional[List[float]] = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"api-reference/vectorstores/base-vectordb/#creating-custom-vector-stores","title":"Creating Custom Vector Stores","text":"<pre><code>from neurosurfer.vectorstores.base import BaseVectorDB, Doc\nfrom typing import List, Dict, Optional, Tuple, Any\n\nclass CustomVectorStore(BaseVectorDB):\n    def __init__(self, collection_name: str):\n        self.collection_name = collection_name\n        # Your initialization\n\n    def add_documents(self, docs: List[Doc]):\n        \"\"\"Add documents with embeddings to the store.\"\"\"\n        # Your implementation\n        pass\n\n    def similarity_search(\n        self,\n        query_embedding: List[float],\n        top_k: int = 5,\n        metadata_filter: Optional[Dict[str, Any]] = None,\n        similarity_threshold: Optional[float] = None\n    ) -&gt; List[Tuple[Doc, float]]:\n        \"\"\"Search for similar documents.\n\n        Returns:\n            List of (Doc, similarity_score) tuples, sorted by score descending.\n        \"\"\"\n        # Your implementation\n        pass\n\n    def count(self) -&gt; int:\n        \"\"\"Return total number of documents.\"\"\"\n        # Your implementation\n        pass\n\n    def list_all_documents(\n        self, \n        metadata_filter: Optional[Dict[str, Any]] = None\n    ) -&gt; List[Doc]:\n        \"\"\"Retrieve all documents, optionally filtered by metadata.\"\"\"\n        # Your implementation\n        pass\n\n    def delete_documents(self, ids: List[str]):\n        \"\"\"Delete specific documents by ID.\"\"\"\n        # Your implementation\n        pass\n\n    def clear_collection(self):\n        \"\"\"Clear all documents but keep collection.\"\"\"\n        # Your implementation\n        pass\n\n    def delete_collection(self):\n        \"\"\"Delete the entire collection.\"\"\"\n        # Your implementation\n        pass\n</code></pre>"},{"location":"api-reference/vectorstores/base-vectordb/#see-also","title":"See Also","text":"<ul> <li>ChromaVectorStore</li> <li>Vector Stores Index</li> </ul>"},{"location":"api-reference/vectorstores/chroma/","title":"Chroma Vector Store","text":"<p>Module: <code>neurosurfer.vectorstores.chroma.ChromaVectorStore</code> Contract: <code>BaseVectorDB</code> \u00b7 Data: <code>Doc</code></p> <p>Chroma-backed vector store using <code>chromadb.PersistentClient</code>. Provides persistent storage, metadata filtering, and similarity search (returns cosine similarity).</p>"},{"location":"api-reference/vectorstores/chroma/#features","title":"Features","text":"<ul> <li>Persistent collections on disk (<code>persist_directory</code>).</li> <li><code>add_documents</code> with upsert (or <code>delete</code>+<code>add</code> depending on Chroma version).</li> <li><code>similarity_search</code> with optional <code>metadata_filter</code> and <code>similarity_threshold</code>.</li> <li><code>list_all_documents</code> with embeddings, metadatas, and texts.</li> <li><code>count</code>, <code>clear_collection</code>, <code>delete_documents</code>, <code>delete_collection</code>.</li> </ul>"},{"location":"api-reference/vectorstores/chroma/#usage","title":"Usage","text":"<pre><code>from neurosurfer.vectorstores import ChromaVectorStore\nfrom neurosurfer.vectorstores.base import Doc\nfrom neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n\nvs = ChromaVectorStore(collection_name=\"my_docs\", persist_directory=\"./chroma_db\")\nembedder = SentenceTransformerEmbedder(model_name=\"intfloat/e5-large-v2\")\n\n# Add docs (ensure embeddings are same dimension across all docs)\ndocs = [\n    Doc(id=\"a1\", text=\"Neural search is fun\", \n        embedding=embedder.embed(\"Neural search is fun\"),\n        metadata={\"topic\": \"search\", \"source\": \"notes.md\"}),\n    Doc(id=\"b2\", text=\"Transformers are powerful\", \n        embedding=embedder.embed(\"Transformers are powerful\"),\n        metadata={\"topic\": \"nlp\"}),\n]\nvs.add_documents(docs)\n\n# Query\nq = embedder.embed(\"power of transformers\")\nhits = vs.similarity_search(q, top_k=3, metadata_filter={\"topic\": \"nlp\"}, similarity_threshold=0.55)\nfor doc, score in hits:\n    print(f\"{score:.3f} :: {doc.metadata.get('topic')} :: {doc.text[:60]}\")\n</code></pre>"},{"location":"api-reference/vectorstores/chroma/#api-notes","title":"API Notes","text":""},{"location":"api-reference/vectorstores/chroma/#__init__collection_name-str-persist_directory-str-chroma_storage","title":"<code>__init__(collection_name: str, persist_directory: str = \"chroma_storage\")</code>","text":"<p>Creates/opens a persistent collection. Internally uses <code>chromadb.PersistentClient(path=...)</code> and <code>get_or_create_collection(name=...)</code>.</p>"},{"location":"api-reference/vectorstores/chroma/#add_documentsdocs-listdoc-none","title":"<code>add_documents(docs: list[Doc]) -&gt; None</code>","text":"<ul> <li>Builds lists of <code>ids</code>, <code>documents</code>, <code>embeddings</code>, <code>metadatas</code>.</li> <li>Uses <code>collection.upsert(...)</code> if available, else falls back to <code>delete(...)</code> + <code>add(...)</code> (supports older Chroma versions).</li> <li>IDs default to <code>Doc.id</code> or fall back to <code>BaseVectorDB._stable_id(doc)</code> if missing.</li> </ul>"},{"location":"api-reference/vectorstores/chroma/#similarity_searchquery_embedding-top_k20-metadata_filternone-similarity_thresholdnone-listtupledoc-float","title":"<code>similarity_search(query_embedding, top_k=20, metadata_filter=None, similarity_threshold=None) -&gt; list[tuple[Doc, float]]</code>","text":"<ul> <li>Applies metadata filters via Chroma\u2019s <code>where</code> clause (values or <code>$in</code> for lists).</li> <li>Fetches up to <code>top_k * 2</code> raw hits, then deduplicates and trims to <code>top_k</code>.</li> <li>Converts Chroma distance to cosine similarity as <code>1.0 - distance</code>.</li> <li>If <code>similarity_threshold</code> is provided, drops hits below it.</li> </ul>"},{"location":"api-reference/vectorstores/chroma/#list_all_documentsmetadata_filternone-listdoc","title":"<code>list_all_documents(metadata_filter=None) -&gt; list[Doc]</code>","text":"<ul> <li>Returns <code>Doc</code> objects with <code>text</code>, <code>embedding</code>, <code>metadata</code>.</li> <li>Uses <code>collection.get(include=[\"documents\", \"metadatas\", \"embeddings\"])</code>.</li> </ul>"},{"location":"api-reference/vectorstores/chroma/#clear_collection-delete_collection-delete_documentsids","title":"<code>clear_collection() / delete_collection() / delete_documents(ids)</code>","text":"<ul> <li><code>clear_collection()</code> recreates the collection (handy for tests).</li> <li><code>delete_collection()</code> deletes and nulls the handle (recreate as needed).</li> <li><code>delete_documents(ids)</code> removes specific records by ID.</li> </ul>"},{"location":"api-reference/vectorstores/chroma/#tips-troubleshooting","title":"Tips &amp; Troubleshooting","text":"<ul> <li>Embeddings: You are responsible for generating embeddings with a consistent dimension per collection.</li> <li>Thresholding: Tune <code>similarity_threshold</code> (as cosine similarity) to filter noisy hits.</li> <li>Filters: For multi-value filters, pass lists (the store converts to <code>$in</code>). Example: <code>{\"topic\": [\"nlp\", \"search\"]}</code>.</li> <li>Persistence: Ensure <code>persist_directory</code> is writable and not on a volatile mount if you expect durability.</li> </ul>"},{"location":"api-reference/vectorstores/in_memory/","title":"In-Memory Vector Store","text":"<p>Module: <code>neurosurfer.vectorstores.in_memory.InMemoryVectorStore</code> Contract: <code>BaseVectorDB</code> \u00b7 Data: <code>Doc</code></p> <p>A lightweight, dependency-free vector store that keeps everything in process memory. Best suited for tests, demos, and small prototypes.</p>"},{"location":"api-reference/vectorstores/in_memory/#features","title":"Features","text":"<ul> <li>No external services; pure Python lists.</li> <li>Cosine similarity implementation (deterministic).</li> <li>Simple CRUD-style operations: <code>add_documents</code>, <code>similarity_search</code>, <code>count</code>, <code>list_all_documents</code>, <code>clear_collection</code>, <code>delete_collection</code>.</li> </ul>"},{"location":"api-reference/vectorstores/in_memory/#usage","title":"Usage","text":"<pre><code>from neurosurfer.vectorstores.in_memory import InMemoryVectorStore\nfrom neurosurfer.vectorstores.base import Doc\nfrom neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n\nembedder = SentenceTransformerEmbedder(model_name=\"intfloat/e5-large-v2\")\ndim = len(embedder.embed(\"probe\"))\n\nvs = InMemoryVectorStore(dim=dim)\n\n# Add docs (must provide embeddings of correct dimension)\ndocs = [\n    Doc(id=\"d1\", text=\"Vector search 101\", embedding=embedder.embed(\"Vector search 101\")),\n    Doc(id=\"d2\", text=\"Intro to embeddings\", embedding=embedder.embed(\"Intro to embeddings\")),\n]\nvs.add_documents(docs)\n\n# Query\nq = embedder.embed(\"learn about embeddings\")\nfor doc, score in vs.similarity_search(q, top_k=2):\n    print(f\"{score:.3f} :: {doc.text}\")\n</code></pre>"},{"location":"api-reference/vectorstores/in_memory/#api-notes","title":"API Notes","text":""},{"location":"api-reference/vectorstores/in_memory/#__init__dim-int","title":"<code>__init__(dim: int)</code>","text":"<p>Initializes storage for vectors of fixed <code>dim</code>ension.</p>"},{"location":"api-reference/vectorstores/in_memory/#add_documentsdocs","title":"<code>add_documents(docs)</code>","text":"<ul> <li>Validates each <code>Doc.embedding</code> is present and length == <code>dim</code>.</li> </ul>"},{"location":"api-reference/vectorstores/in_memory/#similarity_searchquery_embedding-top_k5-metadata_filternone-similarity_thresholdnone","title":"<code>similarity_search(query_embedding, top_k=5, metadata_filter=None, similarity_threshold=None)</code>","text":"<ul> <li>Computes cosine similarity with a simple dot/norm routine.</li> <li>Returns the top <code>k</code> <code>(Doc, score)</code> pairs.</li> </ul>"},{"location":"api-reference/vectorstores/in_memory/#list_all_documents-count-clear_collection-delete_collection","title":"<code>list_all_documents(...)</code>, <code>count()</code>, <code>clear_collection()</code>, <code>delete_collection()</code>","text":"<ul> <li>Straightforward in-memory implementations (no persistence).</li> </ul>"},{"location":"api-reference/vectorstores/in_memory/#tips","title":"Tips","text":"<ul> <li>Use this backend for fast unit tests and local dev where persistence isn\u2019t required.</li> <li>For production or larger corpora, switch to a persistent store (e.g., Chroma) without changing caller code thanks to the shared <code>BaseVectorDB</code> contract.</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Welcome to the living gallery of Neurosurfer examples. This page showcases practical, copy\u2011pasteable snippets to help you get productive quickly. Browse the curated example sets below, then dive into the Basic Examples section for a one\u2011screen tour of the most common patterns (model, agent, RAG, tools).</p>"},{"location":"examples/#available-examples","title":"\ud83d\udcda Available Examples","text":"<ul> <li> <p> Models</p> <p>Initialize and use different chat models and embedders</p> <p> View</p> </li> <li> <p> Agents</p> <p>Initialize and use different agents</p> <p> View</p> </li> <li> <p> Server App</p> <p>A Neurosurfer server app, explained step\u2011by\u2011step\u2014app init, startup wiring, RAG, chat handler, shutdown, cooperative stop, and running the server.</p> <p> View</p> </li> <li> <p> RAG Examples</p> <p>Document Q&amp;A and knowledge retrieval</p> <p> View</p> </li> <li> <p> Custom Tools</p> <p>Creating your own agent tools</p> <p> View</p> </li> </ul>"},{"location":"examples/#basic-examples","title":"\u26a1 Basic Examples","text":"<p>Below are minimal, end\u2011to\u2011end snippets showing a single way to do four core tasks. Keep them simple first; you can mix, match, and scale later.</p>"},{"location":"examples/#model-direct-chat-completion","title":"Model \u2014 direct chat completion","text":"<pre><code>from neurosurfer.models.chat_models.openai import OpenAIModel\n\n# Create a small, fast model\nmodel = OpenAIModel(model_name=\"gpt-4o-mini\")\n\n# One-shot request\nreply = model.ask(user_prompt=\"Say hi in one short sentence.\", stream=False)\nprint(reply.choices[0].message.content)\n</code></pre>"},{"location":"examples/#agent-react-with-no-tools","title":"Agent \u2014 ReAct with no tools","text":"<pre><code>from neurosurfer import ReActAgent\nfrom neurosurfer.models.chat_models.openai import OpenAIModel\n\nmodel = OpenAIModel(model_name=\"gpt-4o-mini\")\nagent = ReActAgent(llm=model, tools=[])\n\nanswer = agent.run(\"What is machine learning, in one sentence?\")\nfor chunk in answer:\n    print(chunk.choices[0].message.content)\n</code></pre>"},{"location":"examples/#rag-files-ingestion-and-retrieval","title":"RAG \u2014 Files ingestion and retrieval","text":"<pre><code>from neurosurfer.models.chat_models.openai import OpenAIModel\nfrom neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\nfrom neurosurfer.vectorstores import ChromaVectorStore\nfrom neurosurfer.rag import RAGIngestor\nfrom neurosurfer import RAGRetrieverAgent\n\n# Components\nmodel = OpenAIModel(model_name=\"gpt-4o-mini\")\nembedder = SentenceTransformerEmbedder(model_name=\"intfloat/e5-large-v2\")\nvectorstore = ChromaVectorStore(collection_name=\"docs\")\n\n# Ingest a single PDF (adjust the path)\ningestor = RAGIngestor(embedder=embedder, vectorstore=vectorstore)\ningestor.ingest_file(\"document.pdf\")\n\n# Ask a question grounded in your file\nrag_agent = RAGRetrieverAgent(llm=model, vectorstore=vectorstore, embedder=embedder)\ncontent = rag_agent.retrieve(\"Summarize the document in two bullet points.\")\nprint(content)\n</code></pre>"},{"location":"examples/#tools-builtin-sql-tools","title":"Tools \u2014 built\u2011in SQL tools","text":"<pre><code>from neurosurfer import ReActAgent\nfrom neurosurfer.models.chat_models.openai import OpenAIModel\nfrom neurosurfer.tools import Toolkit\nfrom neurosurfer.tools.sql import RelevantTableSchemaFinderLLM\nfrom neurosurfer.db.sql_schema_store import SQLSchemaStore\n\nmodel = OpenAIModel(model_name=\"gpt-4o-mini\")\ntoolkit = Toolkit()\nsql_schema_store = SQLSchemaStore(llm=model, db_uri=\"sqlite:///example.db\")\ntoolkit.register_tool(RelevantTableSchemaFinderLLM(llm=model, sql_schema_store=sql_schema_store))\n\n# Attach a single built-in tool\nagent = ReActAgent(llm=model, toolkit=toolkit)\n\nprint(agent.run(\"What is the stock price of Apples this week?\"))\n</code></pre>"},{"location":"examples/#where-to-go-next","title":"\ud83d\udcd6 Where to go next","text":"<ul> <li>Try the Server App Example for a complete, production-ready app</li> <li>Explore and use more Models Examples</li> <li>Build agentic solutions with Agents Examples</li> <li>Build document Q&amp;A with RAG Examples</li> <li>Extend functionality via Custom Tools</li> </ul>"},{"location":"examples/agents-examples/","title":"Agent Examples","text":"<p>Practical, copy\u2011pasteable examples for Neurosurfer\u2019s agents: - ReActAgent \u2014 tool\u2011using, step\u2011by\u2011step reasoning - RAGRetrieverAgent \u2014 retrieval\u2011augmented prompting with token\u2011aware context budgeting - SQLAgent \u2014 database Q&amp;A (schema \u2192 query \u2192 execution \u2192 NL format) - ToolsRouterAgent \u2014 lightweight, single\u2011step tool picker</p> <p>Each example uses the same BaseModel interface shown in model examples.</p>"},{"location":"examples/agents-examples/#reactagent-quick-start","title":"\ud83e\udd16 ReActAgent \u2014 quick start","text":"<p>The ReAct agent thinks \u2192 acts (uses tools) \u2192 observes \u2192 repeats, until it emits a final answer.</p> <p>If you enabled <code>ReActAgentConfig.skip_special_tokens=True</code>, the final\u2011answer delimiters are not emitted in the stream.</p>"},{"location":"examples/agents-examples/#hello-tools-uifriendly","title":"Hello, tools (UI\u2011friendly)","text":"<pre><code>from neurosurfer.agents.react import ReActAgent, ReActAgentConfig\nfrom neurosurfer.tools import Toolkit\nfrom neurosurfer.tools.common import CalculatorTool\nfrom neurosurfer.models.chat_models.openai import OpenAIModel\n\n# 1) Model\nllm = OpenAIModel(model_name=\"gpt-4o-mini\")\n\n# 2) Tools (use a built-in calculator for simplicity)\ntk = Toolkit()\ntk.register_tool(CalculatorTool())\n\n# 3) Agent\nagent = ReActAgent(\n    toolkit=tk,\n    llm=llm,\n    specific_instructions=\"Be concise.\",\n    config=ReActAgentConfig(temperature=0.2, max_new_tokens=256)\n)\n\n# 4) Run (stream to your UI)\nfor chunk in agent.run(\"Solve: (42 * 7) - 5^2\"):\n    # chunk is either plain text or ChatCompletionChunk content\n    print(chunk, end=\"\", flush=True)\nprint()\n</code></pre>"},{"location":"examples/agents-examples/#custom-tool-with-strict-input-validation","title":"Custom tool with strict input validation","text":"<p>Assume your <code>Toolkit</code> validates inputs based on a spec. Here\u2019s a tiny echo tool:</p> <pre><code>from neurosurfer.tools.base_tool import BaseTool, ToolResponse\nfrom neurosurfer.tools import Toolkit\n\nclass EchoTool(BaseTool):\n    name = \"echo\"\n    description = \"Repeat a message. Inputs: message (str).\"\n    def __call__(self, *, message: str, **_):\n        return ToolResponse(observation=f\"[echo] {message}\")\n\ntk = Toolkit()\ntk.register_tool(EchoTool())\n\nagent = ReActAgent(toolkit=tk, llm=llm, specific_instructions=\"Use tools when helpful.\")\nfor chunk in agent.run(\"Say hello using the echo tool. Message='Hello world!'\", temperature=0.1):\n    print(chunk, end=\"\")\n</code></pre>"},{"location":"examples/agents-examples/#ragagent-retrieval-tokenaware-prompts","title":"\ud83d\udcc4 RAGAgent \u2014 retrieval + token\u2011aware prompts","text":"<p>Use any <code>BaseVectorDB</code> and <code>BaseEmbedder</code>. The agent retrieves, builds a formatted context block, trims it to fit the model\u2019s window, and returns budgets you can pass into your generation call.</p>"},{"location":"examples/agents-examples/#retrieve-then-generate","title":"Retrieve then generate","text":"<pre><code>from neurosurfer.agents.rag import RAGAgent, RAGAgentConfig\nfrom neurosurfer.vectorstores.chroma import ChromaVectorStore\nfrom neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\nfrom neurosurfer.models.chat_models.openai import OpenAIModel\n\n# Components\nstore = ChromaVectorStore(collection_name=\"docs\")\nembedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\nllm = OpenAIModel(model_name=\"gpt-4o-mini\")\n\n# Agent\nrag = RAGAgent(llm=llm, vectorstore=store, embedder=embedder, config=RAGAgentConfig(top_k=6))\n\n# Retrieve + build context\nres = rag.retrieve(\n    user_query=\"Summarize how the ingestion pipeline works.\",\n    base_system_prompt=\"You are a precise technical writer.\",\n    base_user_prompt=\"Use the following context to answer succinctly:\\n\\n{context}\\n\\nQuestion: Summarize the ingestion pipeline.\",\n)\n\n# Now call the model with trimmed budgets\nanswer = llm.ask(\n    user_prompt=res.base_user_prompt.replace(\"{context}\", res.context),\n    system_prompt=res.base_system_prompt,\n    max_new_tokens=res.max_new_tokens,     # dynamically chosen to fit\n    temperature=0.2,\n)\nprint(answer.choices[0].message.content)\n</code></pre>"},{"location":"examples/agents-examples/#pick-top-files-by-grouped-chunk-hits","title":"Pick top files by grouped chunk hits","text":"<pre><code>files = rag.pick_files_by_grouped_chunk_hits(\n    section_query=\"vector store indexing rules\",\n    candidate_pool_size=300,\n    n_files=5\n)\nprint(files)  # top relevant file paths from your corpus\n</code></pre> <p>The retriever exposes: <code>context</code>, <code>max_new_tokens</code>, <code>generation_budget</code>, and the retrieved <code>docs</code> &amp; <code>distances</code> for transparency/debugging.</p>"},{"location":"examples/agents-examples/#sqlagent-naturallanguage-database-qa","title":"\ud83d\uddc4\ufe0f SQLAgent \u2014 natural\u2011language database Q&amp;A","text":"<p>The SQL agent wires the ReAct loop with SQL\u2011specific tools: schema discovery, query generation, execution, and result formatting.</p>"},{"location":"examples/agents-examples/#connect-and-query-sqlite-example","title":"Connect and query (SQLite example)","text":"<pre><code>from neurosurfer.agents.sql_agent import SQLAgent\nfrom neurosurfer.models.chat_models.openai import OpenAIModel\n\nllm = OpenAIModel(model_name=\"gpt-4o-mini\")\nagent = SQLAgent(\n    llm=llm,\n    db_uri=\"sqlite:///my.db\",\n    sample_rows_in_table_info=3,\n    verbose=True,\n)\n\n# (Optional) Ensure schema store is ready (train/summarize once and cache)\nfor chunk in agent.train(summarize=True, force=False):\n    print(chunk, end=\"\")\n\n# Ask the database\nfor piece in agent.run(\"Top 5 products by revenue last quarter, with totals.\"):\n    print(piece, end=\"\")\n</code></pre>"},{"location":"examples/agents-examples/#insights-and-error-recovery","title":"Insights and error recovery","text":"<pre><code># The agent can also compute insights / stats via tools (registered in get_toolkit())\n# If a generated SQL has an error, the agent refines and retries using observations.\nfor x in agent.run(\"Average order value per month for 2024; format as a small table.\"):\n    print(x, end=\"\")\n</code></pre>"},{"location":"examples/agents-examples/#toolsrouteragent-let-the-llm-choose-the-tool","title":"\ud83d\udd00 ToolsRouterAgent \u2014 let the LLM choose the tool","text":"<p>Use this when you want a light router that picks a single tool and runs it (no multi\u2011step planning).</p> <pre><code>from neurosurfer.agents.tools_router import ToolsRouterAgent, ToolsRouterConfig, RouterRetryPolicy\nfrom neurosurfer.tools import Toolkit\nfrom neurosurfer.tools.common import CalculatorTool\nfrom neurosurfer.models.chat_models.openai import OpenAIModel\n\nllm = OpenAIModel(model_name=\"gpt-4o-mini\")\n\ntk = Toolkit()\ntk.register_tool(CalculatorTool())          # add your tools\n# tk.register_tool(WeatherTool())\n# tk.register_tool(WebSearchTool())\n\nrouter = ToolsRouterAgent(\n    toolkit=tk,\n    llm=llm,\n    verbose=True,\n    config=ToolsRouterConfig(\n        allow_input_pruning=True,\n        repair_with_llm=True,\n        return_stream_by_default=True,\n        retry=RouterRetryPolicy(max_route_retries=2, max_tool_retries=1, backoff_sec=0.7),\n    ),\n)\n\n# Streaming\nfor chunk in router.run(\"Compute 3.5% of 12000 and explain briefly.\", stream=True):\n    print(chunk, end=\"\")\n\n# Non-stream\ntext = router.run(\"Quickly add 17 + 29\", stream=False)\nprint(\"\\nRESULT:\", text)\n</code></pre>"},{"location":"examples/agents-examples/#tips","title":"Tips","text":"<ul> <li>Keep tools small and deterministic. The ReAct loop improves when each tool has clear inputs/outputs.</li> <li>Use streaming for UX. All agents integrate nicely with token/line streaming UIs.</li> <li>Pass runtime context (e.g., <code>db_engine</code>, <code>vectorstore</code>, <code>embedder</code>) through the agent/tool call as shown.</li> <li>Budget tokens. With RAGAgent, always use <code>res.max_new_tokens</code> to avoid overruns.</li> <li>Cache SQL schema summaries. <code>SQLAgent.train(...)</code> will speed up later queries.</li> <li>Router retries. Tune <code>RouterRetryPolicy</code> for your latency/error profile; set strict mode by turning off <code>allow_input_pruning</code>.</li> </ul>"},{"location":"examples/custom-tools-examples/","title":"Custom Tools \u2014 Examples &amp; Patterns","text":"<p>This page shows how to create custom tools for Neurosurfer, validate their inputs with <code>ToolSpec</code>, register them in a <code>Toolkit</code>, and use them from agents (e.g., <code>ReActAgent</code>). It complements the <code>Toolkit</code>, <code>ToolSpec</code>, and <code>BaseTool</code> modules by providing practical, copy\u2011pasteable examples, including streaming tools and memory\u2011passing via <code>extras</code>.</p>"},{"location":"examples/custom-tools-examples/#overview","title":"\ud83d\udce6 Overview","text":"<ul> <li><code>BaseTool</code>: subclass this and implement <code>__call__(...) -&gt; ToolResponse</code>.</li> <li><code>ToolSpec</code>: defines name, description, when_to_use, inputs (<code>ToolParam</code> list), and returns (<code>ToolReturn</code>). Used for auto\u2011docs and strict runtime validation via <code>check_inputs(...)</code>.</li> <li><code>Toolkit</code>: a registry for tools (<code>register_tool(...)</code>, <code>get_tools_description()</code>).</li> <li>Agents (e.g., <code>ReActAgent</code>): discover tools via <code>Toolkit</code>, validate tool calls, and pass runtime context (e.g., <code>llm</code>, <code>db_engine</code>, <code>vectorstore</code>).</li> </ul> <p>Validation Rules Recap - Required inputs must be present. - No extra/unknown inputs allowed. - Types must match (<code>string</code>, <code>integer</code>, <code>number</code>, <code>boolean</code>, <code>array</code>, <code>object</code>).</p>"},{"location":"examples/custom-tools-examples/#smallest-possible-tool","title":"Smallest possible tool","text":"<p>A simple Echo tool that returns a string. It illustrates <code>ToolSpec</code> and <code>ToolResponse</code>.</p> <pre><code>from neurosurfer.tools.base_tool import BaseTool, ToolResponse\nfrom neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n\nclass EchoTool(BaseTool):\n    spec = ToolSpec(\n        name=\"echo\",\n        description=\"Repeat a message back to the user.\",\n        when_to_use=\"When you need to echo/confirm user input.\",\n        inputs=[\n            ToolParam(name=\"message\", type=\"string\", description=\"The text to echo\", required=True),\n        ],\n        returns=ToolReturn(type=\"string\", description=\"The echoed message\"),\n    )\n\n    def __call__(self, *, message: str, **_) -&gt; ToolResponse:\n        return ToolResponse(final_answer=False, observation=f\"[echo] {message}\")\n</code></pre> <p>Register in a toolkit:</p> <pre><code>from neurosurfer.tools import Toolkit\n\ntk = Toolkit()\ntk.register_tool(EchoTool())\nprint(tk.get_tools_description())  # Markdown description for agents\n</code></pre>"},{"location":"examples/custom-tools-examples/#tool-with-extras-memory-passing","title":"Tool with extras (memory passing)","text":"<p>Use <code>extras</code> to pass intermediate results to subsequent tool calls in a ReAct loop.</p> <pre><code>from neurosurfer.tools.base_tool import BaseTool, ToolResponse\nfrom neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n\nclass SumTool(BaseTool):\n    spec = ToolSpec(\n        name=\"sum_numbers\",\n        description=\"Sum a list of numbers.\",\n        when_to_use=\"When you need the sum of numeric values.\",\n        inputs=[\n            ToolParam(name=\"values\", type=\"array\", description=\"List of numbers\", required=True),\n        ],\n        returns=ToolReturn(type=\"number\", description=\"The total sum\"),\n    )\n\n    def __call__(self, *, values: list, **_) -&gt; ToolResponse:\n        total = sum(float(x) for x in values)\n        return ToolResponse(\n            final_answer=False,\n            observation=f\"Sum = {total}\",\n            extras={\"sum\": total}  # becomes available to the next tool call\n        )\n</code></pre> <p>In a <code>ReActAgent</code>, those <code>extras</code> will be merged into the next tool\u2019s inputs (as the agent code demonstrates).</p>"},{"location":"examples/custom-tools-examples/#streaming-tool-generator-observation","title":"Streaming tool (generator observation)","text":"<p>Return a generator for incremental output. Agents/UIs can stream it in real time.</p> <pre><code>from typing import Generator\nfrom neurosurfer.tools.base_tool import BaseTool, ToolResponse\nfrom neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n\nclass StreamLinesTool(BaseTool):\n    spec = ToolSpec(\n        name=\"stream_lines\",\n        description=\"Emit N lines, one by one.\",\n        when_to_use=\"When user wants incremental updates.\",\n        inputs=[\n            ToolParam(name=\"n\", type=\"integer\", description=\"Number of lines to emit\", required=True),\n        ],\n        returns=ToolReturn(type=\"string\", description=\"A stream of lines (chunked)\"),\n    )\n\n    def __call__(self, *, n: int, **_) -&gt; ToolResponse:\n        def _gen() -&gt; Generator[str, None, None]:\n            for i in range(1, int(n) + 1):\n                yield f\"line {i}\\n\"\n        return ToolResponse(final_answer=False, observation=_gen())\n</code></pre>"},{"location":"examples/custom-tools-examples/#tool-that-uses-runtime-context-eg-llm","title":"Tool that uses runtime context (e.g., <code>llm</code>)","text":"<p>Agents inject runtime context keys (e.g., <code>llm</code>, <code>db_engine</code>, <code>vectorstore</code>). Your tool can accept them as kwargs.</p> <pre><code>from neurosurfer.tools.base_tool import BaseTool, ToolResponse\nfrom neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n\nclass SummarizeWithLLM(BaseTool):\n    spec = ToolSpec(\n        name=\"llm_summarize\",\n        description=\"Summarize a passage using the active LLM context.\",\n        when_to_use=\"When a short summary is needed.\",\n        inputs=[\n            ToolParam(name=\"text\", type=\"string\", description=\"Text to summarize\", required=True),\n        ],\n        returns=ToolReturn(type=\"string\", description=\"A concise summary\"),\n    )\n\n    def __call__(self, *, text: str, llm=None, **_) -&gt; ToolResponse:\n        if llm is None:\n            return ToolResponse(final_answer=True, observation=\"No LLM available.\")\n        resp = llm.ask(user_prompt=f\"Summarize in 3 bullet points:\\n\\n{text}\", temperature=0.2, max_new_tokens=200)\n        out = resp.choices[0].message.content\n        return ToolResponse(final_answer=True, observation=out)\n</code></pre> <p>Register and use in an agent:</p> <pre><code>from neurosurfer.tools import Toolkit\nfrom neurosurfer.agents import ReActAgent\nfrom neurosurfer.models.chat_models.openai_model import OpenAIModel\n\nllm = OpenAIModel(model_name=\"gpt-4o-mini\")\ntk = Toolkit()\ntk.register_tool(SummarizeWithLLM())\n\nagent = ReActAgent(toolkit=tk, llm=llm, verbose=True)\nfor chunk in agent.run(\"Use llm_summarize on: 'Transformers are sequence models...'\"):\n    print(chunk, end=\"\")\n</code></pre>"},{"location":"examples/custom-tools-examples/#validation-failure-how-it-looks","title":"Validation failure (how it looks)","text":"<p><code>Toolkit</code> + agent will reject malformed inputs based on the tool\u2019s spec. You can also call <code>spec.check_inputs(...)</code> manually.</p> <pre><code>from neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n\nspec = ToolSpec(\n    name=\"calc\",\n    description=\"Add two numbers.\",\n    when_to_use=\"Basic arithmetic when both inputs are provided.\",\n    inputs=[\n        ToolParam(name=\"a\", type=\"number\", description=\"First\", required=True),\n        ToolParam(name=\"b\", type=\"number\", description=\"Second\", required=True),\n    ],\n    returns=ToolReturn(type=\"number\", description=\"Sum\"),\n)\n\n# Missing 'b' will raise ValueError\ntry:\n    spec.check_inputs({\"a\": 10})\nexcept ValueError as e:\n    print(\"Validation error:\", e)\n</code></pre>"},{"location":"examples/custom-tools-examples/#endtoend-with-reactagent","title":"End\u2011to\u2011end with <code>ReActAgent</code>","text":"<pre><code>from neurosurfer.tools import Toolkit\nfrom neurosurfer.agents import ReActAgent\nfrom neurosurfer.models.chat_models.openai_model import OpenAIModel\n\n# Register your custom tools\ntk = Toolkit()\ntk.register_tool(EchoTool())\ntk.register_tool(SumTool())\ntk.register_tool(StreamLinesTool())\n\n# Model + Agent\nllm = OpenAIModel(model_name=\"gpt-4o-mini\")\nagent = ReActAgent(toolkit=tk, llm=llm, verbose=True)\n\n# The agent will think \u2192 act \u2192 observe \u2192 repeat, picking tools by spec\nfor piece in agent.run(\"Add [2, 3, 5], then echo the total. Stream 3 lines at the end.\"):\n    print(piece, end=\"\")\n</code></pre>"},{"location":"examples/custom-tools-examples/#design-tips","title":"Design tips","text":"<ul> <li>Be strict in specs: keep inputs minimal and types exact. Agents become more reliable.</li> <li>Keep tools single\u2011purpose: complex tasks emerge from composing small tools.</li> <li>Use <code>extras</code> for intermediate state (IDs, partial results, structured objects).</li> <li>Prefer streaming where latency matters; return a generator for <code>observation</code>.</li> <li>Document \u201cwhen to use\u201d clearly \u2014 it improves tool selection in both ReAct and router agents.</li> <li>Log sparingly: tools may run often; prefer concise, actionable logs.</li> </ul>"},{"location":"examples/custom-tools-examples/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>\u201cUnexpected inputs\u201d \u2192 the LLM/tool passed extra fields; remove or add them to the spec.</li> <li>Type mismatch \u2192 ensure <code>integer</code> vs <code>number</code> and lists (<code>array</code>) are correct.</li> <li>Agent didn\u2019t pick your tool \u2192 improve <code>description</code> and <code>when_to_use</code>; reduce overlap with other tools.</li> <li>No runtime context \u2192 ensure the agent passes <code>llm</code>/<code>db_engine</code>/<code>vectorstore</code> into <code>__call__</code> kwargs.</li> </ul>"},{"location":"examples/custom-tools-examples/#minimal-checklist-before-registering-a-tool","title":"Minimal checklist (before registering a tool)","text":"<ul> <li>Inherit from <code>BaseTool</code></li> <li>Provide a valid <code>ToolSpec</code> (<code>validate()</code> passes)</li> <li>Implement <code>__call__(**kwargs) -&gt; ToolResponse</code></li> <li>Return <code>final_answer=True</code> if the tool\u2019s output should be surfaced directly</li> <li>Register with <code>Toolkit.register_tool(...)</code></li> </ul> <p>All set. You can now plug custom tools into ReActAgent, SQLAgent, and the ToolsRouterAgent with predictable behavior.</p>"},{"location":"examples/models-examples/","title":"Models","text":"<p>This page shows how to use Neurosurfer's chat models (OpenAI-compatible and local Transformers) and embedders with a small, practical API. All chat models share the same <code>BaseModel.ask(...)</code> interface and produce OpenAI\u2011compatible Pydantic responses, so you can swap backends without changing your app code.</p>"},{"location":"examples/models-examples/#at-a-glance","title":"\ud83d\udd0e At a glance","text":"<pre><code>from neurosurfer.models.chat_models.openai_model import OpenAIModel\n\nmodel = OpenAIModel(model_name=\"gpt-4o-mini\")  # or any OpenAI-compatible name\nres = model.ask(\"Say hi in one sentence.\", temperature=0.2)\nprint(res.choices[0].message.content)\n</code></pre>"},{"location":"examples/models-examples/#chat-models","title":"\ud83d\udcac Chat models","text":"<p>Neurosurfer unifies different backends behind a single abstract class:</p> <ul> <li><code>BaseModel</code> \u2014 defines the interface (non-stream + stream, stop/interrupt, stop\u2011words, thinking tag suppression)</li> <li><code>OpenAIModel</code> \u2014 uses OpenAI/compatible servers (OpenAI Cloud, LM Studio, vLLM, Ollama, etc.)</li> <li><code>TransformersModel</code> \u2014 runs local Hugging Face models with <code>transformers</code>, optional 4\u2011bit loading</li> <li><code>UnslothModel</code> \u2014 runs local and Hugging Face models with <code>unsloth</code></li> <li><code>LlamaCppModel</code> \u2014 runs local and Hugging Face GGUF models with <code>llamacpp</code></li> </ul>"},{"location":"examples/models-examples/#return-types-openai-style","title":"Return types (OpenAI-style)","text":"<ul> <li>Non-streaming: <code>ChatCompletionResponse</code> \u2192 <code>response.choices[0].message.content</code></li> <li>Streaming: generator of <code>ChatCompletionChunk</code> \u2192 <code>chunk.choices[0].delta.content</code></li> </ul>"},{"location":"examples/models-examples/#nonstreaming-completion-openaicompatible","title":"Non\u2011streaming completion (OpenAI\u2011compatible)","text":"<pre><code>from neurosurfer.models.chat_models.openai_model import OpenAIModel\n\n# Works with: OpenAI Cloud, LM Studio, vLLM OpenAI server, Ollama (OpenAI compat)\nmodel = OpenAIModel(\n    model_name=\"gpt-4o-mini\",\n    # base_url=None uses OpenAI cloud; set base_url for self-hosted OpenAI-compatible servers\n    # base_url=\"http://localhost:8000/v1\",  # vLLM example\n    # api_key=\"sk-...\",                      # or \"ollama\"/\"lm-studio\" for local compat servers\n)\n\nresp = model.ask(\n    user_prompt=\"Explain RAG in 2 lines.\",\n    temperature=0.3,\n    max_new_tokens=256,\n)\nprint(resp.choices[0].message.content)\n</code></pre> <p>Tip: You can keep a single code path while switching providers just by changing <code>base_url</code> and <code>api_key</code>.</p>"},{"location":"examples/models-examples/#streaming-completion-tokenbytoken","title":"Streaming completion (token\u2011by\u2011token)","text":"<pre><code>from neurosurfer.models.chat_models.openai_model import OpenAIModel\n\nmodel = OpenAIModel(model_name=\"gpt-4o-mini\")\n\nfor chunk in model.ask(\"Stream me a haiku about GPUs.\", stream=True):\n    delta = chunk.choices[0].delta.content or \"\"\n    print(delta, end=\"\", flush=True)\nprint()  # newline\n</code></pre>"},{"location":"examples/models-examples/#system-prompt-chat-history","title":"System prompt + chat history","text":"<pre><code>from neurosurfer.models.chat_models.openai_model import OpenAIModel\n\nmodel = OpenAIModel(model_name=\"gpt-4o-mini\")\n\nhistory = [\n    {\"role\": \"user\", \"content\": \"Who won the 2018 FIFA World Cup?\"},\n    {\"role\": \"assistant\", \"content\": \"France.\"},\n]\n\nresp = model.ask(\n    user_prompt=\"And who was the top scorer? Reply in 1 short sentence.\",\n    system_prompt=\"You are a concise sports assistant.\",\n    chat_history=history,\n    temperature=0.2,\n)\nprint(resp.choices[0].message.content)\n</code></pre>"},{"location":"examples/models-examples/#stop-words-reasoning-thinking-suppression","title":"Stop words &amp; reasoning / thinking suppression","text":"<pre><code>from neurosurfer.models.chat_models.openai_model import OpenAIModel\nfrom neurosurfer.config import config\n\n# Example: cut generation as soon as model emits any of these substrings\nmodel = OpenAIModel(\n    model_name=\"gpt-4o-mini\",\n    stop_words=[\"\\n\\n\", \"&lt;END&gt;\", \"###\"],\n    # Strip/suppress internal chain-of-thought style tags if present\n    # (e.g., &lt;think&gt;...&lt;/think&gt;, &lt;analysis&gt;...&lt;/analysis&gt;, etc.)\n    # To suppress: set enable_thinking=False via BaseModel init (configurable)\n)\n\ntext = \"Give me a step-by-step plan to brew coffee at home.\"\nresp = model.ask(text, temperature=0.4, max_new_tokens=300)\nprint(resp.choices[0].message.content)\n</code></pre> <p>Behind the scenes, <code>BaseModel</code> scans streamed text for the first matching stop token and truncates output. When thinking is disabled, recognized tags like <code>&lt;think&gt;...&lt;/think&gt;</code> are removed from the stream.</p>"},{"location":"examples/models-examples/#local-transformers-cpugpu-optional-4bit","title":"Local Transformers (CPU/GPU), optional 4\u2011bit","text":"<pre><code>from neurosurfer.models.chat_models.transformers_model import TransformersModel\n\n# Example model: a small instruct model from HF (pick something that fits your GPU/CPU)\nlm = TransformersModel(\n    model_name=\"microsoft/Phi-3-mini-4k-instruct\",   # change as needed\n    max_seq_length=4096,\n    load_in_4bit=True,                               # try 4-bit to save VRAM if supported\n    verbose=True,\n)\n\n# Non\u2011streaming\nout = lm.ask(\"Summarize attention mechanisms in one paragraph.\", temperature=0.3)\nprint(out.choices[0].message.content)\n\n# Streaming\nfor ch in lm.ask(\"Stream 2 facts about Transformers (the model).\", stream=True):\n    print(ch.choices[0].delta.content or \"\", end=\"\")\nprint()\n</code></pre> <p>The <code>TransformersModel</code> picks <code>cuda</code> if available, otherwise CPU, and selects a sensible dtype (<code>bfloat16</code> on GPU, <code>float32</code> on CPU).</p>"},{"location":"examples/models-examples/#embedders-for-rag-similarity","title":"\ud83d\udd22 Embedders (for RAG &amp; similarity)","text":"<p>Neurosurfer also standardizes embedding models with a simple <code>BaseEmbedder</code> \u2192 <code>embed(...)</code> API.</p>"},{"location":"examples/models-examples/#sentencetransformer-hf-quick-start","title":"SentenceTransformer (HF) quick start","text":"<pre><code>from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n\nemb = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")  # pick a model that fits your device\n\n# Single text \u2192 one vector\nv = emb.embed(\"vector search is fun\")\nprint(len(v))  # e.g., 384 or 768 depending on the model\n\n# Batch texts \u2192 list of vectors\nB = emb.embed([\"first text\", \"second text\"])\nprint(len(B), len(B[0]))\n</code></pre> <p>Tip: pass <code>quantized=True</code> (default) to attempt 8\u2011bit loading where supported, or <code>quantized=False</code> for full precision.</p>"},{"location":"examples/models-examples/#patterns-tips","title":"\ud83d\udccc Patterns &amp; tips","text":"<ul> <li>Swap backends, keep code: both <code>OpenAIModel</code> and <code>TransformersModel</code> return the same Pydantic types.</li> <li>Streaming UI: iterate chunks and read <code>chunk.choices[0].delta.content</code>.</li> <li>Safety &amp; control: use <code>stop_words</code>, <code>enable_thinking=False</code>, and the stop signal to keep outputs tidy.</li> <li>Latency/Cost: set smaller <code>max_new_tokens</code> and lower <code>temperature</code> for fast, deterministic flows.</li> <li>Reusability: create one model instance and share it across requests where possible.</li> </ul>"},{"location":"examples/rag-examples/","title":"RAG Examples","text":"<p>This page gives a practical, end\u2011to\u2011end tour of Neurosurfer\u2019s RAG building blocks: the <code>Chunker</code> (structure\u2011aware document splitting) and the <code>RAGIngestor</code> (read \u2192 chunk \u2192 embed \u2192 store). You\u2019ll find copy\u2011pasteable snippets for common workflows: chunking files, registering custom strategies, ingesting files/directories/raw text/ZIPs/URLs, retrieving top\u2011K matches, and wiring progress/cancellation.</p>"},{"location":"examples/rag-examples/#chunker-overview","title":"\ud83e\udde9 Chunker \u2014 Overview","text":"<p>The <code>Chunker</code> intelligently splits content into semantically meaningful chunks while preserving structure. It supports multiple strategies out of the box and chooses an approach based on file type and heuristics:</p> <ul> <li>Python: AST\u2011aware chunking</li> <li>JS/TS/React: structure\u2011aware chunking</li> <li>Markdown/Text: header/section aware for docs, line/char for prose</li> <li>JSON: object/array aware chunking</li> <li>Comment\u2011aware filtering, configurable overlaps</li> <li>Extensible via custom strategies and handlers</li> </ul> <p>Internally, it uses configuration from <code>config.chunker</code> (overlap, sizes, etc.).</p>"},{"location":"examples/rag-examples/#quick-start","title":"Quick start","text":"<pre><code>from neurosurfer.rag.chunker import Chunker\n\nchunker = Chunker()\n\ncode = '''\ndef area(r):\n    # Compute circle area\n    return 3.14159 * r * r\n'''\n\nchunks = chunker.chunk(code, file_path=\"utils.py\")\nfor i, ch in enumerate(chunks, 1):\n    print(f\"[{i}] {ch[:80]}\")\n</code></pre>"},{"location":"examples/rag-examples/#register-a-custom-strategy-by-extension","title":"Register a custom strategy (by extension)","text":"<p>Use a simple function <code>(text, file_path) -&gt; List[str]</code> and map it to one or more extensions.</p> <pre><code>from neurosurfer.rag.chunker import Chunker\n\ndef my_double_newline(text: str, file_path: str | None = None):\n    # Split on blank lines; trim tiny fragments\n    parts = [p.strip() for p in text.split(\"\\n\\n\")]\n    return [p for p in parts if len(p) &gt;= 20]\n\nchunker = Chunker()\nchunker.register({\".custom\", \".note\"}, my_double_newline)\n\nsample = \"A block...\\n\\nAnother block...\\n\\nShort\\n\"\nprint(chunker.chunk(sample, file_path=\"notes.custom\"))\n</code></pre>"},{"location":"examples/rag-examples/#custom-handler-full-control","title":"Custom handler (full control)","text":"<p>Handlers can accept <code>config</code> and other meta; useful for advanced routing or parameterized chunking.</p> <pre><code>from typing import Optional, List\nfrom neurosurfer.rag.chunker import Chunker, ChunkerConfig\n\ndef my_handler(text: str, *, file_path: Optional[str] = None, config: Optional[ChunkerConfig] = None) -&gt; List[str]:\n    # Example: fixed-size char windows with slight overlap from cfg\n    size = (config.char_chunk_size if config else 800)\n    overlap = (config.char_overlap if config else 60)\n    out = []\n    i = 0\n    while i &lt; len(text):\n        out.append(text[i:i+size])\n        i += max(1, size - overlap)\n    return out\n\nchunker = Chunker()\n# Register a named handler and bind it to an extension (optional)\nchunker.register_handler(\"wide_chars\", my_handler)\nchunker.map_extension_to_handler(\".log\", \"wide_chars\")\n\nlog_text = \"...\"  # long logs\nprint(len(chunker.chunk(log_text, file_path=\"app.log\")))\n</code></pre> <p>If you only need simple extension\u2192function mapping, <code>register({'.ext'}, fn)</code> is enough. Use handlers when you need access to the <code>ChunkerConfig</code>.</p>"},{"location":"examples/rag-examples/#rag-ingestor-overview","title":"\ud83d\udce5 RAG Ingestor \u2014 Overview","text":"<p><code>RAGIngestor</code> is the production\u2011grade ingestion pipeline: read files/directories/raw text/URLs/ZIPs \u2192 chunk \u2192 embed (batch) \u2192 dedupe \u2192 persist to a vector DB.</p> <p>Key features: multi\u2011source input, parallel processing, progress callbacks, cancellation, content\u2011hash dedupe, and metadata preservation.</p> <p>Typical setup:</p> <pre><code>from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\nfrom neurosurfer.vectorstores import ChromaVectorStore   # or any BaseVectorDB implementation\nfrom neurosurfer.rag.ingestor import RAGIngestor\n\nembedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\nvs = ChromaVectorStore(collection_name=\"docs\")  # must implement BaseVectorDB API\n\ningestor = RAGIngestor(\n    embedder=embedder,\n    vector_store=vs,\n    batch_size=64,\n    max_workers=4,\n    deduplicate=True,\n    normalize_embeddings=True,\n)\n</code></pre>"},{"location":"examples/rag-examples/#add-sources","title":"Add sources","text":"<pre><code># 1) Add individual files\ningestor.add_files([\"README.md\", \"guide.md\", \"src/app.py\"])\n\n# 2) Recursively add a directory (skips common junk like node_modules, .git, etc.)\ningestor.add_directory(\"./docs\")\n\n# 3) Raw texts (with optional per\u2011item metadata)\ningestor.add_texts(\n    [\"Custom paragraph 1\", \"Custom paragraph 2\"],\n    base_id=\"manual\",\n    metadatas=[{\"section\": \"intro\"}, {\"section\": \"notes\"}],\n)\n\n# 4) ZIP archive (safe extraction to a temp folder, then indexed like a dir)\ningestor.add_zipfile(\"./handbook.zip\")\n\n# 5) URLs (requires a fetcher; here\u2019s a tiny example)\ndef fetch(url: str) -&gt; str | None:\n    # return cleaned text from URL (left as an exercise: requests + readability/bs4)\n    return None\n\ningestor.add_urls([\"https://example.com/page1\"], fetcher=fetch)\n</code></pre>"},{"location":"examples/rag-examples/#build-ingest","title":"Build / ingest","text":"<pre><code>stats = ingestor.build()\nprint(stats)  # {'status': 'ok', 'sources': ..., 'chunks': ..., 'unique_chunks': ..., 'added': ...}\n</code></pre>"},{"location":"examples/rag-examples/#with-progress-callback-and-cancellation","title":"With progress callback and cancellation","text":"<pre><code>import threading, time\n\nprogress = []\ndef on_progress(p):\n    progress.append(p)\n    if p.get(\"stage\") == \"embedding\":\n        print(f\"Embedding {p['embedded']}/{p['total']}...\")\n\ningestor = RAGIngestor(\n    embedder=embedder,\n    vector_store=vs,\n    batch_size=64,\n    max_workers=4,\n    progress_cb=on_progress,\n)\n\n# Run build in a background thread\nth = threading.Thread(target=lambda: ingestor.build(), daemon=True)\nth.start()\n\n# Cancel after a moment (simulate user clicking 'Stop')\ntime.sleep(0.2)\ningestor.cancel_event.set()\nth.join()\n</code></pre>"},{"location":"examples/rag-examples/#retrieve-content-two-ways","title":"Retrieve content (two ways)","text":"<p>A) Via <code>RAGIngestor</code> helpers</p> <pre><code>hits = ingestor.search(\"what is the ingestion pipeline?\", top_k=5)\nfor doc, score in hits:\n    print(f\"{score:.3f} | {doc.metadata.get('filename', doc.id)}\")\n    print(doc.text[:120], \"...\n\")\n</code></pre> <p>B) Directly via the vector store</p> <pre><code># Prepare a query embedding\nq = ingestor.embed_query(\"how do we chunk python code?\")\n# Use store's similarity_search with the query vector\nmatches = vs.similarity_search(q, top_k=5)\nfor doc, score in matches:\n    print(doc.id, score)\n</code></pre>"},{"location":"examples/rag-examples/#endtoend-mini-pipeline","title":"\ud83e\uddea End\u2011to\u2011end mini pipeline","text":"<pre><code>from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\nfrom neurosurfer.vectorstores import ChromaVectorStore\nfrom neurosurfer.rag.ingestor import RAGIngestor\n\n# 1) Components\nemb = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\nstore = ChromaVectorStore(collection_name=\"proj-docs\")\ning = RAGIngestor(embedder=emb, vector_store=store, batch_size=48, max_workers=4)\n\n# 2) Intake\ning.add_directory(\"./docs\")\ning.add_files([\"README.md\", \"CHANGELOG.md\"])\ning.add_texts([\"This is a private note about deployment steps.\"], base_id=\"notes\")\n\n# 3) Build\nsummary = ing.build()\nprint(\"Added:\", summary[\"added\"])\n\n# 4) Retrieve\nfor doc, score in ing.search(\"deployment steps\", top_k=3):\n    print(f\"{score:.2f} | {doc.metadata.get('filename', doc.id)}\")\n    print(doc.text[:160], \"...\\n\")\n</code></pre>"},{"location":"examples/rag-examples/#tips","title":"\u2705 Tips","text":"<ul> <li>Prefer batch sizes of 32\u2013128 for sentence\u2011transformers to balance throughput vs memory.</li> <li>Use deduplication (default on) when indexing mixed sources to avoid repeated chunks.</li> <li>Add <code>default_metadata</code> to <code>RAGIngestor(...)</code> to stamp common fields across all docs.</li> <li>Mind your overlaps in chunking \u2014 larger overlaps improve recall at a small cost.</li> <li>For very large repos, start with directory filters and a tighter set of <code>include_exts</code>.</li> </ul>"},{"location":"examples/server-app-example/","title":"Server Application","text":"<p>This is a guided walk\u2011through of a Neurosurfer server app. Instead of one giant snippet, we\u2019ll build it piece by piece and explain how each section fits together. At the end, you\u2019ll find the complete example in one block for easy copy\u2011paste.</p> <p>See also: Lifecycle Hooks \u2022 Chat Handlers \u2022 Configuration \u2022 Auth &amp; Users</p>"},{"location":"examples/server-app-example/#app-initialization","title":"App Initialization","text":"<p>Create the app from configuration so ports, origins, logging, and worker counts stay environment\u2011driven\u2014not hard\u2011coded.</p> <pre><code>import os, logging\nfrom neurosurfer.server import NeurosurferApp\nfrom neurosurfer.config import config\n\nlogging.basicConfig(level=config.app.logs_level.upper())\n\nnm = NeurosurferApp(\n    app_name=config.app.app_name,\n    api_keys=[],  # add static API keys here if you need header-based auth\n    enable_docs=config.app.enable_docs,\n    cors_origins=config.app.cors_origins,\n    host=config.app.host_ip,\n    port=config.app.host_port,\n    reload=config.app.reload,\n    log_level=config.app.logs_level,\n    workers=config.app.workers\n)\n</code></pre> <p>Tip</p> <p>Configure CORS in Configuration if your frontend runs on a different origin. That\u2019s often the cause of \u201cworks with curl, fails in browser.\u201d</p>"},{"location":"examples/server-app-example/#global-state","title":"Global State","text":"<p>Keep shared components explicit\u2014model, embedder, logger, RAG, and a temp directory for file ops.</p> <pre><code>from neurosurfer.models.embedders.base import BaseEmbedder\nfrom neurosurfer.models.chat_models import BaseModel as BaseChatModel\nfrom neurosurfer.server.services.rag_orchestrator import RAGOrchestrator\n\nBASE_DIR = \"./tmp/code_sessions\"; os.makedirs(BASE_DIR, exist_ok=True)\nLLM: BaseChatModel = None\nEMBEDDER_MODEL: BaseEmbedder = None\nLOGGER: logging.Logger = None\nRAG: RAGOrchestrator | None = None\n</code></pre> <p>Note</p> <p>Neurosurfer keeps the surface minimal: the handler orchestrates; heavy lifting lives in services (models, RAG, DB).</p>"},{"location":"examples/server-app-example/#startup-hook","title":"Startup Hook","text":"<p>Load the chat model and embedder, boot the RAG orchestrator, register the model for <code>/v1/models</code>, and warm up for low first\u2011token latency.</p> <pre><code>from neurosurfer.rag.chunker import Chunker\nfrom neurosurfer.rag.filereader import FileReader\n\n@nm.on_startup\nasync def load_model():\n    global EMBEDDER_MODEL, LOGGER, LLM, RAG\n    LOGGER = logging.getLogger(\"neurosurfer\")\n\n    try:\n        import torch\n        LOGGER.info(f\"GPU Available: {torch.cuda.is_available()}\")\n        if torch.cuda.is_available():\n            LOGGER.info(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    except Exception:\n        LOGGER.warning(\"Torch not found...\")\n\n    from neurosurfer.models.chat_models.transformers import TransformersModel\n    from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n\n    LLM = TransformersModel(\n        model_name=\"/path/to/weights/Qwen3-4B-unsloth-bnb-4bit\",\n        max_seq_length=8192,\n        load_in_4bit=True,\n        enable_thinking=False,\n        stop_words=[],\n        logger=LOGGER,\n    )\n\n    nm.model_registry.add(\n        llm=LLM,\n        family=\"qwen3\",\n        provider=\"Qwen\",\n        description=\"Local Qwen3 served by Transformers backend\"\n    )\n\n    EMBEDDER_MODEL = SentenceTransformerEmbedder(\n        model_name=\"/path/to/weights/e5-large-v2\",\n        logger=LOGGER\n    )\n\n    RAG = RAGOrchestrator(\n        embedder=EMBEDDER_MODEL,\n        chunker=Chunker(),\n        file_reader=FileReader(),\n        persist_dir=config.app.database_path,\n        max_context_tokens=2000,\n        top_k=15,\n        min_top_sim_default=0.35,\n        min_top_sim_when_explicit=0.15,\n        min_sim_to_keep=0.20,\n        logger=LOGGER,\n    )\n\n    # Warmup\n    ready = LLM.ask(user_prompt=\"Say hi!\", system_prompt=config.model.system_prompt, stream=False)\n    LOGGER.info(f\"LLM ready: {ready.choices[0].message.content}\")\n</code></pre> <p>Tip</p> <p>Registering the model exposes it via <code>GET /v1/models</code>\u2014handy for UIs and clients to pick a model by id.</p>"},{"location":"examples/server-app-example/#chat-handler","title":"Chat Handler","text":"<p>The heart of the app. It reads the request, compacts history, optionally applies RAG per thread, and calls <code>LLM.ask(...)</code>. The same call supports streaming and non\u2011streaming.</p> <pre><code>from typing import List, Generator\nfrom neurosurfer.server.schemas import ChatCompletionRequest, ChatCompletionResponse, ChatCompletionChunk\nfrom neurosurfer.server.runtime import RequestContext\n\n@nm.chat()\ndef handler(request: ChatCompletionRequest, ctx: RequestContext) -&gt; ChatCompletionResponse | Generator[ChatCompletionChunk, None, None]:\n    global LLM, RAG\n\n    actor_id = (getattr(ctx, \"meta\", {}) or {}).get(\"actor_id\", 0)\n    thread_id = request.thread_id\n\n    user_msgs: List[str] = [m[\"content\"] for m in request.messages if m[\"role\"] == \"user\"]\n    system_msgs = [m[\"content\"] for m in request.messages if m[\"role\"] == \"system\"]\n    system_prompt = system_msgs[0] if system_msgs else config.model.system_prompt\n    user_query = user_msgs[-1] if user_msgs else \"\"\n    chat_history = request.messages[-10:-1]\n\n    temperature = request.temperature if (request.temperature and 2 &gt; request.temperature &gt; 0) else config.model.temperature\n    max_tokens = request.max_tokens if (request.max_tokens and request.max_tokens &gt; 512) else config.model.max_new_tokens\n\n    if RAG and thread_id is not None:\n        rag_res = RAG.apply(\n            actor_id=actor_id,\n            thread_id=thread_id,\n            user_query=user_query,\n            files=[f.model_dump() for f in (request.files or [])],\n        )\n        user_query = rag_res.augmented_query\n        if rag_res.used:\n            LOGGER.info(f\"[RAG] used context (top_sim={rag_res.meta.get('top_similarity', 0):.3f})\")\n\n    return LLM.ask(\n        user_prompt=user_query,\n        system_prompt=system_prompt,\n        chat_history=chat_history,\n        stream=request.stream,\n        temperature=temperature,\n        max_new_tokens=max_tokens,\n    )\n</code></pre> <p>Note</p> <p>The handler returns either a single completion or a generator of SSE chunks, depending on <code>request.stream</code>. The router already formats tokens as OpenAI\u2011style delta chunks.</p>"},{"location":"examples/server-app-example/#shutdown-cleanup","title":"Shutdown Cleanup","text":"<p>Remove transient state so subsequent restarts are clean.</p> <pre><code>import shutil\n\n@nm.on_shutdown\ndef cleanup():\n    shutil.rmtree(BASE_DIR, ignore_errors=True)\n</code></pre>"},{"location":"examples/server-app-example/#cooperative-stop","title":"Cooperative Stop","text":"<p>Expose a stop hook so UIs can cancel long generations gracefully.</p> <pre><code>@nm.stop_generation()\ndef stop_handler():\n    global LLM\n    LLM.stop_generation()\n</code></pre>"},{"location":"examples/server-app-example/#full-example-all-together","title":"Full Example (All Together)","text":"<pre><code>from typing import List, Generator\nimport os, shutil, logging\n\nfrom neurosurfer.models.embedders.base import BaseEmbedder\nfrom neurosurfer.models.chat_models import BaseModel as BaseChatModel\nfrom neurosurfer.rag.chunker import Chunker\nfrom neurosurfer.rag.filereader import FileReader\n\nfrom neurosurfer.server import NeurosurferApp\nfrom neurosurfer.server.schemas import ChatCompletionRequest, ChatCompletionResponse, ChatCompletionChunk\nfrom neurosurfer.server.runtime import RequestContext\nfrom neurosurfer.server.services.rag_orchestrator import RAGOrchestrator\n\nfrom neurosurfer.config import config\nlogging.basicConfig(level=config.app.logs_level.upper())\n\nnm = NeurosurferApp(\n    app_name=config.app.app_name,\n    api_keys=[],\n    enable_docs=config.app.enable_docs,\n    cors_origins=config.app.cors_origins,\n    host=config.app.host_ip,\n    port=config.app.host_port,\n    reload=config.app.reload,\n    log_level=config.app.logs_level,\n    workers=config.app.workers\n)\n\nBASE_DIR = \"./tmp/code_sessions\"; os.makedirs(BASE_DIR, exist_ok=True)\nLLM: BaseChatModel = None\nEMBEDDER_MODEL: BaseEmbedder = None\nLOGGER: logging.Logger = None\nRAG: RAGOrchestrator | None = None\n\n@nm.on_startup\nasync def load_model():\n    global EMBEDDER_MODEL, LOGGER, LLM, RAG\n    LOGGER = logging.getLogger(\"neurosurfer\")\n\n    try:\n        import torch\n        LOGGER.info(f\"GPU Available: {torch.cuda.is_available()}\")\n        if torch.cuda.is_available():\n            LOGGER.info(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    except Exception:\n        LOGGER.warning(\"Torch not found...\")\n\n    from neurosurfer.models.chat_models.transformers import TransformersModel\n    from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n\n    LLM = TransformersModel(\n        model_name=\"/path/to/weights/Qwen3-4B-unsloth-bnb-4bit\",\n        max_seq_length=8192,\n        load_in_4bit=True,\n        enable_thinking=False,\n        stop_words=[],\n        logger=LOGGER,\n    )\n\n    nm.model_registry.add(\n        llm=LLM,\n        family=\"qwen3\",\n        provider=\"Qwen\",\n        description=\"Local Qwen3 served by Transformers backend\"\n    )\n\n    EMBEDDER_MODEL = SentenceTransformerEmbedder(\n        model_name=\"/path/to/weights/e5-large-v2\",\n        logger=LOGGER\n    )\n\n    RAG = RAGOrchestrator(\n        embedder=EMBEDDER_MODEL,\n        chunker=Chunker(),\n        file_reader=FileReader(),\n        persist_dir=config.app.database_path,\n        max_context_tokens=2000,\n        top_k=15,\n        min_top_sim_default=0.35,\n        min_top_sim_when_explicit=0.15,\n        min_sim_to_keep=0.20,\n        logger=LOGGER,\n    )\n\n    ready = LLM.ask(user_prompt=\"Say hi!\", system_prompt=config.model.system_prompt, stream=False)\n    LOGGER.info(f\"LLM ready: {ready.choices[0].message.content}\")\n\n@nm.on_shutdown\ndef cleanup():\n    shutil.rmtree(BASE_DIR, ignore_errors=True)\n\n@nm.chat()\ndef handler(request: ChatCompletionRequest, ctx: RequestContext) -&gt; ChatCompletionResponse | Generator[ChatCompletionChunk, None, None]:\n    global LLM, RAG\n\n    actor_id = (getattr(ctx, \"meta\", {}) or {}).get(\"actor_id\", 0)\n    thread_id = request.thread_id\n\n    user_msgs: List[str] = [m[\"content\"] for m in request.messages if m[\"role\"] == \"user\"]\n    system_msgs = [m[\"content\"] for m in request.messages if m[\"role\"] == \"system\"]\n    system_prompt = system_msgs[0] if system_msgs else config.model.system_prompt\n    user_query = user_msgs[-1] if user_msgs else \"\"\n    chat_history = request.messages[-10:-1]\n\n    temperature = request.temperature if (request.temperature and 2 &gt; request.temperature &gt; 0) else config.model.temperature\n    max_tokens = request.max_tokens if (request.max_tokens and request.max_tokens &gt; 512) else config.model.max_new_tokens\n\n    if RAG and thread_id is not None:\n        rag_res = RAG.apply(\n            actor_id=actor_id,\n            thread_id=thread_id,\n            user_query=user_query,\n            files=[f.model_dump() for f in (request.files or [])],\n        )\n        user_query = rag_res.augmented_query\n\n    return LLM.ask(\n        user_prompt=user_query,\n        system_prompt=system_prompt,\n        chat_history=chat_history,\n        stream=request.stream,\n        temperature=temperature,\n        max_new_tokens=max_tokens,\n    )\n\n@nm.stop_generation()\ndef stop_handler():\n    global LLM\n    LLM.stop_generation()\n\nif __name__ == \"__main__\":\n    nm.run()\n</code></pre>"},{"location":"server/","title":"Server","text":"<p>Neurosurfer\u2019s Server layer brings together a FastAPI backend and a React frontend (NeurosurferUI) that communicate over a clean, OpenAI\u2011style API. You can keep the defaults and ship fast, or swap/extend chat handlers, tools, and routes for a fully custom stack.</p> <p>What you get: OpenAI\u2011compatible endpoints, RAG\u2011ready flows, streaming/tool\u2011calling support, and a React UI designed for iterative agent development.</p>"},{"location":"server/#quick-navigation","title":"\ud83d\ude80 Quick Navigation","text":"<ul> <li> <p> Neurosurfer API (Backend)</p> <p>OpenAI\u2011style <code>/v1/chat/completions</code>, lifecycle hooks, chat handlers, custom endpoints, auth/users.</p> <p> Read the Backend guide</p> </li> <li> <p> NeurosurferUI (Frontend)</p> <p>Chat UX, threads/sessions, file uploads for RAG, and a typed API client. Easy to extend with your own components.</p> <p> Explore the NeurosurferUI</p> </li> <li> <p> Example Application</p> <p>A minimal but complete reference app: startup/shutdown hooks, a custom chat handler, drop\u2011in RAG, and a sample tool/endpoint.</p> <p> Walk through the Example</p> </li> </ul>"},{"location":"server/#usage-overview","title":"Usage Overview","text":"<p>Start the server via the CLI and use Docker/Compose or a reverse proxy in staging/production. The backend is a FastAPI app (exported as <code>NeurosurferApp</code>), and the frontend is a Vite dev server during development (bundled React app in production).</p>"},{"location":"server/#start-everything-dev","title":"Start everything (dev)","text":"<pre><code># Auto-detects UI root (or pass --ui-root). Will run npm install on first run.\nneurosurfer serve\n</code></pre> <ul> <li>Backend binds to <code>NEUROSURF_BACKEND_HOST</code> / <code>NEUROSURF_BACKEND_PORT</code> (from config).  </li> <li>UI root auto\u2011detected; override with <code>--ui-root /path/to/neurowebui</code>.  </li> <li>UI talks to the backend using <code>VITE_BACKEND_URL</code> (injected automatically when binding to <code>0.0.0.0</code> via <code>NEUROSURF_PUBLIC_HOST</code>).</li> </ul> <p>Common variants</p> <pre><code># Backend only (no UI)\nneurosurfer serve --only-backend --backend-host 0.0.0.0 --backend-port 8000\n\n# UI only (point to your UI root)\nneurosurfer serve --only-ui --ui-root ./neurosurferui\n\n# Serve your own app file (must expose a NeurosurferApp instance)\nneurosurfer serve --backend-app ./app.py --backend-reload\n\n# Serve a module with an instance or factory\nneurosurfer serve --backend-app mypkg.myapp:ns\nneurosurfer serve --backend-app mypkg.myapp:create_app()\n</code></pre> <p>Backend app resolution</p> <p><code>--backend-app</code> accepts a module path with <code>:attr</code> or <code>:factory()</code>, or a Python file containing a <code>NeurosurferApp</code> instance. Don\u2019t call <code>app.run()</code> at import time; the CLI orchestrates the process.</p>"},{"location":"server/#deployment","title":"Deployment","text":"<p>You can containerize the backend and UI, or keep the UI as a static build behind a reverse proxy. Below are pragmatic patterns that work well in practice.</p>"},{"location":"server/#docker-compose-backend-focus","title":"Docker &amp; Compose (backend focus)","text":"<p>Dockerfile (backend) <pre><code>FROM python:3.11-slim\n\nWORKDIR /app\nCOPY . /app\n\n# Install core + LLM extras if you\u2019re serving models locally\nRUN pip install -U pip \\\n &amp;&amp; pip install -e . \\\n &amp;&amp; pip install 'neurosurfer[torch]'\n\nENV NEUROSURF_SILENCE=1 \\\n    NEUROSURF_BACKEND_HOST=0.0.0.0 \\\n    NEUROSURF_BACKEND_PORT=8000\n\nEXPOSE 8000\n\n# Run your app module or file; replace with your backend entry\nCMD [\"neurosurfer\", \"serve\", \"--only-backend\", \"--backend-app\", \"neurosurfer.examples.quickstart_app:ns\"]\n</code></pre></p> <p>docker-compose.yml (backend + proxy skeleton) <pre><code>version: \"3.9\"\nservices:\n  api:\n    build: .\n    image: neurosurfer-api:latest\n    environment:\n      NEUROSURF_SILENCE: \"1\"\n      NEUROSURF_BACKEND_HOST: \"0.0.0.0\"\n      NEUROSURF_BACKEND_PORT: \"8000\"\n    ports:\n      - \"8000:8000\"\n    restart: unless-stopped\n\n  # Optional: Caddy / Nginx as reverse proxy serving UI and proxying /v1 to api\n  # proxy:\n  #   image: caddy:2\n  #   volumes:\n  #     - ./Caddyfile:/etc/caddy/Caddyfile:ro\n  #     - ./ui-dist:/srv  # built React app\n  #   ports:\n  #     - \"80:80\"\n  #     - \"443:443\"\n  #   restart: unless-stopped\n</code></pre></p> <p>UI build for production</p> <p>In production, serve the built UI (static files) from your proxy or a CDN and reverse\u2011proxy API calls (e.g., <code>/v1/*</code>) to the backend. During development, the CLI runs the Vite dev server for you.</p>"},{"location":"server/#reverse-proxy-nginxcaddy","title":"Reverse Proxy (Nginx/Caddy)","text":"<p>In a reverse proxy, route <code>/</code> to your built UI and <code>/v1/*</code> to the backend (and <code>/docs</code> if you expose FastAPI docs). Ensure CORS settings in the backend match your deployed origins.</p> <ul> <li>Nginx: <code>location /v1/ { proxy_pass http://api:8000/v1/; }</code></li> <li>Caddy: <code>reverse_proxy /v1/* api:8000</code></li> </ul> <p>TLS and WebSockets</p> <p>If you use streaming or WebSockets, confirm your proxy is configured to forward upgrade headers. Both Nginx and Caddy can do this with their standard reverse_proxy settings.</p>"},{"location":"server/#configuration-environment","title":"Configuration &amp; Environment","text":"<p>Configuration lives with the backend and is also influenced by CLI/env. For a deeper dive, see Backend \u2192 Configuration.</p> <p>Common environment variables</p> <ul> <li><code>NEUROSURF_PUBLIC_HOST</code> \u2014 used to craft <code>VITE_BACKEND_URL</code> when backend binds <code>0.0.0.0</code>/<code>::</code> </li> <li><code>NEUROSURF_UI_ROOT</code> \u2014 path to the UI project for dev mode  </li> <li><code>NEUROSURF_SILENCE=1</code> \u2014 suppress banner/optional\u2011deps warnings on import  </li> <li><code>NEUROSURF_BACKEND_HOST</code> / <code>NEUROSURF_BACKEND_PORT</code> \u2014 default bind for the API  </li> <li><code>NEUROSURF_BACKEND_LOG</code>, <code>NEUROSURF_BACKEND_WORKERS</code>, <code>NEUROSURF_BACKEND_WORKER_TIMEOUT</code> \u2014 backend behavior</li> </ul> <p>Model/runtime dependencies</p> <p>The server core is light. Install the full LLM stack when you need local inference/finetuning: <pre><code>pip install -U 'neurosurfer[torch]'\n</code></pre> For CUDA wheels (Linux x86_64): <pre><code>pip install -U torch --index-url https://download.pytorch.org/whl/cu124\n</code></pre> or CPU\u2011only: <pre><code>pip install -U torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre></p>"},{"location":"server/#worked-example","title":"Worked Example","text":"<p>The Example Application shows: startup hooks (model load, RAG wiring), a custom chat handler, file uploads driving RAG, and a typed server that streams completions.</p> <ul> <li>Read it here \u2192 Example Application</li> <li>Looking for custom handlers? \u2192 Chat Handlers</li> <li>Want to wire tools or extra routes? \u2192 Custom Endpoints</li> </ul>"},{"location":"server/example-app/","title":"Example Application","text":"<p>This is a guided walk\u2011through of a Neurosurfer app. Instead of one giant snippet, we\u2019ll build it piece by piece and explain how each section fits together. At the end, you\u2019ll find the complete example in one block for easy copy\u2011paste.</p> <p>See also: Lifecycle Hooks \u2022 Chat Handlers \u2022 Configuration \u2022 Auth &amp; Users</p>"},{"location":"server/example-app/#1-app-initialization","title":"1) App Initialization","text":"<p>Create the app from configuration so ports, origins, logging, and worker counts stay environment\u2011driven\u2014not hard\u2011coded.</p> <pre><code>import os, logging\nfrom neurosurfer.server import NeurosurferApp\nfrom neurosurfer.config import config\n\nlogging.basicConfig(level=config.app.logs_level.upper())\n\nnm = NeurosurferApp(\n    app_name=config.app.app_name,\n    api_keys=[],  # add static API keys here if you need header-based auth\n    enable_docs=config.app.enable_docs,\n    cors_origins=config.app.cors_origins,\n    host=config.app.host_ip,\n    port=config.app.host_port,\n    reload=config.app.reload,\n    log_level=config.app.logs_level,\n    workers=config.app.workers\n)\n</code></pre> <p>Tip</p> <p>Configure CORS in Configuration if your frontend runs on a different origin. That\u2019s often the cause of \u201cworks with curl, fails in browser.\u201d</p>"},{"location":"server/example-app/#2-global-state","title":"2) Global State","text":"<p>Keep shared components explicit\u2014model, embedder, logger, RAG, and a temp directory for file ops.</p> <pre><code>from neurosurfer.models.embedders.base import BaseEmbedder\nfrom neurosurfer.models.chat_models import BaseModel as BaseChatModel\nfrom neurosurfer.server.services.rag_orchestrator import RAGOrchestrator\n\nBASE_DIR = \"./tmp/code_sessions\"; os.makedirs(BASE_DIR, exist_ok=True)\nLLM: BaseChatModel = None\nEMBEDDER_MODEL: BaseEmbedder = None\nLOGGER: logging.Logger = None\nRAG: RAGOrchestrator | None = None\n</code></pre> <p>Note</p> <p>Neurosurfer keeps the surface minimal: the handler orchestrates; heavy lifting lives in services (models, RAG, DB).</p>"},{"location":"server/example-app/#3-startup-hook","title":"3) Startup Hook","text":"<p>Load the chat model and embedder, boot the RAG orchestrator, register the model for <code>/v1/models</code>, and warm up for low first\u2011token latency.</p> <pre><code>from neurosurfer.rag.chunker import Chunker\nfrom neurosurfer.rag.filereader import FileReader\n\n@nm.on_startup\nasync def load_model():\n    global EMBEDDER_MODEL, LOGGER, LLM, RAG\n    LOGGER = logging.getLogger(\"neurosurfer\")\n\n    try:\n        import torch\n        LOGGER.info(f\"GPU Available: {torch.cuda.is_available()}\")\n        if torch.cuda.is_available():\n            LOGGER.info(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    except Exception:\n        LOGGER.warning(\"Torch not found...\")\n\n    from neurosurfer.models.chat_models.transformers import TransformersModel\n    from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n\n    LLM = TransformersModel(\n        model_name=\"/path/to/weights/Qwen3-4B-unsloth-bnb-4bit\",\n        max_seq_length=8192,\n        load_in_4bit=True,\n        enable_thinking=False,\n        stop_words=[],\n        logger=LOGGER,\n    )\n\n    nm.model_registry.add(\n        llm=LLM,\n        family=\"qwen3\",\n        provider=\"Qwen\",\n        description=\"Local Qwen3 served by Transformers backend\"\n    )\n\n    EMBEDDER_MODEL = SentenceTransformerEmbedder(\n        model_name=\"/path/to/weights/e5-large-v2\",\n        logger=LOGGER\n    )\n\n    RAG = RAGOrchestrator(\n        embedder=EMBEDDER_MODEL,\n        chunker=Chunker(),\n        file_reader=FileReader(),\n        persist_dir=config.app.database_path,\n        max_context_tokens=2000,\n        top_k=15,\n        min_top_sim_default=0.35,\n        min_top_sim_when_explicit=0.15,\n        min_sim_to_keep=0.20,\n        logger=LOGGER,\n    )\n\n    # Warmup\n    ready = LLM.ask(user_prompt=\"Say hi!\", system_prompt=config.model.system_prompt, stream=False)\n    LOGGER.info(f\"LLM ready: {ready.choices[0].message.content}\")\n</code></pre> <p>Tip</p> <p>Registering the model exposes it via <code>GET /v1/models</code>\u2014handy for UIs and clients to pick a model by id.</p>"},{"location":"server/example-app/#4-chat-handler","title":"4) Chat Handler","text":"<p>The heart of the app. It reads the request, compacts history, optionally applies RAG per thread, and calls <code>LLM.ask(...)</code>. The same call supports streaming and non\u2011streaming.</p> <pre><code>from typing import List, Generator\nfrom neurosurfer.server.schemas import ChatCompletionRequest, ChatCompletionResponse, ChatCompletionChunk\nfrom neurosurfer.server.runtime import RequestContext\n\n@nm.chat()\ndef handler(request: ChatCompletionRequest, ctx: RequestContext) -&gt; ChatCompletionResponse | Generator[ChatCompletionChunk, None, None]:\n    global LLM, RAG\n\n    actor_id = (getattr(ctx, \"meta\", {}) or {}).get(\"actor_id\", 0)\n    thread_id = request.thread_id\n\n    user_msgs: List[str] = [m[\"content\"] for m in request.messages if m[\"role\"] == \"user\"]\n    system_msgs = [m[\"content\"] for m in request.messages if m[\"role\"] == \"system\"]\n    system_prompt = system_msgs[0] if system_msgs else config.model.system_prompt\n    user_query = user_msgs[-1] if user_msgs else \"\"\n    chat_history = request.messages[-10:-1]\n\n    temperature = request.temperature if (request.temperature and 2 &gt; request.temperature &gt; 0) else config.model.temperature\n    max_tokens = request.max_tokens if (request.max_tokens and request.max_tokens &gt; 512) else config.model.max_new_tokens\n\n    if RAG and thread_id is not None:\n        rag_res = RAG.apply(\n            actor_id=actor_id,\n            thread_id=thread_id,\n            user_query=user_query,\n            files=[f.model_dump() for f in (request.files or [])],\n        )\n        user_query = rag_res.augmented_query\n        if rag_res.used:\n            LOGGER.info(f\"[RAG] used context (top_sim={rag_res.meta.get('top_similarity', 0):.3f})\")\n\n    return LLM.ask(\n        user_prompt=user_query,\n        system_prompt=system_prompt,\n        chat_history=chat_history,\n        stream=request.stream,\n        temperature=temperature,\n        max_new_tokens=max_tokens,\n    )\n</code></pre> <p>Note</p> <p>The handler returns either a single completion or a generator of SSE chunks, depending on <code>request.stream</code>. The router already formats tokens as OpenAI\u2011style delta chunks.</p>"},{"location":"server/example-app/#5-shutdown-cleanup","title":"5) Shutdown Cleanup","text":"<p>Remove transient state so subsequent restarts are clean.</p> <pre><code>import shutil\n\n@nm.on_shutdown\ndef cleanup():\n    shutil.rmtree(BASE_DIR, ignore_errors=True)\n</code></pre>"},{"location":"server/example-app/#6-cooperative-stop","title":"6) Cooperative Stop","text":"<p>Expose a stop hook so UIs can cancel long generations gracefully.</p>"},{"location":"server/example-app/#nmstop_generation-def-stop_handler-global-llm-llmstop_generation","title":"<pre><code>@nm.stop_generation()\ndef stop_handler():\n    global LLM\n    LLM.stop_generation()\n</code></pre>","text":""},{"location":"server/example-app/#full-example-all-together","title":"Full Example (All Together)","text":"<pre><code>from typing import List, Generator\nimport os, shutil, logging\n\nfrom neurosurfer.models.embedders.base import BaseEmbedder\nfrom neurosurfer.models.chat_models import BaseModel as BaseChatModel\nfrom neurosurfer.rag.chunker import Chunker\nfrom neurosurfer.rag.filereader import FileReader\n\nfrom neurosurfer.server import NeurosurferApp\nfrom neurosurfer.server.schemas import ChatCompletionRequest, ChatCompletionResponse, ChatCompletionChunk\nfrom neurosurfer.server.runtime import RequestContext\nfrom neurosurfer.server.services.rag_orchestrator import RAGOrchestrator\n\nfrom neurosurfer.config import config\nlogging.basicConfig(level=config.app.logs_level.upper())\n\nnm = NeurosurferApp(\n    app_name=config.app.app_name,\n    api_keys=[],\n    enable_docs=config.app.enable_docs,\n    cors_origins=config.app.cors_origins,\n    host=config.app.host_ip,\n    port=config.app.host_port,\n    reload=config.app.reload,\n    log_level=config.app.logs_level,\n    workers=config.app.workers\n)\n\nBASE_DIR = \"./tmp/code_sessions\"; os.makedirs(BASE_DIR, exist_ok=True)\nLLM: BaseChatModel = None\nEMBEDDER_MODEL: BaseEmbedder = None\nLOGGER: logging.Logger = None\nRAG: RAGOrchestrator | None = None\n\n@nm.on_startup\nasync def load_model():\n    global EMBEDDER_MODEL, LOGGER, LLM, RAG\n    LOGGER = logging.getLogger(\"neurosurfer\")\n\n    try:\n        import torch\n        LOGGER.info(f\"GPU Available: {torch.cuda.is_available()}\")\n        if torch.cuda.is_available():\n            LOGGER.info(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    except Exception:\n        LOGGER.warning(\"Torch not found...\")\n\n    from neurosurfer.models.chat_models.transformers import TransformersModel\n    from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n\n    LLM = TransformersModel(\n        model_name=\"/path/to/weights/Qwen3-4B-unsloth-bnb-4bit\",\n        max_seq_length=8192,\n        load_in_4bit=True,\n        enable_thinking=False,\n        stop_words=[],\n        logger=LOGGER,\n    )\n\n    nm.model_registry.add(\n        llm=LLM,\n        family=\"qwen3\",\n        provider=\"Qwen\",\n        description=\"Local Qwen3 served by Transformers backend\"\n    )\n\n    EMBEDDER_MODEL = SentenceTransformerEmbedder(\n        model_name=\"/path/to/weights/e5-large-v2\",\n        logger=LOGGER\n    )\n\n    RAG = RAGOrchestrator(\n        embedder=EMBEDDER_MODEL,\n        chunker=Chunker(),\n        file_reader=FileReader(),\n        persist_dir=config.app.database_path,\n        max_context_tokens=2000,\n        top_k=15,\n        min_top_sim_default=0.35,\n        min_top_sim_when_explicit=0.15,\n        min_sim_to_keep=0.20,\n        logger=LOGGER,\n    )\n\n    ready = LLM.ask(user_prompt=\"Say hi!\", system_prompt=config.model.system_prompt, stream=False)\n    LOGGER.info(f\"LLM ready: {ready.choices[0].message.content}\")\n\n@nm.on_shutdown\ndef cleanup():\n    shutil.rmtree(BASE_DIR, ignore_errors=True)\n\n@nm.chat()\ndef handler(request: ChatCompletionRequest, ctx: RequestContext) -&gt; ChatCompletionResponse | Generator[ChatCompletionChunk, None, None]:\n    global LLM, RAG\n\n    actor_id = (getattr(ctx, \"meta\", {}) or {}).get(\"actor_id\", 0)\n    thread_id = request.thread_id\n\n    user_msgs: List[str] = [m[\"content\"] for m in request.messages if m[\"role\"] == \"user\"]\n    system_msgs = [m[\"content\"] for m in request.messages if m[\"role\"] == \"system\"]\n    system_prompt = system_msgs[0] if system_msgs else config.model.system_prompt\n    user_query = user_msgs[-1] if user_msgs else \"\"\n    chat_history = request.messages[-10:-1]\n\n    temperature = request.temperature if (request.temperature and 2 &gt; request.temperature &gt; 0) else config.model.temperature\n    max_tokens = request.max_tokens if (request.max_tokens and request.max_tokens &gt; 512) else config.model.max_new_tokens\n\n    if RAG and thread_id is not None:\n        rag_res = RAG.apply(\n            actor_id=actor_id,\n            thread_id=thread_id,\n            user_query=user_query,\n            files=[f.model_dump() for f in (request.files or [])],\n        )\n        user_query = rag_res.augmented_query\n\n    return LLM.ask(\n        user_prompt=user_query,\n        system_prompt=system_prompt,\n        chat_history=chat_history,\n        stream=request.stream,\n        temperature=temperature,\n        max_new_tokens=max_tokens,\n    )\n\n@nm.stop_generation()\ndef stop_handler():\n    global LLM\n    LLM.stop_generation()\n\nif __name__ == \"__main__\":\n    nm.run()\n</code></pre>"},{"location":"server/neurosurferui/","title":"NeurosurferUI","text":"<p>NeurosurferUI is a streamlined React frontend purpose-built for the Neurosurfer FastAPI backend. It speaks the backend\u2019s chat and thread APIs, streams tokens over SSE, and manages predictable client state for conversations, uploads, and follow-ups. It favors OpenAPI-described endpoints and standard auth so you can extend capabilities without custom protocols or UI rewrites.</p> <ul> <li>\u26a1 Typed client: thin, type-safe wrappers for auth, models, chats, messages, and streaming</li> <li>\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 Per-user threads: isolate histories, titles, and actions per account</li> <li>\ud83d\udcac Chat UX: durable threads, regenerate last answer, smooth token streaming, quick follow-ups</li> <li>\ud83e\uddfe PDF export: one-click export of any conversation with title and timestamps</li> <li>\ud83d\udcce RAG uploads: per-message file attach, progress chips, optional ingest for retrieval</li> <li>\ud83d\udd10 Session-safe: HttpOnly cookie auth, fast bootstrap from the current user endpoint, clean logout</li> <li>\ud83e\udde9 Extensible: plug custom renderers, tool panels, model pickers; feature-detect and adapt</li> </ul>"},{"location":"server/neurosurferui/#ui-tour-coming-soon","title":"UI Tour (Coming Soon)","text":"<ul> <li>Place a short demo GIF at <code>docs/assets/ui-tour.gif</code> showing login \u2192 new chat \u2192 streaming reply \u2192 follow-ups \u2192 PDF export.  </li> <li>Embed it via <code>![UI Tour](assets/ui-tour.gif)</code> so MkDocs copies it during build.  </li> <li>Capture path: open sidebar, create/select \u201cNew Chat,\u201d send a prompt, watch streaming, click a follow-up, export from the chat menu.</li> </ul> <p>Tip</p> <p>Keep the asset small (\u2264 2\u20133 MB) for fast loads; prefer short clips over full sessions.</p>"},{"location":"server/neurosurferui/#how-the-ui-talks-to-the-backend","title":"How the UI talks to the backend","text":"<p>The UI uses <code>import.meta.env.VITE_BACKEND_URL</code> at build/dev time. If not provided, it falls back to:</p> <pre><code>${window.location.protocol}//${window.location.hostname}:8081\n</code></pre> <p>So by default the UI expects the API at port 8081. Override with:</p> <pre><code># dev\nVITE_BACKEND_URL=http://127.0.0.1:9000 npm run dev\n\n# production build\nVITE_BACKEND_URL=http://api.example.com:8081 npm run build\n</code></pre> <p>TypeScript typings: add <code>src/vite-env.d.ts</code></p> <pre><code>/// &lt;reference types=\"vite/client\" /&gt;\ninterface ImportMetaEnv { readonly VITE_BACKEND_URL?: string; }\ninterface ImportMeta { readonly env: ImportMetaEnv; }\n</code></pre> <p>Ensure <code>tsconfig.json</code> includes <code>\"types\": [\"vite/client\"]</code> and includes that file.</p>"},{"location":"server/neurosurferui/#development-vs-production","title":"Development vs Production","text":""},{"location":"server/neurosurferui/#dev-hot-reload-vite","title":"Dev (hot reload, Vite)","text":"<p>Use when iterating on UI code. Calls your running backend.</p> <pre><code>cd neurosurferui\nnpm ci\nVITE_BACKEND_URL=http://127.0.0.1:8081 npm run dev\n# UI at http://localhost:5173\n</code></pre>"},{"location":"server/neurosurferui/#production-prebuilt-static-assets","title":"Production (prebuilt static assets)","text":"<p>Build once, then serve the static output via Neurosurfer CLI (recommended for users of the pip wheel):</p> <pre><code>cd neurosurferui\nnpm ci\nVITE_BACKEND_URL=http://127.0.0.1:8081 npm run build   # outputs ./dist\n</code></pre> <p>Then copy the compiled assets into the Python package (see next section) so they ship inside the wheel.</p>"},{"location":"server/neurosurferui/#shipping-the-ui-inside-the-wheel-no-node-required-at-runtime","title":"Shipping the UI inside the wheel (no Node required at runtime)","text":"<p>We provide a helper script that builds the React app and syncs its compiled assets into the Python package so the wheel contains <code>neurosurfer/ui_build/**</code>.</p>"},{"location":"server/neurosurferui/#1-use-the-helper-script","title":"1) Use the helper script","text":"<p>From the repo root:</p> <pre><code>chmod +x scripts/build_ui.sh\n./scripts/build_ui.sh         # auto-detects vite output (dist) and syncs -&gt; neurosurfer/ui_build\n</code></pre> <p>Options: - <code>--no-install</code> \u2014 skip <code>npm ci</code> - <code>--no-clean</code> \u2014 avoid deleting removed files in the target (when rsync unavailable) - <code>--out-dir=dist|build</code> \u2014 override output detection if needed</p>"},{"location":"server/neurosurferui/#2-include-assets-in-packaging","title":"2) Include assets in packaging","text":"<p>pyproject.toml <pre><code>[tool.setuptools]\ninclude-package-data = true\n\n[tool.setuptools.package-data]\nneurosurfer = [\"py.typed\", \"ui_build/**\"]\n</code></pre></p> <p>MANIFEST.in <pre><code>graft neurosurfer/ui_build\nglobal-exclude *.map __pycache__ *.py[cod]\n</code></pre></p> <p>Rebuild &amp; install:</p> <pre><code>rm -rf dist build *.egg-info\npython -m build\npip install dist/neurosurfer-*.whl\n</code></pre> <p>Now the installed package contains <code>neurosurfer/ui_build/</code> with your React build.</p>"},{"location":"server/neurosurferui/#serving-the-ui-with-the-neurosurfer-cli","title":"Serving the UI with the Neurosurfer CLI","text":"<p>The CLI supports three modes. It selects the best one automatically, but you can control it with flags.</p>"},{"location":"server/neurosurferui/#1-packaged-static-ui-default-if-present","title":"1) Packaged static UI (default if present)","text":"<p>If your wheel contains <code>neurosurfer/ui_build/index.html</code>, the CLI can serve it via a lightweight static server (no FastAPI mounting).</p> <pre><code>neurosurfer serve --ui-port 5173\n# Backend at http://0.0.0.0:8081, UI at http://0.0.0.0:5173\n</code></pre> <p>Under the hood, the CLI runs <code>npx serve -s neurosurfer/ui_build -l tcp://&lt;ui_host&gt;:&lt;ui_port&gt;</code> (falls back to global <code>serve</code> if <code>npx</code> is unavailable).</p> <p>Note</p> <p>By default the UI runs on a separate port (<code>ui_port</code>) from the API (<code>backend_port</code>). Ensure the backend allows CORS from the UI origin when they differ.</p>"},{"location":"server/neurosurferui/#2-dev-mode-from-a-source-folder","title":"2) Dev mode from a source folder","text":"<p>Point <code>--ui-root</code> at your Vite project to run <code>npm run dev</code>:</p> <pre><code>neurosurfer serve --ui-root ./neurosurferui --ui-port 5173\n# CLI injects VITE_BACKEND_URL -&gt; http://&lt;backend_host&gt;:&lt;backend_port&gt; if not set\n</code></pre>"},{"location":"server/neurosurferui/#3-static-directory-from-a-path","title":"3) Static directory from a path","text":"<p>If you have a prebuilt directory (e.g., <code>dist</code> or any folder with <code>index.html</code>), you can serve it directly:</p> <pre><code>neurosurfer serve --ui-root ./neurosurferui/dist --ui-port 5173\n</code></pre> <p>The CLI will detect it\u2019s a build folder and start <code>serve -s</code> on that path.</p>"},{"location":"server/neurosurferui/#cors-when-ui-and-api-run-on-different-ports","title":"CORS (when UI and API run on different ports)","text":"<p>Add this middleware to your FastAPI app factory:</p> <pre><code>from fastapi.middleware.cors import CORSMiddleware\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\n        \"http://localhost:5173\",\n        \"http://127.0.0.1:5173\",\n        # add deployed UI origins here\n    ],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n</code></pre> <p>If you deploy both behind the same domain/port or reverse proxy, you typically don\u2019t need CORS.</p>"},{"location":"server/neurosurferui/#troubleshooting","title":"Troubleshooting","text":""},{"location":"server/neurosurferui/#address-already-in-use","title":"\u201caddress already in use\u201d","text":"<p>You tried to bind both API and static UI to the same port. Use different ports: <pre><code>neurosurfer serve --ui-port 5173\n</code></pre></p>"},{"location":"server/neurosurferui/#ui-cant-reach-backend","title":"UI can\u2019t reach backend","text":"<ul> <li>Verify <code>VITE_BACKEND_URL</code> points to your API (scheme, host, port).</li> <li>If using different ports, enable CORS on the API (see above).</li> </ul>"},{"location":"server/neurosurferui/#typescript-error-importmetaenv-not-found","title":"TypeScript error: <code>import.meta.env</code> not found","text":"<p>Add <code>src/vite-env.d.ts</code> and <code>\"types\": [\"vite/client\"]</code> in <code>tsconfig.json</code> (see earlier section).</p>"},{"location":"server/neurosurferui/#npm-warnings-about-peer-deps-deprecations","title":"NPM warnings about peer deps / deprecations","text":"<p>They\u2019re typically transitive and not blocking. For Tailwind plugins, match the plugin\u2019s version with your Tailwind major (e.g., <code>tailwind-scrollbar@^3</code> for Tailwind 3).</p>"},{"location":"server/neurosurferui/#recommended-workflow","title":"Recommended workflow","text":"<ol> <li>During UI development <pre><code>cd neurosurferui\nVITE_BACKEND_URL=http://127.0.0.1:8081 npm run dev\n</code></pre></li> <li>Before releasing a wheel <pre><code>./scripts/build_ui.sh   # writes to neurosurfer/ui_build\npython -m build\ntwine upload dist/*\n</code></pre></li> <li>User runs everything <pre><code>pip install neurosurfer\nneurosurfer serve --ui-port 5173\n# open http://localhost:5173\n</code></pre></li> </ol>"},{"location":"server/neurosurferui/#file-structure-reference","title":"File structure (reference)","text":"<pre><code>repo-root/\n\u251c\u2500 neurosurfer/               # Python package (backend)\n\u2502  \u251c\u2500 ui_build/             # (generated) copied from neurosurferui/dist\n\u2502  \u2514\u2500 ...\n\u251c\u2500 neurosurferui/             # React app (Vite)\n\u2502  \u251c\u2500 src/\n\u2502  \u251c\u2500 public/\n\u2502  \u251c\u2500 dist/                 # (generated) vite build output\n\u2502  \u2514\u2500 ...\n\u251c\u2500 scripts/\n\u2502  \u2514\u2500 build_ui.sh           # helper to build &amp; sync UI into neurosurfer/ui_build\n\u2514\u2500 ...\n</code></pre> <p>With this setup, users get a single <code>pip install neurosurfer</code> and <code>neurosurfer serve</code> experience\u2014no Node required at runtime\u2014while you still have a great dev loop with Vite.</p>"},{"location":"server/backend/","title":"Neurosurfer API (Backend)","text":"<p>Neurosurfer\u2019s backend is a FastAPI server that exposes an OpenAI\u2011compatible surface for chat completions and a small set of ergonomic decorators to register your logic. You get streaming, JWT/cookie auth, a model registry, thread\u2011scoped RAG, and typed custom routes\u2014without wiring FastAPI by hand.</p>"},{"location":"server/backend/#what-youll-find-here","title":"What you\u2019ll find here","text":"<ul> <li>A compact request flow for <code>/v1/chat/completions</code> (sync/async, streaming or not)</li> <li>Decorator\u2011driven APIs: <code>@app.chat()</code> and <code>@app.endpoint(...)</code></li> <li>Built\u2011in auth (bcrypt + JWT) that works for browsers (cookies) and API clients (bearer)</li> <li>Integration points for RAG and model registries</li> </ul> <p>For deeper topics, jump directly to the dedicated pages linked below.</p>"},{"location":"server/backend/#components","title":"Components","text":"<ul> <li> <p>Configuration</p> <p>Centralized settings via Pydantic (models, CORS, DB paths, RAG, server flags). Start here to wire your environment.</p> <p> Documentation</p> </li> <li> <p>Lifecycle Hooks</p> <p>Start/stop callbacks to load models, warm resources, initialize RAG, and clean up.</p> <p> Documentation</p> </li> <li> <p>Chat Handlers</p> <p>Your entry point for <code>/v1/chat/completions</code>. Register once, stream or return a final message, and compose RAG/services inside.</p> <p> Documentation</p> </li> <li> <p>Custom Endpoints</p> <p>Add typed REST routes with request/response models and dependencies\u2014perfect for utilities, admin, and service fa\u00e7ades.</p> <p> Documentation</p> </li> <li> <p>Auth &amp; Users</p> <p>bcrypt passwords, JWT tokens (header or HttpOnly cookie), and slim dependencies for required/optional auth.</p> <p> Documentation</p> </li> </ul>"},{"location":"server/backend/#quick-start","title":"Quick start","text":""},{"location":"server/backend/#minimal-handler","title":"Minimal handler","text":"<pre><code>from neurosurfer.server.app import NeurosurferApp\n\napp = NeurosurferApp()\n\n@app.chat()\ndef handle_chat(request, ctx):\n    return f\"You said: {request.messages[-1]['content']}\"\n</code></pre>"},{"location":"server/backend/#with-a-model-registry","title":"With a model registry","text":"<pre><code>app = NeurosurferApp()\n\napp.model_registry.add(\n    id=\"qwen3\",\n    family=\"Qwen\",\n    provider=\"Qwen\",\n    context_length=8192,\n)\n\n@app.chat()\nasync def handle_chat(request, ctx):\n    # stream an answer (pseudo)\n    for token in generate_tokens(request):\n        yield {\"choices\":[{\"delta\":{\"content\": token}}]}\n</code></pre>"},{"location":"server/backend/#with-rag-orchestration","title":"With RAG orchestration","text":"<pre><code>from neurosurfer.server.services.rag_orchestrator import RAGOrchestrator\n\nrag = RAGOrchestrator(embedder=your_embedder, persist_dir=\"./vector_store\", top_k=8)\n\n@app.chat()\ndef handle_chat(request, ctx):\n    user_query = request.messages[-1][\"content\"]\n    # rag.apply(...) may augment the query based on thread uploads\n    # then your LLM answers with the augmented prompt\n    return your_llm.generate(user_query)\n</code></pre>"},{"location":"server/backend/#where-next","title":"Where next?","text":"<ul> <li>Handlers: learn streaming, request shapes, and optional tool prompts \u2192 Chat Handlers </li> <li>Typed routes: build utility/admin APIs \u2192 Custom Endpoints </li> <li>Auth: headers, cookies, dependencies \u2192 Auth &amp; Users </li> <li>Boot sequence: models, RAG, warmups \u2192 Lifecycle Hooks </li> <li>Configure it all: env vars, CORS, ports \u2192 Configuration</li> </ul>"},{"location":"server/backend/auth/","title":"Auth &amp; Users","text":"<p>Neurosurfer ships with a pragmatic auth layer that works for both browser apps and API clients. Passwords are stored with bcrypt; authenticated sessions use JWT access tokens that can live either in the <code>Authorization: Bearer</code> header or in a secure HttpOnly cookie. Sessions automatically extend using a sliding refresh so active users stay signed in without surprise logouts.</p>"},{"location":"server/backend/auth/#capabilities-at-a-glance","title":"Capabilities at a Glance","text":"<ul> <li>Password hashing &amp; verification with bcrypt </li> <li>Short\u2011lived JWT access tokens (<code>HS256</code>), created and verified server\u2011side  </li> <li>Two auth modes: <code>Authorization: Bearer &lt;token&gt;</code> or secure cookie </li> <li>Sliding session refresh that extends tokens on activity  </li> <li>Drop\u2011in dependencies: <code>get_current_user</code> (required) and <code>maybe_current_user</code> (optional)</li> </ul>"},{"location":"server/backend/auth/#token-cookie-model","title":"Token &amp; Cookie Model","text":"<p>A session is a signed JWT containing the user id in <code>sub</code> and standard <code>iat</code> / <code>exp</code> timestamps. You can send the token either:</p> <ul> <li>In the Authorization header: <code>Authorization: Bearer &lt;token&gt;</code> </li> <li>In an HttpOnly cookie (server sets it via <code>set_login_cookie</code>)</li> </ul> <p>For browser apps, the cookie path keeps tokens out of JavaScript by default. Cookies are configured with <code>Secure</code>, <code>HttpOnly</code>, and <code>SameSite</code> using your app config.</p> <p>Sliding refresh. On each authenticated request, if the token is close to expiry (e.g., less than 60 minutes remaining), the server rotates it and resets the cookie, keeping the session alive while the user is active.</p>"},{"location":"server/backend/auth/#core-dependencies","title":"Core Dependencies","text":""},{"location":"server/backend/auth/#get_current_user-required-auth","title":"<code>get_current_user</code> \u2014 required auth","text":"<p>Use this when a route must be authenticated. It retrieves the token (header first, then cookie), validates it, fetches the user from the database, slides the session if needed, and makes the user available at both the return value and <code>request.state.user</code>.</p> <pre><code>from fastapi import Depends\n\n@app.endpoint(\"/me\", method=\"get\", dependencies=[])\ndef me(user = Depends(get_current_user)):\n    return {\"id\": user.id, \"email\": user.email}\n</code></pre> <p>If the token is missing/invalid, the dependency raises <code>401</code> with a compact error message.</p>"},{"location":"server/backend/auth/#maybe_current_user-soft-auth","title":"<code>maybe_current_user</code> \u2014 soft auth","text":"<p>For routes that should work with or without a logged\u2011in user, use the best\u2011effort resolver. It returns a <code>User | None</code> and never raises; invalid or missing tokens are treated as anonymous access.</p> <pre><code>@app.endpoint(\"/whoami\", method=\"get\")\ndef whoami(user = Depends(maybe_current_user)):\n    return {\"user\": getattr(user, \"email\", None)}\n</code></pre>"},{"location":"server/backend/auth/#login-logout-sketch","title":"Login &amp; Logout (Sketch)","text":"<p>A typical login flow verifies credentials, issues a token, and sets the cookie:</p> <pre><code>from fastapi import HTTPException, Response\n\n@app.endpoint(\"/login\", method=\"post\", request=LoginIn, response=LoginOut)\ndef login(data: LoginIn, db = Depends(get_db), response: Response = None):\n    user = db.query(User).filter(User.email == data.email).first()\n    if not user or not verify_password(data.password, user.hashed_password):\n        raise HTTPException(status_code=401, detail=\"Invalid credentials\")\n\n    token = create_access_token({\"sub\": user.id})\n    set_login_cookie(response, token)\n    return LoginOut(ok=True)\n</code></pre> <p>To log out, simply clear the cookie:</p> <pre><code>@app.endpoint(\"/logout\", method=\"post\")\ndef logout(response: Response):\n    clear_login_cookie(response)\n    return {\"ok\": True}\n</code></pre>"},{"location":"server/backend/auth/#using-auth-in-chat-handlers","title":"Using Auth in Chat Handlers","text":"<p>If you want to bind generation to the current user (for example to scope RAG or quota), pull the user from the request state (populated by the dependency), or include <code>maybe_current_user</code> as a dependency on the router that mounts your handler.</p> <pre><code>@app.chat()\ndef handle_chat(request, ctx):\n    user = getattr(ctx.request.state, \"user\", None)\n    actor_id = resolve_actor_id(ctx.request, user)\n    # use actor_id to scope threads, logs, and RAG collections\n    ...\n</code></pre> <p><code>resolve_actor_id</code> picks a stable integer from (in order): the authenticated user, the <code>X-Actor-Id</code> header, <code>?aid=</code> query param, or <code>0</code> (anonymous). This makes it easy to correlate logs and data even for non\u2011authenticated traffic.</p>"},{"location":"server/backend/auth/#curl-examples","title":"cURL Examples","text":""},{"location":"server/backend/auth/#header-bearer-token","title":"Header bearer token","text":"<pre><code>curl -X GET http://localhost:8000/v1/models   -H \"Authorization: Bearer &lt;TOKEN&gt;\"\n</code></pre>"},{"location":"server/backend/auth/#cookie-session-browserstyle","title":"Cookie session (browser\u2011style)","text":"<pre><code># after a login that set the cookie\ncurl -X GET http://localhost:8000/v1/chats   --cookie \"nm_session=&lt;TOKEN&gt;\"\n</code></pre> <p>Replace the cookie name with the one configured in your app. Cookies are <code>HttpOnly</code> and typically set with <code>Secure</code> and <code>SameSite</code> based on environment.</p>"},{"location":"server/backend/auth/#security-notes","title":"Security Notes","text":"<p>Keep sessions short\u2011lived and rotate keys periodically. Ensure <code>Secure</code> cookies in production, enable CORS for your frontend origin, and avoid putting tokens in local storage. The sliding refresh threshold is configurable; pick a value that balances UX and security. For high\u2011risk routes, layer rate limits and device\u2011level checks as dependencies.</p>"},{"location":"server/backend/chat-handlers/","title":"Chat Handlers","text":"<p>The chat handler is the core extension point of the backend. With a single decorator, you implement how your app responds to <code>/v1/chat/completions</code> in both streaming and non\u2011streaming modes. Handlers are intentionally thin and composable: use them to orchestrate models, apply RAG, enforce policy, and shape the final response.</p>"},{"location":"server/backend/chat-handlers/#why-chat-handlers","title":"Why Chat Handlers?","text":"<ul> <li>OpenAI\u2011compatible surface: clients and SDKs integrate without custom glue.</li> <li>One place for logic: prompt strategy, RAG, safety checks, and formatting live together.</li> <li>Streaming\u2011first UX: return tokens as they\u2019re generated (SSE) or a single final message.</li> <li>Minimal ceremony: sync or async function; return string, dict, or a generator.</li> <li>Composability: call services (RAG, toolkits, DB) without leaking internals to clients.</li> </ul>"},{"location":"server/backend/chat-handlers/#endpoint","title":"Endpoint","text":"<ul> <li>URL: <code>/v1/chat/completions</code> </li> <li>Method: <code>POST</code> </li> <li>Auth: API key header or cookie\u2011based session (see Auth)  </li> <li>Streaming: <code>stream=true</code> \u2192 Server\u2011Sent Events (SSE)</li> </ul>"},{"location":"server/backend/chat-handlers/#request-model-essentials","title":"Request Model (Essentials)","text":"Field Type Notes <code>model</code> string <code>required</code> Model identifier from the server model registry. <code>messages</code> array <code>required</code> OpenAI\u2011style chat messages (<code>role</code>, <code>content</code>). <code>stream</code> boolean <code>true</code> enables SSE token streaming. <code>temperature</code> number 0\u20132; defaults from configuration. <code>max_tokens</code> integer Generation cap; defaults from configuration. <code>thread_id</code> string Scopes RAG and uploads to a conversation thread. <code>files</code> array Base64 attachments for RAG ingestion. <code>stop</code> string/array Stop sequences."},{"location":"server/backend/chat-handlers/#message-format","title":"Message Format","text":"<p>Minimal example:</p> <pre><code>{\n  \"messages\": [\n    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n    { \"role\": \"user\", \"content\": \"Summarize the uploaded PDF.\" }\n  ]\n}\n</code></pre> <p>Roles allowed: <code>system</code>, <code>user</code>, <code>assistant</code>, <code>tool</code> (your app may add <code>tool</code> messages as part of a follow\u2011up round).</p>"},{"location":"server/backend/chat-handlers/#response-model","title":"Response Model","text":""},{"location":"server/backend/chat-handlers/#nonstreaming","title":"Non\u2011Streaming","text":"<pre><code>{\n  \"id\": \"cmpl_...\",\n  \"object\": \"chat.completion\",\n  \"created\": 1710000000,\n  \"model\": \"qwen3\",\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": { \"role\": \"assistant\", \"content\": \"Hello!\" },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": { \"prompt_tokens\": 25, \"completion_tokens\": 7, \"total_tokens\": 32 }\n}\n</code></pre>"},{"location":"server/backend/chat-handlers/#streaming-sse","title":"Streaming (SSE)","text":"<ul> <li>Content\u2011Type: <code>text/event-stream</code> </li> <li>Server emits JSON chunks until <code>[DONE]</code>:</li> </ul> <pre><code>event: message\ndata: {\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"content\":\"Hel\"}}]}\n\nevent: message\ndata: {\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"content\":\"lo\"}}]}\n\ndata: [DONE]\n</code></pre> <p>Concatenate <code>choices[].delta.content</code> to reconstruct the final text.</p>"},{"location":"server/backend/chat-handlers/#what-can-i-do-inside-the-handler","title":"What Can I Do Inside the Handler?","text":"<ul> <li>Read the request: messages, <code>stream</code>, sampling params, <code>thread_id</code>, <code>files</code>.</li> <li>Shape prompts: pick a <code>system_prompt</code>, extract the latest user input, trim history.</li> <li>Run RAG: ingest thread files, retrieve relevant chunks, augment the user query.</li> <li>Call your model: via <code>LLM.ask(user_prompt=..., system_prompt=..., stream=...)</code>.</li> <li>Stream: yield OpenAI\u2011compatible delta chunks as tokens arrive.</li> <li>Respect stop: integrate with your model\u2019s cooperative <code>stop_generation</code> handler.</li> <li>Return types: a string, an OpenAI\u2011shaped dict, or a generator (for SSE).</li> <li>Log &amp; meter: attach <code>op_id</code>, <code>thread_id</code>, timing, token counts, RAG usage.</li> </ul> <p>Tip</p> <p>Keep the handler thin. Put heavy lifting (RAG indexing, DB work, tool execution) into services. The handler glues these together and formats the response.</p>"},{"location":"server/backend/chat-handlers/#complete-example-sync-streamingaware","title":"Complete Example (Sync; streaming\u2011aware)","text":"<pre><code>from typing import List\nfrom neurosurfer.server import NeurosurferApp\nfrom neurosurfer.server.schemas import ChatCompletionRequest\nfrom neurosurfer.server.runtime import RequestContext\nfrom neurosurfer.config import config\n\napp = NeurosurferApp()\n\n# Optional: RAG orchestrator/service may be initialized in startup hooks\nRAG = None\nLLM = None\n\n@app.on_startup\ndef boot():\n    global LLM, RAG\n    # Load your model(s), embedder, and RAG orchestrator here.\n    # Add the model to app.model_registry for client discovery.\n    ...\n\n@app.chat()\ndef handle_chat(request: ChatCompletionRequest, ctx: RequestContext):\n    # 1) Extract prompts\n    user_msgs = [m[\"content\"] for m in request.messages if m[\"role\"] == \"user\"]\n    system_msgs = [m[\"content\"] for m in request.messages if m[\"role\"] == \"system\"]\n    user_prompt = user_msgs[-1] if user_msgs else \"\"\n    system_prompt = system_msgs[-1] if system_msgs else config.model.system_prompt\n\n    # 2) Optional: compact recent chat history\n    chat_history = request.messages[-10:-1]\n\n    # 3) Optional: RAG augmentation per thread\n    if RAG and request.thread_id is not None:\n        rag_res = RAG.apply(\n            actor_id=(getattr(ctx, \"meta\", {}) or {}).get(\"actor_id\", 0),\n            thread_id=request.thread_id,\n            user_query=user_prompt,\n            files=[f.model_dump() for f in (request.files or [])]\n        )\n        user_prompt = rag_res.augmented_query  # falls back to original if not used\n\n    # 4) Model call (stream or not)\n    kwargs = {\n        \"temperature\": request.temperature or config.model.temperature,\n        \"max_new_tokens\": request.max_tokens or config.model.max_new_tokens,\n        \"stream\": request.stream,\n    }\n\n    result = LLM.ask(\n        user_prompt=user_prompt,\n        system_prompt=system_prompt,\n        chat_history=chat_history,\n        **kwargs\n    )\n\n    # 5) Return directly for non-streaming\n    if not request.stream:\n        return result\n\n    # 6) For streaming, wrap yielded tokens into OpenAI delta chunks if needed\n    def to_stream(gen):\n        for chunk in gen:\n            # If your model already yields OpenAI-shaped deltas, just yield them.\n            # Otherwise adapt each token to {\"choices\":[{\"delta\":{\"content\": token}}]}\n            yield chunk\n    return to_stream(result)\n</code></pre>"},{"location":"server/backend/chat-handlers/#error-handling","title":"Error Handling","text":"<p>Return compact, user\u2011friendly errors (4xx for client, 5xx for server). In streaming mode, send an error event and close cleanly.</p> <pre><code>{\n  \"error\": { \"code\": \"bad_request\", \"message\": \"Missing 'messages'\" }\n}\n</code></pre>"},{"location":"server/backend/chat-handlers/#observability","title":"Observability","text":"<p>Track: <code>op_id</code>, <code>actor_id</code>, <code>thread_id</code>, <code>model</code>, <code>stream</code>, <code>latency_ms</code>, <code>rag_used</code>, token usage, and error rates. Export metrics for dashboards and alerts.</p>"},{"location":"server/backend/chat-handlers/#optional-minimal-tooling-pattern-promptinjected","title":"Optional: Minimal Tooling Pattern (Prompt\u2011Injected)","text":"<p>Tooling is optional. If you add a toolkit at startup, you may allow clients to pass a tool name and inputs, or you can prompt the model with your tool catalog and ask it to use one tool. So there are two ways to use tools:</p>"},{"location":"server/backend/chat-handlers/#1-explicit-tool-invocation-deterministic","title":"1) Explicit Tool Invocation (Deterministic)","text":"<p>When to use: Buttons, admin flows, or any UX where you want a specific tool to run with known inputs\u2014no model reasoning required.</p> <p>How it works:</p> <ul> <li>Client sends <code>tool</code> and <code>tool_input</code> in the request payload.</li> <li>Handler looks up the tool in <code>toolkit.registry</code>, validates inputs via <code>tool.spec.check_inputs</code>, executes, and returns/streams the result.</li> </ul> <pre><code># Assume 'toolkit' and 'LLM' (and optionally 'EMBEDDER', 'RAG') were initialized in startup.\n@app.chat()\ndef handler(request, ctx):\n    tool_name = getattr(request, \"tool\", None)\n    tool_input = getattr(request, \"tool_input\", {}) or {}\n\n    if tool_name:\n        tool = toolkit.registry.get(tool_name)\n        if not tool:\n            return {\"error\": {\"code\": \"unknown_tool\", \"message\": f\"Tool '{tool_name}' not registered\"}}\n\n        try:\n            data = tool.spec.check_inputs(tool_input)\n        except Exception as e:\n            return {\"error\": {\"code\": \"invalid_tool_input\", \"message\": str(e)}}\n\n        out = tool(**data, llm=LLM, embedder=EMBEDDER, rag=RAG, stream=request.stream)\n        obs = out.observation\n\n        if request.stream and hasattr(obs, \"__iter__\") and not isinstance(obs, (str, bytes)):\n            for token in obs:\n                yield {\"choices\": [{\"delta\": {\"content\": str(token)}}]}\n        else:\n            return {\"choices\": [{\"message\": {\"role\": \"assistant\", \"content\": str(obs)}}]}\n\n    # \u2026otherwise fall back to your standard chat flow (RAG + LLM).\n    ...\n</code></pre>"},{"location":"server/backend/chat-handlers/#2-prompt-driven-selection-model-decides","title":"2) Prompt-Driven Selection (Model Decides)","text":"<p>When to use: Natural language requests where the model should decide whether a tool is needed.</p> <p>Contract: Inject a tool catalog into the system prompt (from toolkit.get_tools_description()), and instruct the model:</p> <ul> <li>If it can answer directly \u2192 output only the final answer.</li> <li>If it needs a tool \u2192 output only strict JSON (no backticks, no prose).</li> </ul> <pre><code>{\"call_tool\": {\"name\": \"&lt;tool_name&gt;\", \"args\": {...}}}\n</code></pre> <p>Minimal handler pattern:</p> <pre><code>import json\n\n@app.chat()\ndef handler(request, ctx):\n    user_last = next((m[\"content\"] for m in reversed(request.messages) if m[\"role\"] == \"user\"), \"\")\n    history = request.messages[-10:-1]\n\n    menu = toolkit.get_tools_description()\n    system_prompt = (\n        \"You are Neurosurfer. Decide if a tool is needed.\\n\\n\"\n        \"TOOLS:\\n\" + menu + \"\\n\\n\"\n        \"INSTRUCTIONS:\\n\"\n        \"- If you can answer directly, output ONLY the final answer text.\\n\"\n        \"- If you need a tool, output ONLY this JSON (no backticks, no extra text):\\n\"\n        '  {\\\"call_tool\\\": {\\\"name\\\": \\\"&lt;tool_name&gt;\\\", \\\"args\\\": { ... }}}\\n'\n        \"- Use exact parameter names/types from the tool spec.\"\n    )\n\n    # First pass (non-streaming) to inspect if tool JSON is returned\n    first = LLM.ask(\n        system_prompt=system_prompt,\n        user_prompt=user_last,\n        chat_history=history,\n        temperature=request.temperature or 0.7,\n        max_new_tokens=request.max_tokens or 1024,\n        stream=False,\n    )\n\n    text = first.choices[0].message.content or \"\"\n    try:\n        call = json.loads(text).get(\"call_tool\")\n    except Exception:\n        call = None\n\n    # No tool requested \u2192 return (or optionally re-ask in streaming mode)\n    if not call:\n        if request.stream:\n            return LLM.ask(\n                system_prompt=system_prompt,\n                user_prompt=user_last,\n                chat_history=history,\n                temperature=request.temperature or 0.7,\n                max_new_tokens=request.max_tokens or 1024,\n                stream=True,\n            )\n        return first\n\n    # Tool requested \u2192 validate and run\n    name, args = call.get(\"name\"), (call.get(\"args\") or {})\n    tool = toolkit.registry.get(name)\n    if not tool:\n        return {\"error\": {\"code\": \"unknown_tool\", \"message\": f\"Tool '{name}' is not registered\"}}\n\n    try:\n        data = tool.spec.check_inputs(args)\n    except Exception as e:\n        return {\"error\": {\"code\": \"invalid_tool_input\", \"message\": str(e)}}\n\n    out = tool(**data, llm=LLM, embedder=EMBEDDER, rag=RAG, stream=request.stream)\n    obs = out.observation\n\n    if request.stream and hasattr(obs, \"__iter__\") and not isinstance(obs, (str, bytes)):\n        for token in obs:\n            yield {\"choices\": [{\"delta\": {\"content\": str(token)}}]}\n    else:\n        return {\"choices\": [{\"message\": {\"role\": \"assistant\", \"content\": str(obs)}}]}\n</code></pre> <p>Notes &amp; Tips</p> <ul> <li>Keep the tool JSON contract strict to avoid ambiguous parsing.</li> <li>Use tool.spec.check_inputs(...) for strong validation (required fields, types, no extras).</li> <li>Cap multi-step tool loops if you extend this to ReAct-style behaviors (e.g., max 3 rounds).</li> </ul>"},{"location":"server/backend/custom-endpoints/","title":"Custom Endpoints","text":"<p>Custom endpoints let you add purpose\u2011built REST routes next to chat completions without wiring FastAPI by hand. With a single decorator you get request parsing, response serialization, dependency execution, and OpenAPI documentation \u2014 all while keeping your business logic as a clean Python function. If you\u2019ve used FastAPI before, this will feel familiar; <code>@app.endpoint(...)</code> simply standardizes the patterns we use across the Neurosurfer server.</p>"},{"location":"server/backend/custom-endpoints/#the-decorator","title":"The Decorator","text":"<p>Use <code>@app.endpoint(path, method=\"post\", request=..., response=..., dependencies=...)</code> to register a route. You can supply Pydantic models for the body and for the response, or omit them and return any JSON\u2011serializable value.</p> <pre><code>@app.endpoint(\n  path: str,\n  *,\n  method: \"get\" | \"post\" | \"put\" | \"patch\" | \"delete\" = \"post\",\n  request: BaseModel | None = None,\n  response: BaseModel | None = None,\n  dependencies: list[Callable] | None = None,\n)\ndef handler(...): ...\n</code></pre> <p>Under the hood, the decorator validates the HTTP verb, wraps your function in a small adapter, and attaches it to the main router with the appropriate <code>response_model</code> and <code>dependencies</code>. Handlers may be sync or async.</p>"},{"location":"server/backend/custom-endpoints/#minimal-examples","title":"Minimal Examples","text":""},{"location":"server/backend/custom-endpoints/#health-check-get-no-models","title":"Health check (GET, no models)","text":"<p>A tiny route is just a function \u2014 no request/response models required:</p> <pre><code>@app.endpoint(\"/health\", method=\"get\")\ndef health():\n    return {\"status\": \"ok\"}\n</code></pre>"},{"location":"server/backend/custom-endpoints/#typed-post-with-requestresponse-models","title":"Typed POST with request/response models","text":"<p>When you want strict validation and a documented schema, add Pydantic models. Neurosurfer will parse the body into your request type and serialize the return value to your response type.</p> <pre><code>from pydantic import BaseModel\n\nclass SummarizeRequest(BaseModel):\n    text: str\n    max_length: int = 100\n\nclass SummarizeResponse(BaseModel):\n    summary: str\n    original_length: int\n    summary_length: int\n\n@app.endpoint(\n    \"/summarize\",\n    method=\"post\",\n    request=SummarizeRequest,\n    response=SummarizeResponse\n)\ndef summarize(req: SummarizeRequest):\n    summary = summarize_fn(req.text, req.max_length)\n    return SummarizeResponse(\n        summary=summary,\n        original_length=len(req.text),\n        summary_length=len(summary),\n    )\n</code></pre>"},{"location":"server/backend/custom-endpoints/#dependencies","title":"Dependencies","text":"<p>Dependencies are regular FastAPI callables that run before your handler. They\u2019re ideal for authentication, database sessions, rate limits, or feature flags. Add them with <code>dependencies=[...]</code> and keep the handler focused on the core logic.</p> <pre><code>def require_api_key():\n    # raise HTTPException(401) on failure\n    ...\n\n@app.endpoint(\"/admin/stats\", method=\"get\", dependencies=[require_api_key])\ndef admin_stats():\n    return {\"users\": 120, \"threads\": 980}\n</code></pre>"},{"location":"server/backend/custom-endpoints/#request-response-behavior","title":"Request &amp; Response Behavior","text":"<p>If you pass a <code>request=Model</code>, the incoming JSON body is validated and injected as the first parameter of your function. If you pass a <code>response=Model</code>, your return value is serialized to match that schema. Omit both and you can return plain JSON (dicts, lists, primitives). For custom status codes or headers, return a FastAPI <code>JSONResponse</code> or raise <code>HTTPException</code> \u2014 the decorator doesn\u2019t constrain you.</p> <p>Async is supported: declare <code>async def</code> and freely <code>await</code> network calls or I/O.</p> <pre><code>@app.endpoint(\"/async-example\", method=\"post\", request=SummarizeRequest, response=SummarizeResponse)\nasync def async_summarize(req: SummarizeRequest):\n    result = await summarize_async(req.text, req.max_length)\n    return SummarizeResponse(summary=result, original_length=len(req.text), summary_length=len(result))\n</code></pre>"},{"location":"server/backend/custom-endpoints/#patterns-youll-use-often","title":"Patterns You\u2019ll Use Often","text":"<p>Small \u201cutility\u201d services (embeddings, text cleaning, conversions), lightweight admin panels (stats, cache purge), content operations (upload \u2192 process \u2192 return IDs), and model control hooks (warmup, reload). Keep each endpoint small and pure; move heavy lifting to services so the route stays testable.</p> <p>Tip</p> <p>Prefer Pydantic models when you want clear validation errors and self\u2011documenting APIs in <code>/docs</code>. Use <code>dependencies</code> instead of ad\u2011hoc checks so you can reuse auth and context setup. Name routes descriptively \u2014 they surface in OpenAPI, and good names make your API pleasant to explore. If you\u2019ll call endpoints from a browser app, configure CORS in Configuration.</p>"},{"location":"server/backend/custom-endpoints/#endtoend-example","title":"End\u2011to\u2011End Example","text":"<p>Below is a compact endpoint that extracts keywords with simple auth, strong typing, and a clean response shape.</p> <pre><code>from pydantic import BaseModel\n\nclass KeywordsIn(BaseModel):\n    text: str\n    top_k: int = 5\n\nclass KeywordsOut(BaseModel):\n    keywords: list[str]\n\ndef require_user():\n    # auth check, attach user to context if you maintain one\n    ...\n\n@app.endpoint(\n    \"/keywords\",\n    method=\"post\",\n    request=KeywordsIn,\n    response=KeywordsOut,\n    dependencies=[require_user],\n)\ndef extract_keywords(data: KeywordsIn):\n    kws = extract_keywords_fn(data.text, data.top_k)\n    return KeywordsOut(keywords=kws)\n</code></pre>"},{"location":"server/backend/lifecycle-hooks/","title":"Lifecycle Hooks","text":"<p>Neurosurfer exposes decorator-driven lifecycle hooks so you can prepare and tear down resources cleanly around the server\u2019s life. Hooks are lightweight, composable, and support both sync and async functions.</p> <ul> <li><code>@app.on_startup</code> \u2014 register one or more functions that run once when the app starts.  </li> <li><code>@app.on_shutdown</code> \u2014 register functions that run once on graceful termination (SIGTERM / server stop).  </li> <li><code>@app.stop_generation</code> \u2014 register a cooperative cancellation handler for in-flight generations.</li> </ul> <p>You can attach multiple functions to each hook; they run in registration order. Both sync and async functions are supported.</p>"},{"location":"server/backend/lifecycle-hooks/#minimal-example","title":"Minimal Example","text":"<pre><code>from neurosurfer.server import NeurosurferApp\n\napp = NeurosurferApp()\n\n@app.on_startup\nasync def warmup():\n    # Create loggers, test GPU, load models, hydrate caches\n    ...\n\n@app.on_shutdown\ndef cleanup():\n    # Close pools, delete temp dirs, flush telemetry\n    ...\n\n@app.stop_generation\ndef on_stop():\n    # Cooperatively ask the active model(s) to stop\n    ...\n</code></pre>"},{"location":"server/backend/lifecycle-hooks/#reference-example-end-to-end","title":"Reference Example (End-to-End)","text":"<p>Below mirrors a production setup with model load, embedder + RAG orchestration, a warmup inference, cleanup, and a cooperative stop signal.</p> <pre><code>from typing import List\nimport logging, shutil, os\n\nfrom neurosurfer.server import NeurosurferApp\nfrom neurosurfer.config import config\n\napp = NeurosurferApp(\n    app_name=config.app.app_name,\n    cors_origins=config.app.cors_origins,\n    enable_docs=config.app.enable_docs,\n    host=config.app.host_ip,\n    port=config.app.host_port,\n    reload=config.app.reload,\n    log_level=config.app.logs_level,\n    workers=config.app.workers,\n)\n\nBASE_DIR = \"./tmp/code_sessions\"; os.makedirs(BASE_DIR, exist_ok=True)\n\nLLM = None\nEMBEDDER = None\nLOGGER = logging.getLogger(\"neurosurfer\")\n\n@app.on_startup\nasync def load_everything():\n    # 1) Logging &amp; hardware probe\n    logging.basicConfig(level=config.app.logs_level.upper())\n    try:\n        import torch\n        LOGGER.info(\"GPU available: %s\", torch.cuda.is_available())\n        if torch.cuda.is_available():\n            LOGGER.info(\"GPU name: %s\", torch.cuda.get_device_name(0))\n    except Exception:\n        LOGGER.warning(\"Torch not available; running on CPU\")\n\n    # 2) Models (LLM + embedder)\n    from neurosurfer.models.chat_models.transformers import TransformersModel\n    from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n\n    global LLM, EMBEDDER\n    LLM = TransformersModel(\n        model_name=config.model.model_name,\n        max_seq_length=config.model.max_seq_length,\n        load_in_4bit=getattr(config.model, \"load_in_4bit\", False),\n        enable_thinking=getattr(config.model, \"enable_thinking\", False),\n        stop_words=config.model.stop_words,\n        logger=LOGGER,\n    )\n\n    # 3) Model registry (for UI pickers and client feature gating)\n    app.model_registry.add(\n        llm=LLM,\n        family=\"qwen3\",\n        provider=\"Qwen\",\n        description=\"Local Transformers model\",\n    )\n\n    EMBEDDER = SentenceTransformerEmbedder(\n        model_name=config.model.embedder, logger=LOGGER\n    )\n\n    # 4) Warmup inference (reduces first-token latency)\n    ping = LLM.ask(\n        user_prompt=\"ping\", system_prompt=config.model.system_prompt, stream=False\n    )\n    LOGGER.info(\"LLM warmup OK: %s\", ping.choices[0].message.content)\n\n@app.on_shutdown\ndef tear_down():\n    # Always resilient: do not raise; log and continue.\n    shutil.rmtree(BASE_DIR, ignore_errors=True)\n    LOGGER.info(\"Cleaned temp workspace at %s\", BASE_DIR)\n\n@app.stop_generation\ndef stop_handler():\n    # Called when a user/client requests to stop a running generation\n    if LLM:\n        LLM.stop_generation()\n</code></pre> <p>Tip</p> <p>Startup must be fast. If something heavy can be deferred, do it lazily on the first request (but keep a small warmup for UX).</p>"},{"location":"server/backend/lifecycle-hooks/#multiple-hooks-ordering","title":"Multiple Hooks &amp; Ordering","text":"<p>You can register several startup/shutdown functions. They run in the order you declare them:</p> <pre><code>@app.on_startup\ndef init_logging(): ...\n\n@app.on_startup\nasync def init_models(): ...\n\n@app.on_startup\ndef init_caches(): ...\n</code></pre>"},{"location":"server/backend/lifecycle-hooks/#async-vs-sync","title":"Async vs Sync","text":"<ul> <li>Use async when calling network I/O (object storage, vector DB, model gateway).  </li> <li>Use sync for pure CPU initialization (loading local weights, creating directories).  </li> <li>Avoid long <code>await</code> chains that can stall the server; consider background tasks after the app is accepting traffic.</li> </ul>"},{"location":"server/backend/lifecycle-hooks/#graceful-shutdown-in-flight-streams","title":"Graceful Shutdown &amp; In-Flight Streams","text":"<p>Streaming clients (SSE) deserve graceful termination:</p> <ol> <li>Register a <code>@app.stop_generation</code> handler that calls your model\u2019s cooperative stop.  </li> <li>On shutdown, the server sends stop signals, then waits briefly for streams to complete.  </li> <li>Keep proxy read timeouts high enough so clients get the final event rather than a TCP reset (see Deployment).</li> </ol> <pre><code>@app.stop_generation\ndef stop_all():\n    if LLM:\n        LLM.stop_generation()\n</code></pre>"},{"location":"server/backend/lifecycle-hooks/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Server boots but first request is slow \u2192 add a warmup call in startup.  </li> <li>Hangs on shutdown \u2192 ensure cooperative <code>@stop_generation</code> handler and raise proxy timeouts.  </li> <li>RAG never triggers \u2192 inspect thresholds and log top similarity; adjust implicit/explicit gates.  </li> <li>Out of memory on startup \u2192 lower quantization/precision or delay heavy loads to first request.</li> </ul>"}]}