# ReActAgent

**Module:** `neurosurfer.agents.react_agent`

## Overview

`ReActAgent` implements the full Reasoning + Acting loop used throughout Neurosurfer. It coordinates an LLM with a [Toolkit](../tools/toolkit.md) of callable tools, streaming every thought, action, and observation as it moves toward a final answer.

Key responsibilities:

- Builds a system prompt that enumerates every registered tool
- Guides the LLM to emit `Action: {"tool": "...", "inputs": {...}}` blocks
- Validates tool inputs against their [`ToolSpec`](../tools/tool-spec.md)
- Runs the tool, captures the observation, and appends it to the reasoning history
- Detects the `<__final_answer__> ... </__final_answer__>` segment that marks completion
- Exposes a `stop_generation()` flag to cancel long runs

Because the agent streams directly from the underlying model and tools, you can surface real-time progress to clients or log it for debugging.

## Constructor

### `ReActAgent.__init__`

```python
ReActAgent(
    toolkit: Toolkit,
    llm: BaseModel,
    *,
    logger: logging.Logger = logging.getLogger(),
    verbose: bool = True,
    specific_instructions: str = "",
)
```

| Parameter | Type | Description |
| --- | --- | --- |
| `toolkit` | [`Toolkit`](../tools/toolkit.md) | Registry of tools the agent is allowed to call. |
| `llm` | [`BaseModel`](../models/chat-models/base-model.md) | Chat model used for reasoning and tool-selection output. |
| `logger` | `logging.Logger` | Logger used for status messages (defaults to the root logger). |
| `verbose` | `bool` | When `True`, prints tool usage and observations to stdout. |
| `specific_instructions` | `str` | Extra text appended to the system prompt (for agent-level customization). |

## Quick start

```python
from neurosurfer.agents.react_agent import ReActAgent
from neurosurfer.models.chat_models.openai import OpenAIModel
from neurosurfer.tools import Toolkit
from neurosurfer.tools.common.general_query_assistant import GeneralQueryAssistantTool

llm = OpenAIModel(model_name="gpt-4o-mini")

toolkit = Toolkit()
toolkit.register_tool(GeneralQueryAssistantTool(llm=llm))

agent = ReActAgent(toolkit=toolkit, llm=llm, verbose=True)

for chunk in agent.run("Explain retrieval augmented generation in one paragraph."):
    print(chunk, end="")
```

The generator yields the agent’s internal trace. Look for the `<__final_answer__>` / `</__final_answer__>` markers to extract the final response.

## Methods

### `run(user_query: str, **kwargs) -> Generator[str, None, str]`

Primary entry point. Streams the reasoning loop and ultimately returns the final answer. Pass any model-specific parameters (`temperature`, `max_new_tokens`, etc.) as keyword arguments—they are forwarded to `llm.ask(...)`.

Common pattern to capture the final answer while showing progress:

```python
chunks = []
for chunk in agent.run("What is 15% of 250?", temperature=0.2):
    print(chunk, end="")
    chunks.append(chunk)

transcript = "".join(chunks)
final = transcript.split("<__final_answer__>")[-1].split("</__final_answer__>")[0].strip()
```

### `update_toolkit(toolkit: Toolkit) -> None`

Swap the active toolkit at runtime. Useful when you want to register additional tools after agent construction.

```python
toolkit.register_tool(MyCustomTool())
agent.update_toolkit(toolkit)
```

### `execute_llm_tool_output(tool_call: dict, **context) -> ToolResponse`

Executes a parsed tool call (name + inputs). The agent automatically merges:

- Inputs generated by the LLM (`tool_call["inputs"]`)
- Runtime context you pass to `run(**context)` (e.g. `db_engine`, `embedder`)
- Temporary memory populated by previous tool calls (`ToolResponse.extras`)

Normally you will not invoke this directly, but it is helpful when unit testing new tools.

### `stop_generation() -> None`

Sets an internal flag that causes the agent to exit the reasoning loop after the current iteration. Ideal for long-running streams that you want to cancel from the outside.

## Tool contract

Each registered tool must return a [`ToolResponse`](../tools/base-tool.md):

- `observation`: string or generator yielding strings/chunks
- `final_answer`: `True` when the tool itself produced the final answer
- `extras`: optional dictionary persisted into the next tool call

When `final_answer` is `True`, the agent prepends `“Final Answer: ”`, emits the observation, and closes with `</__final_answer__>`.

## System prompt

`react_system_prompt()` builds a markdown prompt listing every tool, including their inputs and usage hints. The prompt text comes directly from each tool’s `ToolSpec`, so keeping specs accurate is key to good behaviour.

Example excerpt:

```
You can call tools using:
Action: {"tool": "...", "inputs": {...}, "final_answer": false}

### `sql_executor`
Execute read-only SQL queries against the configured database.
**When to use**: After you have generated a valid SQL query.
**Inputs**:
- `query`: string (required) — SQL query to run.
**Returns**: string — Query results formatted as a table.
```

Use `specific_instructions` in the constructor to append additional guardrails or workflow notes.
