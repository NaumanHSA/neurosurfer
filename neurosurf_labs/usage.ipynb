{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b35214-7a1f-403c-9a7a-b42e481ab431",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# Go up one directory from `b/` to project root\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affc4a2e-d532-4dee-8b50-cab72fd229c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                                                                  â•‘\n",
      "â•‘ â–“â–“â–“â–“â–“   â–“â–“â–“â–“                                  â–“â–“â–“                â•‘\n",
      "â•‘  â–“â–“ â–“â–“   â–“â–“  â–“â–“â–“â–“ â–“  â–“ â–“ â–“ â–“â–“â–“â–“ â–“â–“â–“ â–“  â–“ â–“ â–“  â–“   â–“â–“â–“â–“ â–“ â–“       â•‘\n",
      "â•‘  â–“â–“  â–“â–“  â–“â–“  â–“â–â–â–“ â–“  â–“ â–“â–“â– â–“  â–“ â–“â–  â–“  â–“ â–“â–“â– â–“â–“â–“  â–“â–â–â–“ â–“â–“        â•‘\n",
      "â•‘  â–“â–“   â–“â–“ â–“â–“  â–“    â–“  â–“ â–“   â–“  â–“   â–“ â–“  â–“ â–“    â–“   â–“    â–“         â•‘\n",
      "â•‘ â–“â–“â–“â–“   â–“â–“â–“â–“â–“ â–“â–“â–“â–“ â–“â–“â–“â–“ â–“   â–“â–“â–“â–“ â–“â–“â–“ â–“â–“â–“â–“ â–“    â–“   â–“â–“â–“â–“ â–“         â•‘\n",
      "â•‘ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ â•‘\n",
      "â•‘ Orchestrate Agents - RAG - SQL Tools - Multi-LLM - FastAPI Ready â•‘\n",
      "â•‘ Faster builds, clearer flows, production-first                   â•‘\n",
      "â•‘                                                                  â•‘\n",
      "â•‘ Version: 0.1.3 | Python: 3.11.13                                 â•‘\n",
      "â•‘ OS: Linux 6.14.0-33-generic (x86_64)                             â•‘\n",
      "â•‘ Torch: 2.7.1+cu126   CUDA: yes (12.6)                            â•‘\n",
      "â•‘ MPS: no (built: False)                                           â•‘\n",
      "â•‘ Transformers: 4.51.3   SentEmb: 5.1.0                            â•‘\n",
      "â•‘ Accelerate: 1.10.1   bnb: 0.47.0                                 â•‘\n",
      "â•‘ Unsloth: 2025.8.10                                               â•‘\n",
      "â•‘                                                                  â•‘\n",
      "â•‘ Detected CUDA devices: NVIDIA GeForce RTX 3080 Ti                â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomi/anaconda3/envs/LLMs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:07:57\u001b[0m | \u001b[96mconfig.py:<module>\u001b[0m | PyTorch version 2.7.1+cu126 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomi/anaconda3/envs/LLMs/lib/python3.11/importlib/__init__.py:126: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 11-06 11:07:58 [__init__.py:241] Automatically detected platform cuda.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:00\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Initializing Transformers model.\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-06 11:08:00\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Model is already quantized. Ignoring load_in_4bit=True.\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:01\u001b[0m | \u001b[96mmodeling.py:get_balanced_memory\u001b[0m | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:07\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Transformers model initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from neurosurfer.models.chat_models.transformers import TransformersModel\n",
    "from neurosurfer import config \n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "DEFAULT_TRANSFORMERS_MODEL_PARAMS = dict({\n",
    "    \"model_name\": \"/home/nomi/workspace/Model_Weights/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"max_seq_length\": 16_000,\n",
    "    \"load_in_4bit\": True,\n",
    "    \"enable_thinking\": False,  # main_gpu interpretation\n",
    "    \"verbose\": False\n",
    "})\n",
    "\n",
    "LOGGER = logging.getLogger()\n",
    "LLM = TransformersModel(\n",
    "    **DEFAULT_TRANSFORMERS_MODEL_PARAMS,\n",
    "    stop_words=[\"Observation:\"],\n",
    "    logger = logging.getLogger(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16ff5798-3ed2-4b38-86cd-2498ca0e57be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't skeletons fight each other?  \n",
      "Because they don't have the *guts*! ğŸ˜„"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "system_prompt = \"You are a joker.\"\n",
    "user_prompt = \"\"\"Tell me a light-hearted joke.\"\"\"\n",
    "\n",
    "stream_response = LLM.ask(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=user_prompt,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "md_display = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream_response:\n",
    "    chunk = chunk.choices[0].delta.content or \"\"\n",
    "    print(chunk, flush=True, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b1381",
   "metadata": {},
   "source": [
    "## RAG wiring so the agent â€œunderstandsâ€ the Neurosurf codebase\n",
    "\n",
    "Youâ€™ll ingest the repo once, then run a retriever to answer code questions. The Planner can call the retriever first to form a precise implementation plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f5bf38-1a5b-4ad1-be5f-b5073dbeaaf0",
   "metadata": {},
   "source": [
    "### FileReader and Chunker Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d61982-ac42-46b7-8a6b-bc3ed2248907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:09\u001b[0m | \u001b[96mSentenceTransformer.py:__init__\u001b[0m | Use pytorch device_name: cuda:0\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:09\u001b[0m | \u001b[96msentence_transformer.py:__init__\u001b[0m | SentenceTransformer embedding model initialized.\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:09\u001b[0m | \u001b[96mposthog.py:__init__\u001b[0m | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "[Init] ChromaVectorStore initialized with collection: neurosurf-repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.07it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.08it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  9.27it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.57it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.72it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.11it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.36it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.39it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.12it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'ok', 'sources': 99, 'chunks': 608, 'unique_chunks': 608, 'added': 608, 'finished_at': 1762412892.396136}\n"
     ]
    }
   ],
   "source": [
    "# scripts/index_repo_for_rag.py\n",
    "from pathlib import Path\n",
    "from neurosurfer.rag.ingestor import RAGIngestor\n",
    "from neurosurfer.rag.chunker import Chunker\n",
    "from neurosurfer.rag.filereader import FileReader\n",
    "from neurosurfer.vectorstores.chroma import ChromaVectorStore\n",
    "from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n",
    "\n",
    "embedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\n",
    "vs = ChromaVectorStore(collection_name=\"neurosurf-repo\")\n",
    "ing = RAGIngestor(\n",
    "    embedder=embedder,\n",
    "    vector_store=vs, \n",
    "    chunker=Chunker(), \n",
    "    file_reader=FileReader(),\n",
    "    default_metadata={\"collection\": \"neurosurf\"}\n",
    ")\n",
    "\n",
    "root_dir = Path(os.getcwd()).parent.joinpath(\"neurosurfer\")\n",
    "ing.add_directory(root_dir)  # the repo root\n",
    "print(ing.build())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4c8c2",
   "metadata": {},
   "source": [
    "## RAG AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ebac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] ChromaVectorStore initialized with collection: neurosurf-repo\n"
     ]
    }
   ],
   "source": [
    "# scripts/rag_helper.py\n",
    "from neurosurfer.agents.rag import RAGAgent, RAGAgentConfig\n",
    "from neurosurfer.vectorstores.chroma import ChromaVectorStore\n",
    "from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n",
    "from neurosurfer.models.chat_models.openai import OpenAIModel\n",
    "\n",
    "vs = ChromaVectorStore(collection_name=\"neurosurf-repo\")\n",
    "# embedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\n",
    "# llm = OpenAIModel(model_name=\"gpt-4o-mini\")\n",
    "rag_agent = RAGAgent(\n",
    "    llm=LLM, \n",
    "    vectorstore=vs, \n",
    "    embedder=embedder, \n",
    "    config=RAGAgentConfig(top_k=6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0958db2c",
   "metadata": {},
   "source": [
    "Test RagAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4302830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a **table summarizing the key configurations of the `Chunker` class** based on the provided code and documentation:\n",
       "\n",
       "| **Configuration Parameter**         | **Description**                                                                 | **Default Value**         | **Notes**                                                                 |\n",
       "|------------------------------------|---------------------------------------------------------------------------------|---------------------------|---------------------------------------------------------------------------|\n",
       "| `fallback_chunk_size`             | Maximum number of lines for fallback chunking (used when no specific strategy is found). | `30`                      | Applies to line-based chunking if no other strategy is available.       |\n",
       "| `overlap_lines`                   | Number of lines to overlap between consecutive chunks to preserve context.       | `5`                       | Helps maintain continuity between chunks.                               |\n",
       "| `char_chunk_size`                 | Maximum number of characters per chunk for character-based chunking.            | `1000`                    | Used for prose or generic text content.                                  |\n",
       "| `comment_block_threshold`         | Minimum number of comment lines to consider a block as comment-only.            | `4`                       | Used to filter out comment-only blocks from chunks.                      |\n",
       "| `blacklist_patterns`              | List of regex patterns for files to exclude (e.g., `.git`, `node_modules`, etc.). | As defined in code         | Excludes unwanted file types (e.g., binaries, config files, etc.).       |\n",
       "| `line_comment_markers`            | List of comment markers for line-based comment detection (e.g., `#`, `//`).      | `[\"#\", \"//\", \"/*\", \"*/\"]`  | Used to identify and filter comment-only lines.                           |\n",
       "| `strategy_priority`               | Priority order for selecting chunking strategies (e.g., custom, router, etc.).   | Defined in code           | Priority order: 1. Custom handler, 2. Router, 3. File extension mapping, 4. Built-in strategy, 5. Heuristic fallback. |\n",
       "| `chunking_strategy`               | The main chunking strategy to use (e.g., line-based, character-based, AST-based).| Determined dynamically     | Selected based on file type, content, and configuration.                |\n",
       "| `safety_limits`                   | Safety limits for chunking (e.g., maximum chunk size, minimum content length).   | Defined in `ChunkerConfig`| Prevents overly large or small chunks.                                   |\n",
       "| `custom_handlers`                 | Dictionary of custom handlers for specific file extensions or content types.     | Empty by default          | Registered via `register()` method.                                      |\n",
       "| `router_function`                 | A function to dynamically select a chunking strategy based on content analysis.  | Not set by default        | Used for advanced strategy selection.                                    |\n",
       "| `file_extension_mapping`         | Mapping of file extensions to specific chunking strategies.                      | Defined in code           | Used to override default strategies for specific file types.             |\n",
       "\n",
       "---\n",
       "\n",
       "### Summary:\n",
       "This table provides a structured overview of the **key configuration parameters** of the `Chunker` class, including their **purpose**, **default values**, and **notes**. These configurations allow for **flexible and intelligent chunking** of documents in a RAG (Retrieval-Augmented Generation) system."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 29.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:12\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGRetriever] Retrieved 6 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# test retrieve\n",
    "# retrieved_obj = rag_agent.retrieve(\"What is the main difference between transformers and unsloth?\")\n",
    "# print(retrieved_obj.context)\n",
    "\n",
    "# test run\n",
    "llm_response = rag_agent.run(\"Explain Chunker Configurations in table form.\")\n",
    "response = \"\"\n",
    "md_display = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in llm_response:\n",
    "    response += chunk\n",
    "    md_display.update(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481a891",
   "metadata": {},
   "source": [
    "## Tools Router Agent Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3bebb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:31\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: calculator\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:31\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: general_query_assistant\n"
     ]
    }
   ],
   "source": [
    "from neurosurfer.agents.tools_router_agent import ToolsRouterAgent\n",
    "from neurosurfer.tools.toolkit import Toolkit\n",
    "\n",
    "from neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n",
    "from neurosurfer.tools.base_tool import BaseTool, ToolResponse\n",
    "from neurosurfer.tools.common.general_query_assistant import GeneralQueryAssistantTool\n",
    "\n",
    "# Simple Calculator Tool\n",
    "class CalculatorTool(BaseTool):\n",
    "    spec = ToolSpec(\n",
    "        name=\"calculator\",\n",
    "        description=\"Perform basic arithmetic operations such as addition, subtraction, multiplication, and division.\",\n",
    "        when_to_use=\"Use this tool when you need to perform basic arithmetic operations.\",\n",
    "        inputs=[\n",
    "            ToolParam(name=\"num1\", type=\"float\", description=\"The first number.\", required=True),\n",
    "            ToolParam(name=\"num2\", type=\"float\", description=\"The second number.\", required=True),\n",
    "            ToolParam(name=\"operation\", type=\"string\", description=\"The operation to perform: 'add', 'subtract', 'multiply', or 'divide'.\", required=True)\n",
    "        ],\n",
    "        returns=ToolReturn(type=\"float\", description=\"The result of the arithmetic operation.\")\n",
    "    )\n",
    "\n",
    "    def __init__(self, final_answer: bool = False):\n",
    "        self.final_answer = final_answer\n",
    "\n",
    "    def __call__(self, num1: float, num2: float, operation: str) -> ToolResponse:\n",
    "        if operation not in [\"add\", \"subtract\", \"multiply\", \"divide\"]:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                observation=\"Invalid operation. Supported operations are 'add', 'subtract', 'multiply', and 'divide'.\",\n",
    "                extras={}\n",
    "            )\n",
    "        \n",
    "        if operation == \"divide\" and num2 == 0:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                observation=\"Division by zero is not allowed.\",\n",
    "                extras={}\n",
    "            )\n",
    "        try:\n",
    "            num1 = float(num1)\n",
    "            num2 = float(num2)\n",
    "            if operation == \"add\":\n",
    "                result = num1 + num2\n",
    "            elif operation == \"subtract\":\n",
    "                result = num1 - num2\n",
    "            elif operation == \"multiply\":\n",
    "                result = num1 * num2\n",
    "            elif operation == \"divide\":\n",
    "                result = num1 / num2\n",
    "        except Exception as e:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                observation=f\"An error occurred: {str(e)}\",\n",
    "                extras={}\n",
    "            )\n",
    "        \n",
    "        return ToolResponse(\n",
    "            final_answer=self.final_answer,\n",
    "            observation=float(result),\n",
    "            extras={}\n",
    "        )\n",
    "\n",
    "toolkit = Toolkit(\n",
    "    tools=[\n",
    "        CalculatorTool(),\n",
    "        GeneralQueryAssistantTool(llm=LLM, stream=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "tools_router_agent = ToolsRouterAgent(\n",
    "    toolkit=toolkit,\n",
    "    llm=LLM,\n",
    "    verbose=True,\n",
    "    specific_instructions=\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f9ca96",
   "metadata": {},
   "source": [
    "Test ToolsRouterAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98d83d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:32\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Using tool: calculator\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:32\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Raw inputs: {'num1': 20.0, 'num2': 90.0, 'operation': 'multiply'}\n",
      "1800.0"
     ]
    }
   ],
   "source": [
    "query = \"Perform the calculation 20 * 90\"\n",
    "\n",
    "for chunk in tools_router_agent.run(query, temperature=0.7, max_new_tokens=4000):\n",
    "    print(chunk, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c6635c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:33\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Using tool: general_query_assistant\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:33\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Raw inputs: {'query': 'Tell me a light-hearted joke!'}\n",
      "Why don't skeletons fight each other? They don't have the guts!None"
     ]
    }
   ],
   "source": [
    "query = \"Tell me a light-hearted joke!\"\n",
    "\n",
    "for chunk in tools_router_agent.run(query, temperature=0.7, max_new_tokens=4000):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafe29f",
   "metadata": {},
   "source": [
    "## ReactAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa87f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[ğŸ§ ] Chain of Thoughts...\n",
      "Thought: I need to calculate 100 + 200 / 300 first, and then find a light-hearted joke based on the result. Let's start with the calculation.\n",
      "\n",
      "Action: {\n",
      "  \"tool\": \"calculator\",\n",
      "  \"inputs\": {\n",
      "    \"num1\": 100,\n",
      "    \"num2\": 200 / 300,\n",
      "    \"operation\": \"add\"\n",
      "  },\n",
      "  \"final_answer\": false\n",
      "}<__final_answer__>The result of $100 + \\frac{200}{300}$ is approximately $100.67$. \n",
      "\n",
      "Here's a light-hearted joke:  \n",
      "Why did the fraction get a promotion?  \n",
      "Because it was *always* in its **best interest**! ğŸ˜„</__final_answer__>"
     ]
    }
   ],
   "source": [
    "from neurosurfer.agents.react import ReActAgent, ReActConfig\n",
    "\n",
    "react_agent = ReActAgent(\n",
    "    toolkit=toolkit,\n",
    "    llm=LLM,\n",
    "    specific_instructions=\"Always be concise in your answers. Break the task into steps if needed.\",\n",
    "    config=ReActConfig(\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=4096,\n",
    "        allow_input_pruning=True,\n",
    "        repair_with_llm=True,\n",
    "        skip_special_tokens=True,\n",
    "        verbose=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# print(react_agent._system_prompt())\n",
    "TASK = \"\"\"Calculate 100 + 200 / 300. Then tell me a light-hearted joke about that result.\"\"\"\n",
    "\n",
    "for chunk in react_agent.run(TASK):\n",
    "    print(chunk, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41580e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c7a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1d90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766512d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad2c2b-4a95-4ac0-9727-4c746e97a629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ea014-d67c-4af9-b2f6-1a8d80d63b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
