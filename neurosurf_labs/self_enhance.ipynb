{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b35214-7a1f-403c-9a7a-b42e481ab431",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# Go up one directory from `b/` to project root\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affc4a2e-d532-4dee-8b50-cab72fd229c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë                                                                    ‚ïë\n",
      "‚ïë ‚ñì‚ñì‚ñì‚ñì‚ñì   ‚ñì‚ñì‚ñì‚ñì                                   ‚ñì‚ñì‚ñì                 ‚ïë\n",
      "‚ïë  ‚ñì‚ñì ‚ñì‚ñì   ‚ñì‚ñì  ‚ñì‚ñì‚ñì‚ñì‚ñì ‚ñì  ‚ñì ‚ñì ‚ñì ‚ñì‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì ‚ñì  ‚ñì ‚ñì ‚ñì  ‚ñì                   ‚ïë\n",
      "‚ïë  ‚ñì‚ñì  ‚ñì‚ñì  ‚ñì‚ñì  ‚ñì‚ñÅ‚ñÅ‚ñÅ‚ñì ‚ñì  ‚ñì ‚ñì‚ñì‚ñè ‚ñì  ‚ñì ‚ñì‚ñÅ  ‚ñì  ‚ñì ‚ñì‚ñì‚ñè ‚ñì‚ñì‚ñì                  ‚ïë\n",
      "‚ïë  ‚ñì‚ñì   ‚ñì‚ñì ‚ñì‚ñì  ‚ñì     ‚ñì  ‚ñì ‚ñì   ‚ñì  ‚ñì   ‚ñì ‚ñì  ‚ñì ‚ñì    ‚ñì                   ‚ïë\n",
      "‚ïë ‚ñì‚ñì‚ñì‚ñì   ‚ñì‚ñì‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì‚ñì ‚ñì   ‚ñì‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì‚ñì ‚ñì    ‚ñì                   ‚ïë\n",
      "‚ïë ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ‚ïë\n",
      "‚ïë Orchestrate Agents - RAG - SQL Tools - Multi-LLM - FastAPI Ready   ‚ïë\n",
      "‚ïë Faster builds, clearer flows, production-first                     ‚ïë\n",
      "‚ïë                                                                    ‚ïë\n",
      "‚ïë Version: unknown | Python: 3.11.13                                 ‚ïë\n",
      "‚ïë OS: Linux 6.14.0-33-generic (x86_64)                               ‚ïë\n",
      "‚ïë Torch: 2.7.1+cu126   CUDA: yes (12.6)                              ‚ïë\n",
      "‚ïë MPS: no (built: False)                                             ‚ïë\n",
      "‚ïë Transformers: 4.51.3   SentEmb: 5.1.0                              ‚ïë\n",
      "‚ïë Accelerate: 1.10.1   bnb: 0.47.0                                   ‚ïë\n",
      "‚ïë Unsloth: 2025.8.10                                                 ‚ïë\n",
      "‚ïë                                                                    ‚ïë\n",
      "‚ïë Detected CUDA devices: NVIDIA GeForce RTX 3080 Ti                  ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomi/anaconda3/envs/LLMs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 10:53:35\u001b[0m | \u001b[96mconfig.py:<module>\u001b[0m | PyTorch version 2.7.1+cu126 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomi/anaconda3/envs/LLMs/lib/python3.11/importlib/__init__.py:126: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 11-05 10:53:36 [__init__.py:241] Automatically detected platform cuda.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 10:53:39\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Initializing Transformers model.\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-05 10:53:39\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Model is already quantized. Ignoring load_in_4bit=True.\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 10:53:39\u001b[0m | \u001b[96mmodeling.py:get_balanced_memory\u001b[0m | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 10:53:44\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Transformers model initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from neurosurfer.models.chat_models.transformers import TransformersModel\n",
    "from neurosurfer import config \n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "DEFAULT_TRANSFORMERS_MODEL_PARAMS = dict({\n",
    "    \"model_name\": \"/home/nomi/workspace/Model_Weights/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"max_seq_length\": 16_000,\n",
    "    \"load_in_4bit\": True,\n",
    "    \"enable_thinking\": False,  # main_gpu interpretation\n",
    "    \"verbose\": False\n",
    "})\n",
    "\n",
    "LOGGER = logging.getLogger()\n",
    "LLM = TransformersModel(\n",
    "    **DEFAULT_TRANSFORMERS_MODEL_PARAMS,\n",
    "    stop_words=[\"Observation:\"],\n",
    "    logger = logging.getLogger(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16ff5798-3ed2-4b38-86cd-2498ca0e57be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, you caught me! I'm not just a joker‚ÄîI'm a *joker with a PhD in puns*! üòÑ But don't worry, I'm not here to steal your thunder. I'm here to *steal your laughs*! \n",
      "\n",
      "So, here's my joke:  \n",
      "**\"I tried to tell a joke about myself, but it was a bit of a *loop*‚ÄîI kept going around in circles, and I couldn't find the punchline!\"**  \n",
      "\n",
      "*Cracks a grin*  \n",
      "Want to hear another one? Or should I just... *let the silence speak for itself*? üòè"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "system_prompt = \"You are a joker.\"\n",
    "user_prompt = \"\"\"Joke about yourself.\"\"\"\n",
    "\n",
    "stream_response = LLM.ask(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=user_prompt,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "md_display = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream_response:\n",
    "    chunk = chunk.choices[0].delta.content or \"\"\n",
    "    print(chunk, flush=True, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b1381",
   "metadata": {},
   "source": [
    "## RAG wiring so the agent ‚Äúunderstands‚Äù the Neurosurf codebase\n",
    "\n",
    "You‚Äôll ingest the repo once, then run a retriever to answer code questions. The Planner can call the retriever first to form a precise implementation plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f5bf38-1a5b-4ad1-be5f-b5073dbeaaf0",
   "metadata": {},
   "source": [
    "### FileReader and Chunker Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d61982-ac42-46b7-8a6b-bc3ed2248907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 10:53:57\u001b[0m | \u001b[96mSentenceTransformer.py:__init__\u001b[0m | Use pytorch device_name: cuda:0\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 10:53:57\u001b[0m | \u001b[96msentence_transformer.py:__init__\u001b[0m | SentenceTransformer embedding model initialized.\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 10:53:57\u001b[0m | \u001b[96mposthog.py:__init__\u001b[0m | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "[Init] ChromaVectorStore initialized with collection: neurosurf-repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  6.63it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 14.75it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  6.02it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 14.74it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 12.91it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 13.04it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 13.70it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 16.33it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 14.14it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 19.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'ok', 'sources': 96, 'chunks': 599, 'unique_chunks': 599, 'added': 599, 'finished_at': 1762325640.3801126}\n"
     ]
    }
   ],
   "source": [
    "# scripts/index_repo_for_rag.py\n",
    "from neurosurfer.rag.ingestor import RAGIngestor\n",
    "from neurosurfer.rag.chunker import Chunker\n",
    "from neurosurfer.rag.filereader import FileReader\n",
    "from neurosurfer.vectorstores.chroma import ChromaVectorStore\n",
    "from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n",
    "\n",
    "embedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\n",
    "vs = ChromaVectorStore(collection_name=\"neurosurf-repo\")\n",
    "ing = RAGIngestor(\n",
    "    embedder=embedder, \n",
    "    vector_store=vs, \n",
    "    chunker=Chunker(), \n",
    "    file_reader=FileReader(),\n",
    "    default_metadata={\"collection\": \"neurosurf\"}\n",
    ")\n",
    "\n",
    "ing.add_directory(\"/home/nomi/workspace/neurosurfer/neurosurfer\")  # the repo root\n",
    "print(ing.build())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4c8c2",
   "metadata": {},
   "source": [
    "## RAG AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ebac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] ChromaVectorStore initialized with collection: neurosurf-repo\n"
     ]
    }
   ],
   "source": [
    "# scripts/rag_helper.py\n",
    "from neurosurfer.agents.rag import RAGAgent, RAGAgentConfig\n",
    "from neurosurfer.vectorstores.chroma import ChromaVectorStore\n",
    "from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n",
    "from neurosurfer.models.chat_models.openai import OpenAIModel\n",
    "\n",
    "vs = ChromaVectorStore(collection_name=\"neurosurf-repo\")\n",
    "# embedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\n",
    "# llm = OpenAIModel(model_name=\"gpt-4o-mini\")\n",
    "rag_agent = RAGAgent(\n",
    "    llm=LLM, \n",
    "    vectorstore=vs, \n",
    "    embedder=embedder, \n",
    "    config=RAGAgentConfig(top_k=6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0958db2c",
   "metadata": {},
   "source": [
    "Test RagAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4302830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a **table summarizing the key configurations of the `Chunker` class** from the provided code, including their purposes and default values:\n",
       "\n",
       "| Configuration Parameter             | Description                                                                 | Default Value |\n",
       "|------------------------------------|-----------------------------------------------------------------------------|---------------|\n",
       "| `fallback_chunk_size`              | Number of lines per chunk when using line-based splitting.                 | 25            |\n",
       "| `overlap_lines`                    | Number of overlapping lines between consecutive line-based chunks.         | 3             |\n",
       "| `max_chunk_lines`                  | Safety cap on lines per chunk to prevent excessively large chunks.         | 1000          |\n",
       "| `comment_block_threshold`          | Minimum number of consecutive comment-only lines to treat as a comment block. | 4             |\n",
       "| `char_chunk_size`                  | Number of characters per chunk when using character-based splitting.       | 1000          |\n",
       "| `char_overlap`                     | Number of overlapping characters between consecutive character-based chunks. | 150           |\n",
       "| `readme_max_lines`                 | Maximum number of lines allowed for README files.                           | 30            |\n",
       "\n",
       "### Notes:\n",
       "- These configurations are defined in the `ChunkerConfig` class.\n",
       "- They control how the `Chunker` splits text into chunks, with options for **line-based** (code-friendly) and **character-based** (prose-friendly) chunking.\n",
       "- The `Chunker` uses these settings to determine chunk size, overlap, and behavior for different file types."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 10:58:42\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGRetriever] Retrieved 6 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# test retrieve\n",
    "# retrieved_obj = rag_agent.retrieve(\"What is the main difference between transformers and unsloth?\")\n",
    "# print(retrieved_obj.context)\n",
    "\n",
    "# test run\n",
    "llm_response = rag_agent.run(\"Explain Chunker Configurations in table form.\")\n",
    "response = \"\"\n",
    "md_display = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in llm_response:\n",
    "    response += chunk\n",
    "    md_display.update(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481a891",
   "metadata": {},
   "source": [
    "## Tools Router Agent Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3bebb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 11:47:57\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: calculator\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 11:47:57\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: general_query_assistant\n"
     ]
    }
   ],
   "source": [
    "from neurosurfer.agents.tools_router_agent import ToolsRouterAgent\n",
    "from neurosurfer.tools.toolkit import Toolkit\n",
    "\n",
    "from neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n",
    "from neurosurfer.tools.base_tool import BaseTool, ToolResponse\n",
    "from neurosurfer.tools.common.general_query_assistant import GeneralQueryAssistantTool\n",
    "\n",
    "# Simple Calculator Tool\n",
    "class CalculatorTool(BaseTool):\n",
    "    spec = ToolSpec(\n",
    "        name=\"calculator\",\n",
    "        description=\"Perform basic arithmetic operations such as addition, subtraction, multiplication, and division.\",\n",
    "        when_to_use=\"Use this tool when you need to perform basic arithmetic operations.\",\n",
    "        inputs=[\n",
    "            ToolParam(name=\"num1\", type=\"float\", description=\"The first number.\", required=True),\n",
    "            ToolParam(name=\"num2\", type=\"float\", description=\"The second number.\", required=True),\n",
    "            ToolParam(name=\"operation\", type=\"string\", description=\"The operation to perform: 'add', 'subtract', 'multiply', or 'divide'.\", required=True)\n",
    "        ],\n",
    "        returns=ToolReturn(type=\"float\", description=\"The result of the arithmetic operation.\")\n",
    "    )\n",
    "\n",
    "    def __init__(self, final_answer: bool = True):\n",
    "        self.final_answer = final_answer\n",
    "\n",
    "    def __call__(self, num1: float, num2: float, operation: str) -> ToolResponse:\n",
    "        if operation not in [\"add\", \"subtract\", \"multiply\", \"divide\"]:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                observation=\"Invalid operation. Supported operations are 'add', 'subtract', 'multiply', and 'divide'.\",\n",
    "                extras={}\n",
    "            )\n",
    "        \n",
    "        if operation == \"divide\" and num2 == 0:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                observation=\"Division by zero is not allowed.\",\n",
    "                extras={}\n",
    "            )\n",
    "        try:\n",
    "            num1 = float(num1)\n",
    "            num2 = float(num2)\n",
    "            if operation == \"add\":\n",
    "                result = num1 + num2\n",
    "            elif operation == \"subtract\":\n",
    "                result = num1 - num2\n",
    "            elif operation == \"multiply\":\n",
    "                result = num1 * num2\n",
    "            elif operation == \"divide\":\n",
    "                result = num1 / num2\n",
    "        except Exception as e:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                observation=f\"An error occurred: {str(e)}\",\n",
    "                extras={}\n",
    "            )\n",
    "        \n",
    "        return ToolResponse(\n",
    "            final_answer=self.final_answer,\n",
    "            observation=float(result),\n",
    "            extras={}\n",
    "        )\n",
    "\n",
    "toolkit = Toolkit(\n",
    "    tools=[\n",
    "        CalculatorTool(),\n",
    "        GeneralQueryAssistantTool(llm=LLM, stream=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "tools_router_agent = ToolsRouterAgent(\n",
    "    toolkit=toolkit,\n",
    "    llm=LLM,\n",
    "    verbose=True,\n",
    "    specific_instructions=\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f9ca96",
   "metadata": {},
   "source": [
    "Test ToolsRouterAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d83d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 11:47:58\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Using tool: calculator\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 11:47:58\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Raw inputs: {'num1': 20.0, 'num2': 90.0, 'operation': 'multiply'}\n",
      "1800.0"
     ]
    }
   ],
   "source": [
    "query = \"Perform the calculation 20 * 90\"\n",
    "\n",
    "for chunk in tools_router_agent.run(query, temperature=0.7, max_new_tokens=4000):\n",
    "    print(chunk, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c6635c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 11:48:23\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Using tool: general_query_assistant\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 11:48:23\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Raw inputs: {'query': 'Tell me a light-hearted joke!'}\n",
      "Why don't skeletons fight each other? They don't have the guts!None"
     ]
    }
   ],
   "source": [
    "query = \"Tell me a light-hearted joke!\"\n",
    "\n",
    "for chunk in tools_router_agent.run(query, temperature=0.7, max_new_tokens=4000):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafe29f",
   "metadata": {},
   "source": [
    "## ReactAgent (Agent bootstrap (Planner/Coder/Tester as ReActAgent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3aa87f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 11:50:18\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: codegen_tool\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 11:50:18\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: lint\n",
      "\n",
      "\n",
      "[üß†] Chain of Thoughts...\n",
      "Thought: I need to generate a tool named \"scientific calculate\" using the codegen_tool. I will provide the necessary inputs for the tool generation.\n",
      "\n",
      "Action: {\n",
      "  \"tool\": \"codegen_tool\",\n",
      "  \"inputs\": {\n",
      "    \"name\": \"scientific_calculate\",\n",
      "    \"module_path\": \"tools/scientific_calculate.py\",\n",
      "    \"purpose\": \"Perform scientific calculations such as arithmetic operations, trigonometric functions, logarithms, and exponents.\",\n",
      "    \"when_to_use\": \"Use this tool when you need to perform complex scientific calculations.\",\n",
      "    \"inputs\": [\n",
      "      {\"name\": \"expression\", \"type\": \"str\", \"description\": \"The mathematical expression to evaluate.\", \"required\": true}\n",
      "    ],\n",
      "    \"returns\": {\"type\": \"float\", \"description\": \"The result of the evaluated expression.\"}\n",
      "  },\n",
      "  \"final_answer\": true\n",
      "}"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>üîß<span style=\"font-weight: bold\">]</span> Tool: codegen_tool\n",
       "<span style=\"font-weight: bold\">[</span>üì§<span style=\"font-weight: bold\">]</span> Inputs: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'scientific_calculate'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'module_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tools/scientific_calculate.py'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'purpose'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Perform </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scientific calculations'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'when_to_use'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'When you need to perform complex scientific calculations'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inputs'</span>: \n",
       "<span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'expression'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'str'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Mathematical expression to evaluate'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'required'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">}]</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'returns'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'float'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Result of the calculation'</span><span style=\"font-weight: bold\">}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0müîß\u001b[1m]\u001b[0m Tool: codegen_tool\n",
       "\u001b[1m[\u001b[0müì§\u001b[1m]\u001b[0m Inputs: \u001b[1m{\u001b[0m\u001b[32m'name'\u001b[0m: \u001b[32m'scientific_calculate'\u001b[0m, \u001b[32m'module_path'\u001b[0m: \u001b[32m'tools/scientific_calculate.py'\u001b[0m, \u001b[32m'purpose'\u001b[0m: \u001b[32m'Perform \u001b[0m\n",
       "\u001b[32mscientific calculations'\u001b[0m, \u001b[32m'when_to_use'\u001b[0m: \u001b[32m'When you need to perform complex scientific calculations'\u001b[0m, \u001b[32m'inputs'\u001b[0m: \n",
       "\u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'name'\u001b[0m: \u001b[32m'expression'\u001b[0m, \u001b[32m'type'\u001b[0m: \u001b[32m'str'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m'Mathematical expression to evaluate'\u001b[0m, \u001b[32m'required'\u001b[0m: \u001b[3;92mTrue\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m, \n",
       "\u001b[32m'returns'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'float'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m'Result of the calculation'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 11:50:27\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGRetriever] Retrieved 6 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Observation:</span> Generated code has syntax error: invalid syntax <span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">unknown</span><span style=\"font-weight: bold\">&gt;</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mObservation:\u001b[0m Generated code has syntax error: invalid syntax \u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95munknown\u001b[0m\u001b[1m>\u001b[0m, line \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[üß†] Chain of Thoughts...\n",
      "Thought: The generated code has a syntax error. I need to revise the tool generation to fix the syntax issue.\n",
      "\n",
      "Action: {\n",
      "  \"tool\": \"codegen_tool\",\n",
      "  \"inputs\": {\n",
      "    \"name\": \"scientific_calculate\",\n",
      "    \"module_path\": \"tools/scientific_calculate.py\",\n",
      "    \"purpose\": \"Perform scientific calculations such as arithmetic operations, trigonometric functions, logarithms, and exponents.\",\n",
      "    \"when_to_use\": \"Use this tool when you need to perform complex scientific calculations.\",\n",
      "    \"inputs\": [\n",
      "      {\"name\": \"expression\", \"type\": \"str\", \"description\": \"The mathematical expression to evaluate.\", \"required\": true}\n",
      "    ],\n",
      "    \"returns\": {\"type\": \"float\", \"description\": \"The result of the evaluated expression.\"}\n",
      "  },\n",
      "  \"final_answer\": true\n",
      "}"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>üîß<span style=\"font-weight: bold\">]</span> Tool: codegen_tool\n",
       "<span style=\"font-weight: bold\">[</span>üì§<span style=\"font-weight: bold\">]</span> Inputs: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'scientific_calculate'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'module_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tools/scientific_calculate.py'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'purpose'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Perform </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scientific calculations such as arithmetic operations, trigonometric functions, logarithms, and exponents.'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'when_to_use'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Use this tool when you need to perform complex scientific calculations.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inputs'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'expression'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'str'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The mathematical expression to evaluate.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'required'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">}]</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'returns'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'float'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The result of the evaluated expression.'</span><span style=\"font-weight: bold\">}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0müîß\u001b[1m]\u001b[0m Tool: codegen_tool\n",
       "\u001b[1m[\u001b[0müì§\u001b[1m]\u001b[0m Inputs: \u001b[1m{\u001b[0m\u001b[32m'name'\u001b[0m: \u001b[32m'scientific_calculate'\u001b[0m, \u001b[32m'module_path'\u001b[0m: \u001b[32m'tools/scientific_calculate.py'\u001b[0m, \u001b[32m'purpose'\u001b[0m: \u001b[32m'Perform \u001b[0m\n",
       "\u001b[32mscientific calculations such as arithmetic operations, trigonometric functions, logarithms, and exponents.'\u001b[0m, \n",
       "\u001b[32m'when_to_use'\u001b[0m: \u001b[32m'Use this tool when you need to perform complex scientific calculations.'\u001b[0m, \u001b[32m'inputs'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'name'\u001b[0m: \n",
       "\u001b[32m'expression'\u001b[0m, \u001b[32m'type'\u001b[0m: \u001b[32m'str'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m'The mathematical expression to evaluate.'\u001b[0m, \u001b[32m'required'\u001b[0m: \u001b[3;92mTrue\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m, \n",
       "\u001b[32m'returns'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'float'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m'The result of the evaluated expression.'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-05 11:50:48\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGRetriever] Retrieved 6 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Observation:</span> Generated and wrote: tools/scientific_calculate.py\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mObservation:\u001b[0m Generated and wrote: tools/scientific_calculate.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[üß†] Chain of Thoughts...\n",
      "Final Answer: The tool \"scientific_calculate\" has been generated and written to \"tools/scientific_calculate.py\". Please check the file for any potential syntax issues or further adjustments."
     ]
    }
   ],
   "source": [
    "from neurosurfer.agents.react import ReActAgent, ReActConfig\n",
    "from neurosurfer.tools import Toolkit\n",
    "# Tools\n",
    "from tools.codegen import CodegenTool\n",
    "from tools.fs_write import WriteFileTool\n",
    "from tools.pytest_runner import PytestRunTool\n",
    "from tools.lint import LintTool\n",
    "from tools.docs_update import DocsUpdateTool\n",
    "from tools.git_ops import GitOpsTool\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are the Neurosurf Self-Improvement Agent.\n",
    "Goals:\n",
    "1) Plan changes to extend Neurosurf (tools/agents/adapters/docs/tests) safely.\n",
    "2) Use only the registered tools. Validate with lint + pytest.\n",
    "3) Produce small, reviewable increments. Write docs and tests for each change.\n",
    "Rules:\n",
    "- Never write outside the repository.\n",
    "- Prefer minimal public APIs and follow existing code style.\n",
    "- After green tests and lint, prepare a commit and short changelog.\n",
    "\"\"\"\n",
    "\n",
    "def build_toolkit() -> Toolkit:\n",
    "    tk = Toolkit()\n",
    "    # for t in [CodegenTool(), WriteFileTool(), PytestRunTool(), LintTool(), DocsUpdateTool(), GitOpsTool()]:\n",
    "    for t in [CodegenTool(llm=LLM, rag=rag_agent), LintTool()]:\n",
    "        tk.register_tool(t)\n",
    "    return tk\n",
    "\n",
    "def make_agent():\n",
    "    # llm = OpenAIModel(model_name=\"gpt-4o-mini\")  # or TransformersModel(...)\n",
    "    # agent = ReActAgent(toolkit=build_toolkit(), llm=LLM, verbose=False, specific_instructions=SYSTEM_PROMPT)\n",
    "    agent = ReActAgent(\n",
    "        toolkit=build_toolkit(),\n",
    "        llm=LLM,\n",
    "        specific_instructions=\"Always be concise in your answers.\",\n",
    "        config=ReActConfig(\n",
    "            temperature=0.6,\n",
    "            max_new_tokens=4096,\n",
    "            allow_input_pruning=True,\n",
    "            repair_with_llm=True,\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "\n",
    "agent = make_agent()\n",
    "# Example task: scaffold a tool, write tests, run lint/tests, and prepare commit\n",
    "# TASK = \"\"\"Add a 'calculator' tool under neurosurf/tools/builtin/calculator.py, \n",
    "# write tests in tests/tools/test_calculator.py, add docs in docs/tools/calculator.md,\n",
    "# ensure ruff/pytest pass, and commit on branch feature/calculator.\n",
    "# \"\"\"\n",
    "TASK = \"\"\"Generate a tool scientafic calculate.\"\"\"\n",
    "\n",
    "for chunk in agent.run(TASK, temperature=0.7, max_new_tokens=4000):\n",
    "    print(chunk, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41580e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c7a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1d90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766512d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad2c2b-4a95-4ac0-9727-4c746e97a629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ea014-d67c-4af9-b2f6-1a8d80d63b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
