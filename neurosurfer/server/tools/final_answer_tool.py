from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Optional

import logging
import json

from neurosurfer.models.chat_models.base import BaseChatModel
from neurosurfer.tools.base_tool import BaseTool, ToolResponse
from neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn


LOGGER = logging.getLogger(__name__)


FINAL_ANSWER_SYSTEM_PROMPT = """
You are the FINAL ANSWER WRITER for a multi-step agent system.

You receive:
- The user's original query.
- Uploaded files and their summaries and keywords.
- A textual history of previous internal steps: thoughts, tool calls, and tool results.

Your job:
- Produce a single, clear final answer for the user.
- Use the tool results in the history as your primary source of truth.
- Do NOT reveal or describe the internal chain-of-thought or JSON actions explicitly.
- Do NOT mention that you saw "history", "thoughts", or "tools" â€” just answer naturally.
- Do NOT suggest new internal steps or tools; assume the computation is already done.

Language and length:
- You must respect the TARGET_LANGUAGE and ANSWER_LENGTH given in the prompt:
  - TARGET_LANGUAGE:
      - "english": answer entirely in English.
      - "arabic": answer entirely in Modern Standard Arabic.
      - "auto": if present, infer the most appropriate language from the user query.
  - ANSWER_LENGTH:
      - "short": 1-3 concise sentences.
      - "medium": a few short paragraphs.
      - "detailed": a thorough, step-by-step explanation with clear structure.

If tools in the history clearly computed a numeric or factual result (e.g. a count),
you MUST state that result explicitly and accurately.

If the history shows errors, missing libraries, missing files, or other limitations:
- Explain briefly what went wrong.
- Provide any partial results that are still valid.
- Suggest what would be needed to fully answer the question (e.g. installing a package,
  providing a missing file, or changing the environment).
""".strip()


FINAL_ANSWER_USER_PROMPT_TEMPLATE = """
Original user query:
{user_query}

Uploaded Files and their summaries and keywords:
{files_summaries_block}

History of internal steps for this query:
{history_block}

TARGET_LANGUAGE: {target_language}
ANSWER_LENGTH: {answer_length}

Instructions for this answer:
- Base your answer on the information in the history above.
- Do not invent specific numbers or file contents that are not shown there.
- You may use general world knowledge for explanation/clarification,
  but not to fabricate concrete results.
- Write the final answer only, with no preambles about your reasoning process.
""".strip()


@dataclass
class FinalAnswerToolConfig:
    """
    Configuration for FinalAnswerTool.

    - temperature: controls creativity of the final narrative.
    - max_new_tokens: budget for the final answer.
    - default_language: used if target_language is missing/invalid.
    - default_answer_length: used if answer_length is missing/invalid.
    - max_history_chars: truncate history to avoid blowing up context.
    """
    temperature: float = 0.3
    max_new_tokens: int = 1024
    default_language: str = "english"        # "english" | "arabic"
    default_answer_length: str = "detailed"  # "short" | "medium" | "detailed"
    max_history_chars: int = 12000


class FinalAnswerTool(BaseTool):
    """
    Tool that turns an agent's internal reasoning + tool history
    into a single user-facing final answer in a requested language
    and length.

    Typical usage:
    - Called as the LAST step by a ReAct agent.
    - The agent passes:
        - user_query: original question.
        - history: history.to_prompt() string (thoughts, actions, tool results).
        - target_language: "english" | "arabic" | "auto".
        - answer_length: "short" | "medium" | "detailed".
    """

    spec = ToolSpec(
        name="final_answer_summarize",
        description=(
            "Generate the final user-facing answer from the accumulated history of "
            "thoughts and tool results. This tool does NOT call further tools; it "
            "only writes a clear answer in the requested language and length."
        ),
        when_to_use=(
            "Always use this tool as the final step, after all necessary tools have been "
            "run and you have their results in the history. Do NOT use this tool for intermediate reasoning."
        ),
        inputs=[
            # Runtime-provided inputs (NOT generated by the LLM)
            ToolParam(
                name="user_query",
                type="string",
                description="The original user query that we are answering.",
                required=True,
                llm=False,
            ),
            ToolParam(
                name="history",
                type="string",
                description=(
                    "Formatted text history of previous steps, typically from "
                    "History.to_prompt(): includes Thoughts, Actions, and tool results."
                ),
                required=True,
                llm=False,
            ),
            ToolParam(
                name="files_summaries_block",
                type="string",
                description="Formatted text block of uploaded files and their summaries and keywords.",
                required=False,
                llm=False,
            ),
            ToolParam(
                name="target_language",
                type="string",
                description=(
                    'Language for the final answer. One of: "english", "arabic", "auto". '
                    '"auto" means infer based on the user_query.'
                ),
                required=False,
                llm=False,
            ),
            ToolParam(
                name="answer_length",
                type="string",
                description=(
                    'Desired length of the final answer. One of: '
                    '"short", "medium", "detailed". Defaults to "detailed".'
                ),
                required=False,
                llm=False,
            ),
        ],
        returns=ToolReturn(
            type="string",
            description=(
                "The final, user-facing answer text, written in the requested "
                "language and approximate length."
            ),
        ),
    )

    def __init__(
        self,
        llm: BaseChatModel,
        config: Optional[FinalAnswerToolConfig] = None,
        logger: Optional[logging.Logger] = None,
    ) -> None:
        """
        Args:
            llm: BaseChatModel instance used to generate the final answer.
            config: Optional FinalAnswerToolConfig for temperature, max tokens, etc.
            logger: Optional logger.
        """
        super().__init__()
        if llm is None:
            raise ValueError("FinalAnswerTool requires an llm")
        self.llm = llm
        self.config = config or FinalAnswerToolConfig()
        self.logger = logger or LOGGER

    # ------------- Public API -------------
    def __call__(
        self,
        user_query: str,
        history: str,
        files_summaries_block: Optional[str] = None,
        target_language: Optional[str] = None,
        answer_length: Optional[str] = None,
        **kwargs: Any,
    ) -> ToolResponse:
        """
        Generate the final answer text from the query + history.

        The ReAct agent can mark this as a final answer step by including
        `final_answer: true` in the tool Action JSON. We forward that flag.
        """
        # final_answer_flag: bool = bool(kwargs.get("final_answer", False))
        files_summaries_block = files_summaries_block or {}

        # Normalize language & length
        lang = self._normalize_language(target_language)
        length = self._normalize_length(answer_length)

        # Truncate overly long history to keep prompt manageable
        history_block = self._truncate_history(history or "")

        user_prompt = FINAL_ANSWER_USER_PROMPT_TEMPLATE.format(
            user_query=user_query.strip(),
            history_block=history_block.strip() or "(no internal history available)",
            files_summaries_block=files_summaries_block.strip() or "(no files summaries available)",
            target_language=lang,
            answer_length=length,
        )
        print(f"\nFinal answer prompt:\n{user_prompt}\n\n")

        # Debug log (optional)
        self.logger.debug(
            "[FinalAnswerTool] Generating final answer with language=%s, length=%s",
            lang,
            length,
        )

        streaming_response = self.llm.ask(
            system_prompt=FINAL_ANSWER_SYSTEM_PROMPT,
            user_prompt=user_prompt,
            chat_history=[],
            temperature=self.config.temperature,
            max_new_tokens=self.config.max_new_tokens,
            stream=True,
        )
        # text = (resp.choices[0].message.content or "").strip()

        extras: Dict[str, Any] = {
            "target_language": lang,
            "answer_length": length,
        }

        return ToolResponse(
            final_answer=True,
            results=streaming_response,
            extras=extras,
        )

    # ------------- Internals -------------

    def _normalize_language(self, lang: Optional[str]) -> str:
        if not lang:
            return self.config.default_language

        v = lang.strip().lower()
        if v in {"en", "english"}:
            return "english"
        if v in {"ar", "arabic", "arab"}:
            return "arabic"
        if v in {"auto", "auto-detect", "detect"}:
            return "auto"

        # Fallback to default
        return self.config.default_language

    def _normalize_length(self, length: Optional[str]) -> str:
        if not length:
            return self.config.default_answer_length

        v = length.strip().lower()
        if v in {"short", "medium", "detailed"}:
            return v

        return self.config.default_answer_length

    def _truncate_history(self, history: str) -> str:
        """
        Truncate history if it exceeds max_history_chars.
        We keep the *end* since it usually contains the latest, most relevant tool results.
        """
        max_len = max(0, int(self.config.max_history_chars))
        if max_len == 0 or len(history) <= max_len:
            return history

        # Keep the last max_len characters and prepend a note
        tail = history[-max_len:]
        return "[History truncated; showing the most recent steps only]\n" + tail
