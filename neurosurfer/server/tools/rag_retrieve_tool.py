from __future__ import annotations

import logging
import os
import re
from typing import Any, Dict, List, Optional

from neurosurfer.tools.base_tool import BaseTool, ToolResponse
from neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn

from neurosurfer.server.services.rag.gate import RAGGate
from neurosurfer.server.services.rag.orchestrator import RAGOrchestrator, RAGResult


LOGGER = logging.getLogger(__name__)


class RAGRetrieveTool(BaseTool):
    """
    Tool that performs the *retrieval half* of RAG:

    - Assumes files for the given (user_id, thread_id) are already ingested.
    - Uses RAGGate to decide:
        * rag: true/false
        * related_files
        * retrieval_scope
        * answer_breadth
    - Uses RAGAgent.retrieve(...) to fetch context from the vector store.
    - Returns the retrieved context as a string (for the LLM to consume) and
      attaches structured metadata in `extras` for the agent's memory.

    This tool is meant to be called by the ReActAgent, not by raw HTTP.
    """

    spec = ToolSpec(
        name="rag_retrieve",
        description=(
            "Retrieve relevant context from the user's uploaded files using a "
            "vector store and a routing (gate) model. "
            "This tool decides whether RAG is needed, which files to use, and "
            "how broad the retrieval should be."
        ),
        when_to_use=(
            "Use this tool whenever the user's question clearly depends on, or "
            "might depend on, the content of uploaded files (PDFs, CSVs, docs, etc.). "
            "For example: 'summarize the attached PDF', 'compare the two reports I uploaded', "
            "or 'based on the CSV I sent, what is the average score per subject?'. "
            "If the question is purely general and does not rely on the files, "
            "you should NOT call this tool."
        ),
        inputs=[
            # --- LLM-provided input ---
            ToolParam(
                name="query",
                type="string",
                description="The user's natural-language question.",
                required=True,
                llm=True,
            ),
            # --- Runtime-provided inputs (NOT generated by LLM) ---
            ToolParam(
                name="user_id",
                type="integer",
                description="Internal user ID for this chat session.",
                required=True,
                llm=False,
            ),
            ToolParam(
                name="thread_id",
                type="integer",
                description="Internal thread ID for this conversation.",
                required=True,
                llm=False,
            ),
        ],
        returns=ToolReturn(
            type="string",
            description=(
                "Retrieved textual context wrapped in [RAG CONTEXT] tags, or a short "
                "diagnostic message if RAG was not used or no context was found."
            ),
        ),
    )

    def __init__(
        self,
        rag_orchestrator: RAGOrchestrator,
        *,
        logger: Optional[logging.Logger] = None,
    ) -> None:
        """
        Args:
            rag_orchestrator: Shared RAGOrchestrator instance (configured with embedder/vectorstore).
            logger: Optional logger.
        """
        super().__init__()
        self.rag_orchestrator = rag_orchestrator
        self.logger = logger or LOGGER

    # ------------- Public API -------------
    def __call__(
        self,
        query: str,
        user_id: int,
        thread_id: int,
        **kwargs: Any,
    ) -> ToolResponse:
        """
        Execute the RAG retrieval flow:

        1. Resolve per-thread vector store path + collection.
        2. Check if there are any ingested files for this thread.
        3. Use RAGGate to decide rag / related_files / scope / breadth.
        4. If rag=False, return a non-final observation (agent can still answer directly).
        5. If rag=True, call RAGAgent.retrieve(...) to get context.
        6. If context is empty, return non-final observation.
        7. Otherwise, return [RAG CONTEXT]...[/RAG CONTEXT] as observation and
           stash metadata in extras.
        """
        if not user_id or not thread_id:
            return ToolResponse(
                final_answer=False,
                results="[RAG] No user_id/thread_id provided; skipping retrieval.",
                extras={"rag_used": False, "reason": "missing_ids"},
            )
        result: RAGResult = self.rag_orchestrator.retrieve(
            user_id=user_id,
            thread_id=thread_id,
            user_query=query,
            files=[],
        )
        return ToolResponse(
            final_answer=False,
            results=result.augmented_query,
            extras={
                "rag_used": True,
                "gate": result.meta,
                "optimized_query": result.augmented_query,
                "collection": result.meta.get("collection"),
                "rag_context": result.context,
                "retrieval_meta": {
                    "base_tokens": result.meta.get("base_tokens"),
                    "context_tokens_used": result.meta.get("context_tokens_used"),
                    "token_budget": result.meta.get("token_budget"),
                },
            },
        )
