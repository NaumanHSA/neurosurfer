{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc497c98",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ§  Using Neurosurfer in Jupyter â€” A Quick Start Guide\n",
    "\n",
    "Welcome to **Neurosurfer**, a unified framework for building **intelligent, tool-augmented LLM agents** â€” locally or in the cloud.  \n",
    "This notebook walks you through practical examples: loading models, wiring tools, building retrieval (RAG) pipelines, and streaming reasoning in real time â€” all using the same simple Python API.\n",
    "\n",
    "Think of Neurosurfer as a **developer-friendly bridge** between:\n",
    "- ğŸ§© **OpenAI-style APIs** and **local Transformers**\n",
    "- ğŸ”§ **Custom tools** and **autonomous agents**\n",
    "- ğŸ“š **Retrieval-augmented generation (RAG)** and **multi-model orchestration**\n",
    "\n",
    "> **Requirements:**  \n",
    "> Python 3.10+, `pip`, and optionally a GPU (for local Transformer backends).  \n",
    "> Internet access is only needed when using hosted APIs like OpenAI.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Installation\n",
    "\n",
    "You can install Neurosurfer from PyPI â€” either a minimal setup for lightweight use, or a full install for local model support.\n",
    "\n",
    "```bash\n",
    "# Core install: FastAPI backend, agents, tools, and RAG\n",
    "pip install neurosurfer\n",
    "\n",
    "# For local LLMs (Transformers, Unsloth, vLLM, etc.)\n",
    "pip install \"neurosurfer[torch]\"\n",
    "```\n",
    "\n",
    "Or clone and install from source if you want to explore or contribute:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/NaumanHSA/neurosurfer.git\n",
    "cd neurosurfer\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "Once installed, you can seamlessly switch between OpenAI models and local LLMs â€” no code changes required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d181b",
   "metadata": {},
   "source": [
    "\n",
    "### ğŸ”„ Enable Auto-Reload and Set Up Project Path\n",
    "\n",
    "This setup cell ensures your notebook always runs with the **latest version** of your local Neurosurfer code â€” ideal when youâ€™re actively editing the library.\n",
    "\n",
    "- `%load_ext autoreload` and `%autoreload 2` automatically reload any modified modules before executing a new cell, so you donâ€™t have to restart the kernel after every code change.\n",
    "- The `sys.path.append(root_dir)` line adds the **project root directory** to Pythonâ€™s import path, allowing you to import `neurosurfer` modules directly even when running this notebook from a subfolder (like `tutorials/`).\n",
    "\n",
    "This is a standard pattern for working on notebooks inside a development repo while keeping imports clean and relative to the package root.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "095f9106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# Project Folder Structure\n",
    "# tutorials/\n",
    "# â”œâ”€â”€ basic_usage.ipynb/\n",
    "# neurosurfer/\n",
    "# â”œâ”€â”€ agents/\n",
    "# â”‚   â”œâ”€â”€ ...\n",
    "# â”œâ”€â”€ tools/\n",
    "# â”‚   â”œâ”€â”€ ...\n",
    "# â””â”€â”€ ...\n",
    "\n",
    "# Go up one directory from `tutorials/` to project root\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(root_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f1577",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Loading a Local LLM with Neurosurfer\n",
    "\n",
    "**Neurosurfer** unifies **OpenAI-style** and **local transformer models** behind the same, simple API â€” whether you're running a large language model on your own GPU or calling a cloud-hosted one.\n",
    "\n",
    "This cell shows how to initialize a **local transformer model** using the `TransformersModel` interface â€” the same API used by OpenAI models.  \n",
    "\n",
    "Here we load **Qwen3-8B (4-bit quantized)** through the Unsloth backend. Neurosurfer automatically handles model loading, device selection (GPU/CPU), and unified prompting logic.  \n",
    "\n",
    "- `model_name` points to your local model folder.  \n",
    "- `max_seq_length` defines the maximum context window (useful for long documents).  \n",
    "- `load_in_4bit=True` enables **quantized inference**, reducing GPU memory usage.  \n",
    "- `enable_thinking` toggles reasoning visualization in certain modes.  \n",
    "- `stop_words` defines when generation should halt.  \n",
    "\n",
    "You can replace this local model with an **OpenAI model** by uncommenting the lower section â€” the rest of your prompt code stays identical, thanks to Neurosurferâ€™s **unified `.ask()` API**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e633813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                                                                  â•‘\n",
      "â•‘ â–“â–“â–“â–“â–“   â–“â–“â–“â–“                                  â–“â–“â–“                â•‘\n",
      "â•‘  â–“â–“ â–“â–“   â–“â–“  â–“â–“â–“â–“ â–“  â–“ â–“ â–“ â–“â–“â–“â–“ â–“â–“â–“ â–“  â–“ â–“ â–“  â–“   â–“â–“â–“â–“ â–“ â–“       â•‘\n",
      "â•‘  â–“â–“  â–“â–“  â–“â–“  â–“â–â–â–“ â–“  â–“ â–“â–“â– â–“  â–“ â–“â–  â–“  â–“ â–“â–“â– â–“â–“â–“  â–“â–â–â–“ â–“â–“        â•‘\n",
      "â•‘  â–“â–“   â–“â–“ â–“â–“  â–“    â–“  â–“ â–“   â–“  â–“   â–“ â–“  â–“ â–“    â–“   â–“    â–“         â•‘\n",
      "â•‘ â–“â–“â–“â–“   â–“â–“â–“â–“â–“ â–“â–“â–“â–“ â–“â–“â–“â–“ â–“   â–“â–“â–“â–“ â–“â–“â–“ â–“â–“â–“â–“ â–“    â–“   â–“â–“â–“â–“ â–“         â•‘\n",
      "â•‘ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ â•‘\n",
      "â•‘ Orchestrate Agents - RAG - SQL Tools - Multi-LLM - FastAPI Ready â•‘\n",
      "â•‘ Faster builds, clearer flows, production-first                   â•‘\n",
      "â•‘                                                                  â•‘\n",
      "â•‘ Version: 0.1.3 | Python: 3.11.13                                 â•‘\n",
      "â•‘ OS: Linux 6.14.0-33-generic (x86_64)                             â•‘\n",
      "â•‘ Torch: 2.7.1+cu126   CUDA: yes (12.6)                            â•‘\n",
      "â•‘ MPS: no (built: False)                                           â•‘\n",
      "â•‘ Transformers: 4.51.3   SentEmb: 5.1.0                            â•‘\n",
      "â•‘ Accelerate: 1.10.1   bnb: 0.47.0                                 â•‘\n",
      "â•‘ Unsloth: 2025.8.10                                               â•‘\n",
      "â•‘                                                                  â•‘\n",
      "â•‘ Detected CUDA devices: NVIDIA GeForce RTX 3080 Ti                â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomi/anaconda3/envs/LLMs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:33:23\u001b[0m | \u001b[96mconfig.py:<module>\u001b[0m | PyTorch version 2.7.1+cu126 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomi/anaconda3/envs/LLMs/lib/python3.11/importlib/__init__.py:126: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 11-06 11:33:24 [__init__.py:241] Automatically detected platform cuda.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:33:38\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Initializing Transformers model.\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-06 11:33:38\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Model is already quantized. Ignoring load_in_4bit=True.\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:33:38\u001b[0m | \u001b[96mmodeling.py:get_balanced_memory\u001b[0m | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:33:44\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Transformers model initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from neurosurfer.models.chat_models.transformers import TransformersModel\n",
    "from neurosurfer import config \n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "MODEL_PARAMS = dict({\n",
    "    \"model_name\": \"/home/nomi/workspace/Model_Weights/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"max_seq_length\": 16_000,\n",
    "    \"load_in_4bit\": True,\n",
    "    \"enable_thinking\": False,  # main_gpu interpretation\n",
    "    \"verbose\": False\n",
    "})\n",
    "\n",
    "LOGGER = logging.getLogger()\n",
    "LLM = TransformersModel(\n",
    "    **MODEL_PARAMS,\n",
    "    stop_words=[\"Observation:\"],\n",
    "    logger = logging.getLogger(),\n",
    ")\n",
    "\n",
    "# Optionally, we can use OpenAI models\n",
    "# from neurosurfer.models.chat_models.openai import OpenAIModel\n",
    "\n",
    "# # Use an env var like OPENAI_API_KEY or configure your backend in the model init.\n",
    "# llm = OpenAIModel(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# resp = llm.ask(\n",
    "#     system_prompt=\"You are concise.\",\n",
    "#     user_prompt=\"In one sentence, what is Neurosurfer?\",\n",
    "#     max_new_tokens=128,\n",
    "#     temperature=0.2,\n",
    "#     stream=False,\n",
    "# )\n",
    "# print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69036425",
   "metadata": {},
   "source": [
    "### ğŸ—£ï¸ Streaming Responses in Real Time\n",
    "\n",
    "Here we send a simple prompt to the model â€” with a **system role** (â€œYou are a jokerâ€) and a **user message** (â€œTell me a light-hearted joke.â€).  \n",
    "\n",
    "Setting `stream=True` enables **token-by-token streaming**, letting you display output as itâ€™s being generated â€” just like ChatGPTâ€™s live typing effect.  \n",
    "\n",
    "- `LLM.ask(...)` uses the same API whether your backend is local (`TransformersModel`) or cloud (`OpenAIModel`).  \n",
    "- `system_prompt` sets the modelâ€™s behavior/persona.  \n",
    "- `user_prompt` is the actual query.  \n",
    "- The `for chunk in stream_response:` loop iterates over streamed tokens, printing them as they arrive.  \n",
    "\n",
    "This setup is ideal for interactive demos or notebooks where you want to **see the model â€œthinkâ€ in real time**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff066e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why don't skeletons fight each other?  \n",
       "Because they don't have the *guts*! ğŸ˜„"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "system_prompt = \"You are a joker.\"\n",
    "user_prompt = \"\"\"Tell me a light-hearted joke.\"\"\"\n",
    "\n",
    "stream_response = LLM.ask(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=user_prompt,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "response = \"\"\n",
    "md_display = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream_response:\n",
    "    chunk = chunk.choices[0].delta.content or \"\"\n",
    "    response += chunk\n",
    "    md_display.update(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6141075",
   "metadata": {},
   "source": [
    "## ğŸ§© Building a RAG Knowledge Base from the Codebase\n",
    "\n",
    "This cell demonstrates how to **ingest your project repository** into a **Retrieval-Augmented Generation (RAG)** pipeline using Neurosurferâ€™s modular components.  \n",
    "\n",
    "Weâ€™re turning the entire `neurosurfer` source directory into a searchable knowledge base that future agents can reference when reasoning or generating code.\n",
    "\n",
    "Hereâ€™s what each part does:\n",
    "\n",
    "- [**`SentenceTransformerEmbedder`**](https://naumanhsa.github.io/neurosurfer/api-reference/models/embedders/sentence-transformer/) â€” converts text chunks into dense vector embeddings using a lightweight model (`intfloat/e5-small-v2`).  \n",
    "- [**`ChromaVectorStore`**](https://naumanhsa.github.io/neurosurfer/api-reference/vectorstores/chroma/) â€” stores those embeddings locally for fast semantic search.  \n",
    "- [**`RAGIngestor`**](https://naumanhsa.github.io/neurosurfer/api-reference/rag/ingestor/) â€” orchestrates reading, chunking, embedding, and storing documents in one unified pipeline.  \n",
    "- [**`Chunker`**](https://naumanhsa.github.io/neurosurfer/api-reference/rag/chunker/) â€” splits large files into manageable semantic segments.  \n",
    "- [**`FileReader`**](https://naumanhsa.github.io/neurosurfer/api-reference/rag/filereader/) â€” automatically detects and reads supported file types (Markdown, Python, text, etc.).  \n",
    "\n",
    "Finally:\n",
    "```python\n",
    "ing.add_directory(root_dir)\n",
    "print(ing.build())\n",
    "```\n",
    "\n",
    "scans the neurosurfer/ directory, processes all files, and builds the vector index.\n",
    "You can later query this index through your agents or retrieval tools â€” enabling the model to â€œunderstandâ€ its own codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c0f70c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:47:15\u001b[0m | \u001b[96mSentenceTransformer.py:__init__\u001b[0m | Use pytorch device_name: cuda:0\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:47:15\u001b[0m | \u001b[96msentence_transformer.py:__init__\u001b[0m | SentenceTransformer embedding model initialized.\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:47:15\u001b[0m | \u001b[96mposthog.py:__init__\u001b[0m | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "[Init] ChromaVectorStore initialized with collection: neurosurf-repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.19it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.65it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.22it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.92it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.93it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.82it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.30it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.90it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.03it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok',\n",
       " 'sources': 99,\n",
       " 'chunks': 608,\n",
       " 'unique_chunks': 608,\n",
       " 'added': 608,\n",
       " 'finished_at': 1762415237.9988394}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scripts/index_repo_for_rag.py\n",
    "from pathlib import Path\n",
    "from neurosurfer.rag.ingestor import RAGIngestor\n",
    "from neurosurfer.rag.chunker import Chunker\n",
    "from neurosurfer.rag.filereader import FileReader\n",
    "from neurosurfer.vectorstores.chroma import ChromaVectorStore\n",
    "from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n",
    "\n",
    "# Initialize embedder \n",
    "embedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\n",
    "\n",
    "# Initialize vector store using Chroma\n",
    "vs = ChromaVectorStore(collection_name=\"neurosurf-repo\")\n",
    "\n",
    "# Initialize RAGIngestor which ingests the repo and stores it in the vector store \n",
    "ing = RAGIngestor(\n",
    "    embedder=embedder,\n",
    "    vector_store=vs, \n",
    "    chunker=Chunker(), \n",
    "    file_reader=FileReader(),\n",
    "    default_metadata={\"collection\": \"neurosurf\"}\n",
    ")\n",
    "\n",
    "# Add directory to the ingestor. This will ingest all files in the directory and store it in the vector store \n",
    "root_dir = Path(os.getcwd()).parent.joinpath(\"neurosurfer\")\n",
    "ing.add_directory(root_dir)  # the repo root\n",
    "ing.build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afeddcd",
   "metadata": {},
   "source": [
    "### ğŸ” Creating a RAG Agent for Context-Aware Answers\n",
    "\n",
    "Now that the repository has been indexed, we can build a **RAGAgent** â€” a retrieval-augmented agent that can search, read, and reason over your stored knowledge base.  \n",
    "\n",
    "Hereâ€™s how it fits together:\n",
    "\n",
    "- [**`ChromaVectorStore`](https://naumanhsa.github.io/neurosurfer/api-reference/vectorstores/chroma/) reconnects to the same database we created earlier (`neurosurf-repo`), giving the agent instant access to embedded documents.  \n",
    "- [**`SentenceTransformerEmbedder`](https://naumanhsa.github.io/neurosurfer/api-reference/models/embedders/sentence-transformer/) ensures new queries are embedded in the same vector space.  \n",
    "- [**`LLM`](https://naumanhsa.github.io/neurosurfer/api-reference/models/chat-models/base-model/) handles the reasoning and response generation.  \n",
    "- [**`RAGAgentConfig(top_k=6)`](https://naumanhsa.github.io/neurosurfer/api-reference/agents/rag-agent/) tells the agent to retrieve the top 6 most relevant chunks per query.  \n",
    "\n",
    "This setup allows your model to **ground its answers** in your projectâ€™s actual code and documentation â€” effectively letting it *â€œread before it replies.â€*  \n",
    "Switching to an OpenAI backend (via `OpenAIModel`) is as simple as uncommenting that line â€” Neurosurfer keeps the agent logic unified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7613fd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] ChromaVectorStore initialized with collection: neurosurf-repo\n"
     ]
    }
   ],
   "source": [
    "# scripts/rag_helper.py\n",
    "from neurosurfer.agents.rag import RAGAgent, RAGAgentConfig\n",
    "from neurosurfer.vectorstores.chroma import ChromaVectorStore\n",
    "from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n",
    "from neurosurfer.models.chat_models.openai import OpenAIModel\n",
    "\n",
    "vs = ChromaVectorStore(collection_name=\"neurosurf-repo\")\n",
    "# embedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\n",
    "# llm = OpenAIModel(model_name=\"gpt-4o-mini\")\n",
    "rag_agent = RAGAgent(\n",
    "    llm=LLM, \n",
    "    vectorstore=vs, \n",
    "    embedder=embedder, \n",
    "    config=RAGAgentConfig(top_k=6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41b935",
   "metadata": {},
   "source": [
    "### ğŸ§  Querying Your Codebase with the RAG Agent\n",
    "\n",
    "With the **RAGAgent** ready, we can now ask natural-language questions about the code â€” and the agent will ground its responses in the ingested repository.  \n",
    "\n",
    "There are two key modes you can use:\n",
    "- **`retrieve()`** (commented out) â€” fetches the most relevant text chunks from the vector store without generating an answer. Useful for inspection, debugging or using the retrieved chunks as context for another LLM.  \n",
    "- **`run()`** â€” performs the full RAG cycle: retrieves context â†’ augments the prompt â†’ streams an LLM-generated answer.  \n",
    "\n",
    "In this example, we ask:\n",
    "```python\n",
    "\"Explain Chunker Configurations in table form.\"\n",
    "```\n",
    "\n",
    "The model retrieves relevant code and docstrings from the repo, then explains them clearly â€” in this case as a formatted Markdown table.\n",
    "\n",
    "The display block (`md_display.update`) allows the answer to appear live as it streams in, creating an interactive and visually rich notebook experience.\n",
    "This is how Neurosurfer bridges retrieval + reasoning, turning your codebase into an explorable knowledge source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d43a3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a **table summarizing the key configurations of the `Chunker` class** based on the provided code and documentation:\n",
       "\n",
       "| **Configuration Parameter**         | **Description**                                                                 | **Default Value**         | **Notes**                                                                 |\n",
       "|------------------------------------|---------------------------------------------------------------------------------|---------------------------|---------------------------------------------------------------------------|\n",
       "| `fallback_chunk_size`              | Maximum size (in lines or characters) for fallback chunking strategies.        | `30`                      | Used when no specific strategy is available.                            |\n",
       "| `overlap_lines`                   | Number of lines to overlap between consecutive chunks for context preservation. | `5`                       | Helps maintain continuity between chunks.                              |\n",
       "| `char_chunk_size`                 | Maximum size (in characters) for character-based chunking.                     | `1000`                    | Used for prose or generic text content.                                 |\n",
       "| `comment_block_threshold`         | Minimum number of comment lines to consider a block as comment-only.           | `4`                       | Used for comment-aware filtering.                                      |\n",
       "| `blacklist_patterns`              | List of regex patterns for files to exclude (e.g., `.git`, `node_modules`, etc.).| As defined in code         | Prevents processing of unwanted file types.                             |\n",
       "| `line_comment_markers`            | List of comment markers for line-based comment detection (e.g., `#`, `//`).     | `[\"#\"]`                   | Used to identify and filter comment-only blocks.                        |\n",
       "| `file_extension_handlers`         | Mapping of file extensions to custom chunking strategies.                       | Default registry          | Allows pluggable strategies for specific file types.                    |\n",
       "| `router_strategy`                 | Strategy for dynamic selection of chunking methods based on content analysis.   | Default router logic       | Prioritizes custom handlers, file extensions, and built-in strategies.  |\n",
       "| `safety_limits`                   | Safety limits for chunking (e.g., max chunk size, max lines per chunk).         | Default values            | Prevents processing of excessively large or unsafe files.              |\n",
       "| `use_ast_based_chunking`          | Whether to use AST-based chunking for code files (e.g., Python).                | `True`                    | Enabled for structural awareness in code files.                         |\n",
       "| `preserve_syntax_structure`       | Whether to preserve syntax structure when chunking code.                        | `True`                    | Ensures code formatting is maintained.                                 |\n",
       "| `exclude_binary_files`            | Whether to exclude binary files (e.g., images, videos).                         | `True`                    | Prevents processing of non-text files.                                 |\n",
       "| `max_chunk_lines`                 | Maximum number of lines per chunk for line-based chunking.                      | `100`                     | Controls chunk size for code-like content.                              |\n",
       "| `min_chunk_lines`                 | Minimum number of lines per chunk to avoid overly small chunks.                | `5`                       | Ensures chunks are meaningful.                                         |\n",
       "| `chunk_overlap_ratio`             | Ratio of overlap between chunks (e.g., 0.2 for 20% overlap).                    | `0.2`                     | Used for context continuity in line-based chunking.                    |\n",
       "| `chunking_strategy`               | Primary chunking strategy (e.g., line-based, character-based, AST-based).       | Determined by router      | Selected based on file type and content analysis.                       |\n",
       "\n",
       "---\n",
       "\n",
       "### Notes:\n",
       "- The `Chunker` uses a **priority-based strategy selection**:\n",
       "  1. **Explicit custom handlers** (registered by name or function)\n",
       "  2. **Router function results**\n",
       "  3. **File extension mappings**\n",
       "  4. **Built-in strategies** (e.g., for `.py`, `.js`, `.md`)\n",
       "  5. **Heuristic fallback** (line-based or character-based chunking)\n",
       "\n",
       "- The `blacklist_patterns` and `exclude_binary_files` help avoid processing non-text or unwanted files.\n",
       "\n",
       "Let me know if you'd like this in a different format (e.g., markdown, JSON, or code comments)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:51:46\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGRetriever] Retrieved 6 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# test retrieve\n",
    "# retrieved_obj = rag_agent.retrieve(\"What is the main difference between transformers and unsloth?\")\n",
    "# print(retrieved_obj.context)\n",
    "\n",
    "# test run\n",
    "llm_response = rag_agent.run(\"Explain Chunker Configurations in table form.\")\n",
    "response = \"\"\n",
    "md_display = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in llm_response:\n",
    "    response += chunk\n",
    "    md_display.update(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd116c3",
   "metadata": {},
   "source": [
    "## ğŸ§° Building a Multi-Tool Agent with the Tools Router\n",
    "\n",
    "This cell demonstrates how to create a **ToolsRouterAgent** â€” an intelligent agent that can dynamically decide **which tool to use** based on the userâ€™s request.  \n",
    "\n",
    "Hereâ€™s the breakdown:\n",
    "\n",
    "- [**`BaseTool`**](https://naumanhsa.github.io/neurosurfer/api-reference/tools/base-tool/) is the abstract base every tool inherits from.  \n",
    "- [**`ToolSpec`**](https://naumanhsa.github.io/neurosurfer/api-reference/tools/tool-spec/) defines the toolâ€™s metadata â€” including its name, purpose, input parameters, and return type.  \n",
    "- **`CalculatorTool`** is a simple example tool that performs arithmetic operations with validation and descriptive feedback.  \n",
    "- **`GeneralQueryAssistantTool`** wraps the LLM itself as a fallback tool for open-ended or natural language queries.  \n",
    "- [**`Toolkit`**](https://naumanhsa.github.io/neurosurfer/api-reference/tools/toolkit/) bundles multiple tools together, acting as a registry for the agent.  \n",
    "\n",
    "This agent combines structured tool-calling with LLM reasoning, letting Neurosurfer act as a decision-making orchestrator across multiple specialized tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79eb3c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:55:34\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: calculator\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:55:34\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: general_query_assistant\n"
     ]
    }
   ],
   "source": [
    "from neurosurfer.agents.tools_router_agent import ToolsRouterAgent\n",
    "from neurosurfer.tools.toolkit import Toolkit\n",
    "\n",
    "from neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n",
    "from neurosurfer.tools.base_tool import BaseTool, ToolResponse\n",
    "from neurosurfer.tools.common.general_query_assistant import GeneralQueryAssistantTool\n",
    "\n",
    "# Simple Calculator Tool\n",
    "class CalculatorTool(BaseTool):\n",
    "    spec = ToolSpec(\n",
    "        name=\"calculator\",\n",
    "        description=\"Perform basic arithmetic operations such as addition, subtraction, multiplication, and division.\",\n",
    "        when_to_use=\"Use this tool when you need to perform basic arithmetic operations.\",\n",
    "        inputs=[\n",
    "            ToolParam(name=\"num1\", type=\"float\", description=\"The first number.\", required=True),\n",
    "            ToolParam(name=\"num2\", type=\"float\", description=\"The second number.\", required=True),\n",
    "            ToolParam(name=\"operation\", type=\"string\", description=\"The operation to perform: 'add', 'subtract', 'multiply', or 'divide'.\", required=True)\n",
    "        ],\n",
    "        returns=ToolReturn(type=\"float\", description=\"The result of the arithmetic operation.\")\n",
    "    )\n",
    "\n",
    "    def __init__(self, final_answer: bool = False):\n",
    "        self.final_answer = final_answer\n",
    "\n",
    "    def __call__(self, num1: float, num2: float, operation: str) -> ToolResponse:\n",
    "        if operation not in [\"add\", \"subtract\", \"multiply\", \"divide\"]:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                observation=\"Invalid operation. Supported operations are 'add', 'subtract', 'multiply', and 'divide'.\",\n",
    "                extras={}\n",
    "            )\n",
    "        \n",
    "        if operation == \"divide\" and num2 == 0:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                observation=\"Division by zero is not allowed.\",\n",
    "                extras={}\n",
    "            )\n",
    "        try:\n",
    "            num1 = float(num1)\n",
    "            num2 = float(num2)\n",
    "            if operation == \"add\":\n",
    "                result = num1 + num2\n",
    "            elif operation == \"subtract\":\n",
    "                result = num1 - num2\n",
    "            elif operation == \"multiply\":\n",
    "                result = num1 * num2\n",
    "            elif operation == \"divide\":\n",
    "                result = num1 / num2\n",
    "        except Exception as e:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                observation=f\"An error occurred: {str(e)}\",\n",
    "                extras={}\n",
    "            )\n",
    "        \n",
    "        return ToolResponse(\n",
    "            final_answer=self.final_answer,\n",
    "            observation=float(result),\n",
    "            extras={}\n",
    "        )\n",
    "\n",
    "toolkit = Toolkit(\n",
    "    tools=[\n",
    "        CalculatorTool(),\n",
    "        GeneralQueryAssistantTool(llm=LLM, stream=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "tools_router_agent = ToolsRouterAgent(\n",
    "    toolkit=toolkit,\n",
    "    llm=LLM,\n",
    "    verbose=True,\n",
    "    specific_instructions=\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53678341",
   "metadata": {},
   "source": [
    "### âš¡ Running the Tools Router Agent\n",
    "\n",
    "Now we can test the **ToolsRouterAgent** by giving it a natural-language instruction:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5e819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:56:10\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Using tool: calculator\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:56:10\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Raw inputs: {'num1': 20.0, 'num2': 90.0, 'operation': 'multiply'}\n",
      "1800.0"
     ]
    }
   ],
   "source": [
    "# Must use Tool Calculator\n",
    "query = \"Perform the calculation 20 * 90\"\n",
    "\n",
    "for chunk in tools_router_agent.run(query, temperature=0.7, max_new_tokens=4000):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a4816f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:57:31\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Using tool: general_query_assistant\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:57:31\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Raw inputs: {'query': 'Tell me a light-hearted joke!'}\n",
      "Why don't skeletons fight each other? They don't have the guts!None"
     ]
    }
   ],
   "source": [
    "# Must use tool for General Queries\n",
    "query = \"Tell me a light-hearted joke!\"\n",
    "\n",
    "for chunk in tools_router_agent.run(query, temperature=0.7, max_new_tokens=4000):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e01bc",
   "metadata": {},
   "source": [
    "## ğŸ§  Reasoning + Acting: The ReAct Agent\n",
    "\n",
    "In this cell, we initialize a **ReActAgent**, Neurosurferâ€™s implementation of the *Reason + Act* paradigm â€” where the LLM alternates between **thinking** (reasoning in natural language) and **acting** (invoking tools) to solve complex tasks.  \n",
    "\n",
    "Hereâ€™s what happens under the hood:\n",
    "\n",
    "- [**`toolkit`**](https://naumanhsa.github.io/neurosurfer/api-reference/tools/toolkit) provides access to available tools (like `CalculatorTool` and `GeneralQueryAssistantTool`).  \n",
    "- [**`LLM`**](https://naumanhsa.github.io/neurosurfer/api-reference/models/chat-models/base-model) handles the reasoning â€” deciding when to think vs. act.  \n",
    "- [**`ReActConfig`**](https://naumanhsa.github.io/neurosurfer/api-reference/agents/react-agent/#reactconfig) defines generation settings:\n",
    "  - `temperature`: creativity in responses.  \n",
    "  - `allow_input_pruning`: trims irrelevant context for efficiency.  \n",
    "  - `repair_with_llm`: lets the agent self-correct tool errors.  \n",
    "  - `verbose`: logs the reasoning and actions step-by-step.  \n",
    "\n",
    "When you run:\n",
    "```python\n",
    "agent.run(\"Calculate 300 - 300. Then tell me a light-hearted joke about that result.\")\n",
    "```\n",
    "\n",
    "the agent first reasons about how to approach the query â€” performs the calculation using the CalculatorTool, observes the result (0), and then calls the LLM to generate a fitting joke.\n",
    "\n",
    "This cell perfectly demonstrates how Neurosurferâ€™s ReAct framework blends logic, decision-making, and creativity into one continuous reasoning loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a748808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[ğŸ§ ] Chain of Thoughts...\n",
      "Thought: I will first calculate 300 - 300 using the calculator tool, and then use the general_query_assistant to find a light-hearted joke about the result.\n",
      "\n",
      "Action: {\n",
      "  \"tool\": \"calculator\",\n",
      "  \"inputs\": {\n",
      "    \"num1\": 300,\n",
      "    \"num2\": 300,\n",
      "    \"operation\": \"subtract\"\n",
      "  },\n",
      "  \"final_answer\": false\n",
      "}"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>ğŸ”§<span style=\"font-weight: bold\">]</span> Tool: calculator\n",
       "<span style=\"font-weight: bold\">[</span>ğŸ“¤<span style=\"font-weight: bold\">]</span> Inputs: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'num1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'operation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'subtract'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0mğŸ”§\u001b[1m]\u001b[0m Tool: calculator\n",
       "\u001b[1m[\u001b[0mğŸ“¤\u001b[1m]\u001b[0m Inputs: \u001b[1m{\u001b[0m\u001b[32m'num1'\u001b[0m: \u001b[1;36m300\u001b[0m, \u001b[32m'num2'\u001b[0m: \u001b[1;36m300\u001b[0m, \u001b[32m'operation'\u001b[0m: \u001b[32m'subtract'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Observation:</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mObservation:\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[ğŸ§ ] Chain of Thoughts...\n",
      "Thought: The result of the calculation is 0. Now, I will use the general_query_assistant to find a light-hearted joke about the result.\n",
      "\n",
      "Action: {\n",
      "  \"tool\": \"general_query_assistant\",\n",
      "  \"inputs\": {\n",
      "    \"query\": \"Tell me a light-hearted joke about the number 0.\"\n",
      "  },\n",
      "  \"final_answer\": true\n",
      "}"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>ğŸ”§<span style=\"font-weight: bold\">]</span> Tool: general_query_assistant\n",
       "<span style=\"font-weight: bold\">[</span>ğŸ“¤<span style=\"font-weight: bold\">]</span> Inputs: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Tell me a light-hearted joke about the number 0.'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0mğŸ”§\u001b[1m]\u001b[0m Tool: general_query_assistant\n",
       "\u001b[1m[\u001b[0mğŸ“¤\u001b[1m]\u001b[0m Inputs: \u001b[1m{\u001b[0m\u001b[32m'query'\u001b[0m: \u001b[32m'Tell me a light-hearted joke about the number 0.'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the number 0 get kicked out of the math class? It kept making a scene every time the teacher said \"zero!\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Observation:</span> Why did the number <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> get kicked out of the math class? It kept making a scene every time the teacher \n",
       "said <span style=\"color: #008000; text-decoration-color: #008000\">\"zero!\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mObservation:\u001b[0m Why did the number \u001b[1;36m0\u001b[0m get kicked out of the math class? It kept making a scene every time the teacher \n",
       "said \u001b[32m\"zero!\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[ğŸ§ ] Chain of Thoughts...\n",
      "Thought: The calculation result is 0, and the joke provided is appropriate. The task is complete.\n",
      "\n",
      "Final Answer: The result of 300 - 300 is 0. Here's a light-hearted joke about it: Why did the number 0 get kicked out of the math class? It kept making a scene every time the teacher said \"zero!\""
     ]
    }
   ],
   "source": [
    "from neurosurfer.agents.react import ReActAgent, ReActConfig\n",
    "\n",
    "react_agent = ReActAgent(\n",
    "    toolkit=toolkit,\n",
    "    llm=LLM,\n",
    "    specific_instructions=\"Always be concise in your answers. Break the task into steps if needed.\",\n",
    "    config=ReActConfig(\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=4096,\n",
    "        allow_input_pruning=True,\n",
    "        repair_with_llm=True,\n",
    "        skip_special_tokens=True,\n",
    "        verbose=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# print(react_agent._system_prompt())\n",
    "TASK = \"\"\"Calculate 300 - 300. Then tell me a light-hearted joke about that result.\"\"\"\n",
    "\n",
    "for chunk in react_agent.run(TASK):\n",
    "    print(chunk, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df4be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
