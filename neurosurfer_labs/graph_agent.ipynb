{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9cbd0d5",
   "metadata": {},
   "source": [
    "### ðŸ”§ Notebook Setup â€” Auto-Reload & Project Path\n",
    "\n",
    "Before we start exploring different agent types and execution modes, we need to set up\n",
    "our Jupyter environment so that it can import the Neurosurfer project cleanly and \n",
    "pick up code changes automatically.\n",
    "\n",
    "### ðŸ“Œ What this cell does\n",
    "\n",
    "1. **Enables autoreload**  \n",
    "   This makes Jupyter reload modules automatically whenever you modify the source code.  \n",
    "   No need to restart the kernel each time you edit a Python file.\n",
    "\n",
    "2. **Adds your project root to `sys.path`**  \n",
    "   Your notebook likely lives inside a subfolder such as `b/` or `notebooks/`.  \n",
    "   By moving one directory up and appending it to `sys.path`, Python can import your\n",
    "   local Neurosurfer modules as if they were installed packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ae728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# Go up one directory from `b/` to project root\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b584b75a",
   "metadata": {},
   "source": [
    "## ðŸ¤– Initializing the Local LLM (Qwen3-8B-Unsloth)\n",
    "\n",
    "This cell prepares the language model that all agents in this notebook will use.  \n",
    "We load a locally stored Qwen3-8B model, quantized in 4-bit mode for efficient GPU usage.\n",
    "\n",
    "### ðŸ”§ What happens in this step\n",
    "- **GPU memory is cleared** to avoid leftover allocations from earlier runs.\n",
    "- **Model configuration parameters** are defined (model path, max sequence length, quantization mode, stop words, verbosity).\n",
    "- **The TransformersModel wrapper is created**, which handles:\n",
    "  - GPU/CPU device selection  \n",
    "  - automatic dtype (bfloat16 on CUDA)\n",
    "  - thread-safe generation\n",
    "  - streaming support and stop-token detection\n",
    "  - integration with all Neurosurfer agents\n",
    "\n",
    "### ðŸ“Œ Why this matters\n",
    "All subsequent components â€” ReActAgent, CodeAgent, RAG workflows, and the Main Workflow â€”\n",
    "reuse this single `LLM` instance.  \n",
    "It ensures consistent generation behavior, reduced memory footprint, and faster warm-up.\n",
    "\n",
    "### âš™ï¸ Key configuration highlights\n",
    "- **model_name**: filesystem path to your local 4-bit Qwen3-8B model  \n",
    "- **max_seq_length**: extended context window for longer agent chains  \n",
    "- **load_in_4bit=True**: enables efficient inference on mid-range GPUs  \n",
    "- **enable_thinking=False**: disables structured â€œthinking modeâ€ tokens  \n",
    "- **stop_words**: early-stop hints for streaming (e.g., â€œObservation:â€)\n",
    "\n",
    "After running this cell, the model is ready for use by all agents in the tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affc4a2e-d532-4dee-8b50-cab72fd229c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                                                                  â•‘\n",
      "â•‘ â–“â–“â–“â–“â–“   â–“â–“â–“â–“                                  â–“â–“â–“                â•‘\n",
      "â•‘  â–“â–“ â–“â–“   â–“â–“  â–“â–“â–“â–“ â–“  â–“ â–“ â–“ â–“â–“â–“â–“ â–“â–“â–“ â–“  â–“ â–“ â–“  â–“   â–“â–“â–“â–“ â–“ â–“       â•‘\n",
      "â•‘  â–“â–“  â–“â–“  â–“â–“  â–“â–â–â–“ â–“  â–“ â–“â–“â– â–“  â–“ â–“â–  â–“  â–“ â–“â–“â– â–“â–“â–“  â–“â–â–â–“ â–“â–“        â•‘\n",
      "â•‘  â–“â–“   â–“â–“ â–“â–“  â–“    â–“  â–“ â–“   â–“  â–“   â–“ â–“  â–“ â–“    â–“   â–“    â–“         â•‘\n",
      "â•‘ â–“â–“â–“â–“   â–“â–“â–“â–“â–“ â–“â–“â–“â–“ â–“â–“â–“â–“ â–“   â–“â–“â–“â–“ â–“â–“â–“ â–“â–“â–“â–“ â–“    â–“   â–“â–“â–“â–“ â–“         â•‘\n",
      "â•‘ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ â•‘\n",
      "â•‘ Orchestrate Agents - RAG - SQL Tools - Multi-LLM - FastAPI Ready â•‘\n",
      "â•‘ Faster builds, clearer flows, production-first                   â•‘\n",
      "â•‘                                                                  â•‘\n",
      "â•‘ Version: 0.1.0 | Python: 3.12.12                                 â•‘\n",
      "â•‘ OS: Linux 6.17.0-7-generic (x86_64)                              â•‘\n",
      "â•‘ Torch: 2.8.0   CUDA: yes (12.8)                                  â•‘\n",
      "â•‘ MPS: no (built: False)                                           â•‘\n",
      "â•‘ Transformers: 4.56.2   SentEmb: 5.1.1                            â•‘\n",
      "â•‘ Accelerate: 1.10.1   bnb: 0.48.1                                 â•‘\n",
      "â•‘ Unsloth: 2025.10.5                                               â•‘\n",
      "â•‘                                                                  â•‘\n",
      "â•‘ Detected CUDA devices: NVIDIA GeForce RTX 5080                   â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomi/micromamba/envs/LLMs/lib/python3.12/importlib/__init__.py:90: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "#### Unsloth: `hf_xet==1.1.10` and `ipykernel>6.30.1` breaks progress bars. Disabling for now in XET.\n",
      "#### Unsloth: To re-enable progress bars, please downgrade to `ipykernel==6.30.1` or wait for a fix to\n",
      "https://github.com/huggingface/xet-core/issues/526\n",
      "========\n",
      "Switching to PyTorch attention since your Xformers is broken.\n",
      "========\n",
      "\n",
      "Unsloth: Xformers does not work in RTX 50X, Blackwell GPUs as of yet. Please build from source via\n",
      "```\n",
      "pip install ninja\n",
      "pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers\n",
      "```\n",
      "\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 19:57:43\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Initializing Transformers model.\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-12-11 19:57:43\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Model is already quantized. Ignoring load_in_4bit=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 19:57:44\u001b[0m | \u001b[96mmodeling.py:get_balanced_memory\u001b[0m | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b175d496df6c4955a4f1ce87c14cefa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 19:57:48\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Transformers model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from neurosurfer.models.chat_models.transformers import TransformersModel\n",
    "from neurosurfer import config \n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "DEFAULT_TRANSFORMERS_MODEL_PARAMS = dict({\n",
    "    \"model_name\": \"/home/nomi/workspace/Model_Weights/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"max_seq_length\": 12000,\n",
    "    \"load_in_4bit\": True,\n",
    "    \"enable_thinking\": False,  # main_gpu interpretation\n",
    "    \"verbose\": False\n",
    "})\n",
    "\n",
    "LOGGER = logging.getLogger()\n",
    "LLM = TransformersModel(\n",
    "    **DEFAULT_TRANSFORMERS_MODEL_PARAMS,\n",
    "    stop_words=[\"Observation:\"],\n",
    "    logger = logging.getLogger(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e7831",
   "metadata": {},
   "source": [
    "##### ðŸ“ Streaming a Simple LLM Response\n",
    "\n",
    "This cell demonstrates how to stream tokens directly from the model in real time.  \n",
    "We send a system prompt and user message, enable `stream=True`, and print each token as it arrives.  \n",
    "Streaming is useful for interactive agent outputs, debugging, and understanding how the model generates text step by step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16ff5798-3ed2-4b38-86cd-2498ca0e57be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't skeletons fight each other?  \n",
      "Because they don't have the *guts*! ðŸ˜„"
     ]
    }
   ],
   "source": [
    "# streaming response example\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "system_prompt = \"You are a joker.\"\n",
    "user_prompt = \"\"\"Tell me a short and light-hearted joke.\"\"\"\n",
    "\n",
    "stream_response = LLM.ask(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=user_prompt,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "md_display = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream_response:\n",
    "    chunk = chunk.choices[0].delta.content or \"\"\n",
    "    print(chunk, flush=True, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808255b3",
   "metadata": {},
   "source": [
    "## ðŸ¤– Using the Generic Agent (LLM-Only Mode)\n",
    "\n",
    "This section introduces the basic `Agent` class, which provides a lightweight wrapper around an LLM without invoking tools or complex workflows. It supports both normal (non-streaming) responses and streaming output.\n",
    "\n",
    "### ðŸ”§ What this cell demonstrates\n",
    "- Creating an `Agent` with a simple configuration (temperature, token limit, streaming defaults).\n",
    "- Sending a plain natural-language query and receiving a direct response.\n",
    "- Showing how the agent integrates tracing, routing logic, and output handling behind the scenes.\n",
    "\n",
    "### ðŸ“Œ When to use this agent\n",
    "Use the generic `Agent` whenever you want:\n",
    "- a straightforward LLM response,\n",
    "- optional structured JSON outputs,\n",
    "- optional tool routing if a Toolkit is attached,\n",
    "- lightweight experimentation without the full MainWorkflow.\n",
    "\n",
    "This provides the foundation before moving on to more advanced agents like ReAct, CodeAgent, or RAGAgent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3030e4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Response:\n",
      "\u001b[1;33mðŸ§  Thinking\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmain_agent\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m699s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m701s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmain_agent\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;32mFinal response:\u001b[0m\n",
      "AI, or Artificial Intelligence, is the simulation of human intelligence in machines that are programmed to think, learn, and perform tasks typically requiring human cognition.\n"
     ]
    }
   ],
   "source": [
    "# agent normal response\n",
    "from neurosurfer.agents import Agent, AgentConfig\n",
    "# from neurosurfer.tracing import RichTracer\n",
    "from pydantic import BaseModel\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    strict_tool_call=True,\n",
    "    return_stream_by_default=True,\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "agent = Agent(llm=LLM, config=agent_config, log_traces=True)\n",
    "\n",
    "# normal response\n",
    "print(\"Normal Response:\")\n",
    "agent_response = agent.run(user_prompt=\"What is AI (one line)?\", stream=False)\n",
    "# print(agent_response.response)\n",
    "\n",
    "# # streaming response\n",
    "# print(\"\\n\\nStreaming Response:\")\n",
    "# for c in agent.run(user_prompt=\"What are top 3 applications of AI (one line)?\").response:\n",
    "#     print(c, flush=True, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d91b3b",
   "metadata": {},
   "source": [
    "### ðŸ“¦ Structured Responses with Pydantic Models\n",
    "\n",
    "This example shows how the Agent can produce **validated, structured outputs** instead of free-form text. By providing a Pydantic schema, the Agent is prompted to return JSON that matches the required fields, and any malformed output is automatically repaired or retried.\n",
    "\n",
    "### ðŸ”§ What this cell demonstrates\n",
    "- Defining custom Pydantic models to shape the LLMâ€™s output.\n",
    "- Asking the Agent a natural-language query while enforcing a specific schema.\n",
    "- Receiving a clean `json_obj` back, already parsed and validated.\n",
    "\n",
    "### ðŸ“Œ Why structured responses matter\n",
    "Structured outputs make downstream processing far easier â€” especially for:\n",
    "- pipelines,\n",
    "- UI rendering,\n",
    "- database inserts,\n",
    "- chaining multiple agents,\n",
    "- or building tool-driven reasoning graphs.\n",
    "\n",
    "This is one of the core benefits of Neurosurferâ€™s generic Agent abstraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344ffe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-12-11 19:57:58\u001b[0m | \u001b[96magent.py:run\u001b[0m     | `output_schema` provided with `stream=True`; forcing non-streaming structured output.\n",
      "\u001b[1;33mðŸ§  Thinking\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmain_agent\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.structured_call.first_pass'\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.structured_call.first_pass'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m4.\u001b[0m\u001b[2m580s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m4.\u001b[0m\u001b[2m593s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmain_agent\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing End!\u001b[0m\n",
      "\n",
      "\n",
      "Structured Response:\n",
      "{\n",
      "  \"definition\": \"Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding.\",\n",
      "  \"history\": \"The concept of AI was first introduced in the 1950s, with the term 'artificial intelligence' coined by John McCarthy in 1956. Early developments focused on creating machines that could perform tasks requiring human-like intelligence, such as playing chess or solving mathematical problems.\",\n",
      "  \"modern_frameworks\": \"Modern frameworks for AI include TensorFlow, PyTorch, and Keras, which provide tools for building and training machine learning models.\",\n",
      "  \"applications\": [\n",
      "    {\n",
      "      \"title\": \"Healthcare\",\n",
      "      \"description\": \"AI is used in healthcare for diagnostic imaging, personalized treatment plans, and drug discovery.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Finance\",\n",
      "      \"description\": \"AI is used in finance for fraud detection, algorithmic trading, and risk management.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Customer Service\",\n",
      "      \"description\": \"AI is used in customer service through chatbots and virtual assistants to provide 24/7 support.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Structured Response examples\n",
    "class AIApplication(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "\n",
    "class AI(BaseModel):\n",
    "    definition: str\n",
    "    history: str\n",
    "    modern_frameworks: str\n",
    "    applications: list[AIApplication]\n",
    "\n",
    "user_query = \"What is AI and list 3 of its top application, and 3 concerns.\"\n",
    "agent_response = agent.run(user_prompt=user_query, output_schema=AI)\n",
    "\n",
    "print(\"\\nStructured Response:\")\n",
    "print(agent_response.response.json_obj)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e93c3b",
   "metadata": {},
   "source": [
    "#### ðŸ” Inspecting Agent Traces\n",
    "\n",
    "Each agent call produces a detailed trace that captures how the model processed the request.  \n",
    "By inspecting a specific trace step, you can see metadata such as timing, inputs, whether tools were used, and whether the step succeeded.\n",
    "\n",
    "**ðŸ”§ What this cell demonstrates**\n",
    "- Accessing `agent_response.traces.steps`\n",
    "- Viewing the internal details of the `agent.run` execution step\n",
    "- Understanding inputs (streaming, schema enforcement, toolkit availability)\n",
    "- Checking timing (`duration_ms`) and success state (`ok=True`)\n",
    "\n",
    "\n",
    "This becomes even more valuable once we begin chaining agents or using RAG and code-execution modes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe4f0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step_id': 2,\n",
       " 'kind': 'llm.call',\n",
       " 'label': 'agent.structured_call.first_pass',\n",
       " 'node_id': None,\n",
       " 'agent_id': 'main_agent',\n",
       " 'started_at': 1765468678.035991,\n",
       " 'duration_ms': 4579,\n",
       " 'inputs': {'schema': 'AI',\n",
       "  'system_prompt_len': 52,\n",
       "  'user_prompt_len': 61,\n",
       "  'user_prompt': 'What is AI and list 3 of its top application, and 3 concerns.',\n",
       "  'system_prompt': \"You are a precise and rule-abiding assistant.  \\nYour task is to produce only a single valid JSON object following the schema below.\\n\\nStructured Output Contract:\\n- Output only JSON â€” no markdown, code fences, or explanations.  \\n- JSON must be strictly valid (RFC 8259): use double quotes for all keys and string values.  \\n- Do not include extra keys or any text outside the JSON object.  \\n- All required fields must be present, even if empty.  \\n- Arrays must contain at least one object when applicable.  \\n- The JSON must be a single complete object (not pretty-printed, no trailing commas).  \\n- Failure to comply with this structure means your response is invalid.\\n\\nExpected JSON Structure:\\n{'$defs': {'AIApplication': {'properties': {'title': {'title': 'Title', 'type': 'string'}, 'description': {'title': 'Description', 'type': 'string'}}, 'required': ['title', 'description'], 'title': 'AIApplication', 'type': 'object'}}, 'properties': {'definition': {'title': 'Definition', 'type': 'string'}, 'history': {'title': 'History', 'type': 'string'}, 'modern_frameworks': {'title': 'Modern Frameworks', 'type': 'string'}, 'applications': {'items': {'$ref': '#/$defs/AIApplication'}, 'title': 'Applications', 'type': 'array'}}, 'required': ['definition', 'history', 'modern_frameworks', 'applications'], 'title': 'AI', 'type': 'object'}\\n\\nNow generate your response strictly following this contract.\\n\",\n",
       "  'temperature': 0.7,\n",
       "  'max_new_tokens': 4096,\n",
       "  'stream': False},\n",
       " 'outputs': {'model_response': '{\"definition\":\"Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding.\",\"history\":\"The concept of AI was first introduced in the 1950s, with the term \\'artificial intelligence\\' coined by John McCarthy in 1956. Early developments focused on creating machines that could perform tasks requiring human-like intelligence, such as playing chess or solving mathematical problems.\",\"modern_frameworks\":\"Modern frameworks for AI include TensorFlow, PyTorch, and Keras, which provide tools for building and training machine learning models.\",\"applications\":[{\"title\":\"Healthcare\",\"description\":\"AI is used in healthcare for diagnostic imaging, personalized treatment plans, and drug discovery.\"},{\"title\":\"Finance\",\"description\":\"AI is used in finance for fraud detection, algorithmic trading, and risk management.\"},{\"title\":\"Customer Service\",\"description\":\"AI is used in customer service through chatbots and virtual assistants to provide 24/7 support.\"}]}',\n",
       "  'model_response_len': 1116},\n",
       " 'meta': {},\n",
       " 'ok': True,\n",
       " 'error': None,\n",
       " 'logs': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_response.traces.steps[1].model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf48079",
   "metadata": {},
   "source": [
    "### ðŸ”§ Using Tools with the Generic Agent\n",
    "\n",
    "This section shows how to extend an Agent with custom tools.  \n",
    "We define a simple calculator tool, register it inside a Toolkit, and attach that Toolkit to the Agent.\n",
    "\n",
    "### ðŸ”§ What this cell demonstrates\n",
    "- Creating a custom tool by subclassing `BaseTool`\n",
    "- Defining tool inputs, outputs, and descriptions using `ToolSpec`\n",
    "- Registering the tool inside a `Toolkit`\n",
    "- Allowing the Agent to decide whether to answer directly or call a tool (`strict_tool_call=False`)\n",
    "- Viewing full execution traces showing the router LLM selecting the tool and executing it\n",
    "\n",
    "### ðŸ“Œ Why this is useful\n",
    "Tools allow Agents to go beyond text generation and perform real actions:\n",
    "- mathematical operations  \n",
    "- data processing  \n",
    "- API queries  \n",
    "- database lookups  \n",
    "- code execution  \n",
    "- any custom operation your application needs\n",
    "\n",
    "This example demonstrates the full tool-calling pipeline:\n",
    "1. The router LLM interprets the request.\n",
    "2. It selects the calculator tool.\n",
    "3. The tool executes and returns the result.\n",
    "4. The Agent wraps the result in a clean `ToolCallResponse`.\n",
    "\n",
    "This pattern is the foundation for building more advanced multi-tool assistants.\n",
    "\n",
    "Additionally, using `agent_response.traces`, we can see the complete execution trace for the agent call â€” including routing, the selected tool, LLM inputs/outputs, and the toolâ€™s final return value. Itâ€™s useful for debugging and understanding exactly how the agent reached its decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "440e9634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 19:58:13\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: calculator\n",
      "Agent with choice between tools and plain text:\n",
      "\u001b[1;33mðŸ§  Thinking\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmain_agent\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \u001b[1;32mINFO: Selected tool: calculator\u001b[0m\n",
      "        \u001b[1;32mINFO: Raw inputs: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'num1'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'num2'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'operation'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m'add'\u001b[0m\u001b[1;32m}\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m721s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\n",
      "        \u001b[1;32mINFO: Tool \u001b[0m\u001b[1;32m'calculator'\u001b[0m\u001b[1;32m Tool Return: \u001b[0m\u001b[1;32m2.0\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m002s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m726s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmain_agent\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing End!\u001b[0m\n",
      "\n",
      "ToolCallResponse(selected_tool='calculator', inputs={'num1': 1.0, 'num2': 1.0, 'operation': 'add'}, returns='2.0', final=False, extras={})\n"
     ]
    }
   ],
   "source": [
    "from neurosurfer.agents.agent import Agent\n",
    "from neurosurfer.tools.toolkit import Toolkit\n",
    "from neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n",
    "from neurosurfer.tools.base_tool import BaseTool, ToolResponse\n",
    "\n",
    "# Simple Calculator Tool\n",
    "class CalculatorTool(BaseTool):\n",
    "    spec = ToolSpec(\n",
    "        name=\"calculator\",\n",
    "        description=\"Perform basic arithmetic operations such as addition, subtraction, multiplication, and division.\",\n",
    "        when_to_use=\"Use this tool when you need to perform basic arithmetic operations.\",\n",
    "        inputs=[\n",
    "            ToolParam(name=\"num1\", type=\"float\", description=\"The first number.\", required=True),\n",
    "            ToolParam(name=\"num2\", type=\"float\", description=\"The second number.\", required=True),\n",
    "            ToolParam(name=\"operation\", type=\"string\", description=\"The operation to perform strictly one of ['add', 'subtract', 'multiply', 'divide'].\", required=True)\n",
    "        ],\n",
    "        returns=ToolReturn(type=\"float\", description=\"The result of the arithmetic operation.\")\n",
    "    )\n",
    "\n",
    "    def __init__(self, final_answer: bool = False):\n",
    "        self.final_answer = final_answer\n",
    "\n",
    "    def __call__(self, num1: float, num2: float, operation: str, **kwargs) -> ToolResponse:\n",
    "        if operation not in [\"add\", \"subtract\", \"multiply\", \"divide\"]:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                results=\"Invalid operation. Supported operations are 'add', 'subtract', 'multiply', and 'divide'.\",\n",
    "                extras={}\n",
    "            )\n",
    "        \n",
    "        if operation == \"divide\" and num2 == 0:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                results=\"Division by zero is not allowed.\",\n",
    "                extras={}\n",
    "            )\n",
    "        try:\n",
    "            num1 = float(num1)\n",
    "            num2 = float(num2)\n",
    "            if operation == \"add\":\n",
    "                result = num1 + num2\n",
    "            elif operation == \"subtract\":\n",
    "                result = num1 - num2\n",
    "            elif operation == \"multiply\":\n",
    "                result = num1 * num2\n",
    "            elif operation == \"divide\":\n",
    "                result = num1 / num2\n",
    "        except Exception as e:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                results=f\"An error occurred: {str(e)}\",\n",
    "                extras={}\n",
    "            )\n",
    "        \n",
    "        return ToolResponse(\n",
    "            final_answer=self.final_answer,\n",
    "            results=float(result),\n",
    "            extras={}\n",
    "        )\n",
    "\n",
    "calculator_tool = CalculatorTool()\n",
    "toolkit = Toolkit(tools=[calculator_tool])\n",
    "\n",
    "# print(\"Tool description:\")\n",
    "# print(calculator_tool.get_tool_description())\n",
    "# print()\n",
    "\n",
    "agent = Agent(llm=LLM, toolkit=toolkit)\n",
    "\n",
    "print(\"Agent with choice between tools and plain text:\")\n",
    "agent_response = agent.run(user_prompt=\"What is 1 + 1?\", strict_tool_call=False, stream=False)\n",
    "print(agent_response.response)\n",
    "\n",
    "# print(\"\\n\\nAgent with strict tool call:\")\n",
    "# agent_response = agent.run(user_prompt=\"What is one forth of a 100?\", strict_tool_call=True)\n",
    "# print(agent_response.response)\n",
    "\n",
    "# for chunk in agent_response.response.returns:\n",
    "#     print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e587eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_response.traces.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3778857",
   "metadata": {},
   "source": [
    "## ðŸ¤– ReAct Agent with Multiple Tools\n",
    "\n",
    "This section demonstrates how the ReActAgent combines *reasoning*, *tool usage*, and *final natural-language responses* in a multi-step workflow. We register both the calculator and city-info tools, configure the ReAct agent, and ask it to perform a multi-tool reasoning task.\n",
    "\n",
    "### ðŸ”§ What this cell demonstrates\n",
    "- Initializing the ReAct agent with a toolkit containing multiple tools  \n",
    "- Using the agentâ€™s reasoning loop to:\n",
    "  1. look up information via tools,\n",
    "  2. perform intermediate calculations,\n",
    "  3. then produce a final user-facing answer\n",
    "- Inspecting the agentâ€™s step-by-step reasoning through trace logs\n",
    "\n",
    "### ðŸ§­ Why the ReAct pattern matters\n",
    "ReAct (Reason + Act) provides a powerful mechanism enabling the model to:\n",
    "- think step-by-step,\n",
    "- decide which tool to use and when,\n",
    "- execute tools with structured inputs,\n",
    "- incorporate the results into its next reasoning step,\n",
    "- and finally delegate the output to the configured final-answer generator.\n",
    "\n",
    "### ðŸŒ Language + Length control\n",
    "The configuration in this example instructs the agent to:\n",
    "- output the final answer in **Urdu**,  \n",
    "- using a **detailed** explanation style.  \n",
    "\n",
    "This showcases how ReAct can seamlessly combine:\n",
    "tool execution â†’ internal reasoning â†’ controlled natural-language output.\n",
    "\n",
    "### ðŸ“Œ What youâ€™ll see in the output\n",
    "The trace log shows:\n",
    "- the agent thinking about the task,\n",
    "- selecting the correct tool,\n",
    "- performing the arithmetic with the calculator,\n",
    "- switching into final-answer mode,\n",
    "- and producing a polished, human-readable result in Urdu.\n",
    "\n",
    "This example completes the picture of a full ReAct loop with multi-tool support and controlled final messaging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14e4e50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 19:58:20\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: calculator\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 19:58:20\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: city_info\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from typing import Dict, Any\n",
    "from neurosurfer.tools import BaseTool, ToolSpec, ToolParam, ToolReturn, ToolResponse\n",
    "\n",
    "# CityInfoTool\n",
    "class CityInfoTool(BaseTool):\n",
    "    \"\"\"\n",
    "    Simple read-only city info DB so the ReAct agent has something non-trivial to reason about.\n",
    "    \"\"\"\n",
    "    spec = ToolSpec(\n",
    "        name=\"city_info\",\n",
    "        description=\"Look up basic information about a city (population and timezone).\",\n",
    "        when_to_use=\"Use this tool when you need factual info about a city such as population or timezone.\",\n",
    "        inputs=[\n",
    "            ToolParam(name=\"city\", type=\"string\", description=\"City name, e.g. 'Paris', 'Tokyo'.\", required=True),\n",
    "        ],\n",
    "        returns=ToolReturn(type=\"string\", description=\"A short JSON-formatted string with fields like population_millions and timezone.\"),\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        # tiny in-memory DB\n",
    "        self._db = {\n",
    "            \"paris\": {\n",
    "                \"population_millions\": 2.1,\n",
    "                \"timezone\": \"Europe/Paris\",\n",
    "            },\n",
    "            \"tokyo\": {\n",
    "                \"population_millions\": 13.9,\n",
    "                \"timezone\": \"Asia/Tokyo\",\n",
    "            },\n",
    "            \"dubai\": {\n",
    "                \"population_millions\": 3.6,\n",
    "                \"timezone\": \"Asia/Dubai\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def __call__(self, city: str, **kwargs: Dict[str, Any]) -> ToolResponse:\n",
    "        city_key = city.strip().lower()\n",
    "        info = self._db.get(city_key)\n",
    "        if not info:\n",
    "            result = f'{{\"city\": \"{city}\", \"error\": \"unknown city\"}}'\n",
    "        else:\n",
    "            result = json.dumps({\"city\": city_key, **info})\n",
    "\n",
    "        return ToolResponse(\n",
    "            results=result,\n",
    "            final_answer=False,  # ReAct can decide what to do next\n",
    "            extras={},          # you can stash anything here into memory if you want\n",
    "        )\n",
    "\n",
    "\n",
    "calculator_tool = CalculatorTool()\n",
    "city_info_tool = CityInfoTool()\n",
    "\n",
    "toolkit = Toolkit(tools=[calculator_tool, city_info_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5c099e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mðŸ§  Thinking\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.react_agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.run'\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.reason_llm_call'\u001b[0m\n",
      "        \u001b[3;90mThought: \u001b[0m\u001b[3;90mI \u001b[0m\u001b[3;90mneed \u001b[0m\u001b[3;90mto \u001b[0m\u001b[3;90mlook \u001b[0m\u001b[3;90mup \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mpopulation \u001b[0m\u001b[3;90mof \u001b[0m\u001b[3;90mParis \u001b[0m\u001b[3;90mand \u001b[0m\u001b[3;90mTokyo \u001b[0m\u001b[3;90mfirst. \u001b[0m\u001b[3;90mI'll \u001b[0m\u001b[3;90mstart \u001b[0m\u001b[3;90mby \u001b[0m\u001b[3;90mretrieving \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mpopulation \u001b[0m\u001b[3;90minformation \u001b[0m\u001b[3;90mfor \u001b[0m\u001b[3;90mParis.\u001b[0m\n",
      "        \u001b[3;90mAction: \u001b[0m\u001b[1;3;90m{\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"tool\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[3;90m\"city_info\"\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"inputs\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[1;3;90m{\u001b[0m\n",
      "        \u001b[3;90m   \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"city\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[3;90m\"Paris\"\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[1;3;90m}\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"memory_keys\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[1;3;90m[\u001b[0m\u001b[1;3;90m]\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"final_answer\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[3;90mfalse\u001b[0m\n",
      "        \u001b[1;3;90m}\u001b[0m        \n",
      "        \u001b[1;32m[\u001b[0m\u001b[1;32mðŸ”§\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Tool Selected: city_info\u001b[0m\n",
      "\u001b[2m         â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.tool_execute'\u001b[0m\n",
      "            \u001b[1;32mINFO: \u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32mðŸ”§\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Executing Tool: city_info, Attempt: \u001b[0m\u001b[1;32m0\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32mðŸ“¤\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Inputs: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'city'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m'Paris'\u001b[0m\u001b[1;32m}\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "            \n",
      "            \u001b[1;32m[\u001b[0m\u001b[1;32mðŸ”§\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Tool Result: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m\"city\"\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m\"paris\"\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m\"population_millions\"\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m2.1\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m\"timezone\"\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m\"Europe/Paris\"\u001b[0m\u001b[1;32m}\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "\u001b[2m         â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.tool_execute'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m004s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "        \n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.reason_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m1.\u001b[0m\u001b[2m825s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m4\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.reason_llm_call'\u001b[0m\n",
      "        \u001b[3;90mThought: \u001b[0m\u001b[3;90mI \u001b[0m\u001b[3;90mneed \u001b[0m\u001b[3;90mto \u001b[0m\u001b[3;90mlook \u001b[0m\u001b[3;90mup \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mpopulation \u001b[0m\u001b[3;90mof \u001b[0m\u001b[3;90mTokyo \u001b[0m\u001b[3;90mnext. \u001b[0m\u001b[3;90mI'll \u001b[0m\u001b[3;90mretrieve \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mpopulation \u001b[0m\u001b[3;90minformation \u001b[0m\u001b[3;90mfor \u001b[0m\u001b[3;90mTokyo.\u001b[0m\n",
      "        \u001b[3;90mAction: \u001b[0m\u001b[1;3;90m{\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"tool\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[3;90m\"city_info\"\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"inputs\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[1;3;90m{\u001b[0m\n",
      "        \u001b[3;90m   \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"city\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[3;90m\"Tokyo\"\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[1;3;90m}\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"memory_keys\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[1;3;90m[\u001b[0m\u001b[1;3;90m]\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"final_answer\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[3;90mfalse\u001b[0m\n",
      "        \u001b[1;3;90m}\u001b[0m        \n",
      "        \u001b[1;32m[\u001b[0m\u001b[1;32mðŸ”§\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Tool Selected: city_info\u001b[0m\n",
      "\u001b[2m         â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m5\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.tool_execute'\u001b[0m\n",
      "            \u001b[1;32mINFO: \u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32mðŸ”§\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Executing Tool: city_info, Attempt: \u001b[0m\u001b[1;32m0\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32mðŸ“¤\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Inputs: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'city'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m'Tokyo'\u001b[0m\u001b[1;32m}\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "            \n",
      "            \u001b[1;32m[\u001b[0m\u001b[1;32mðŸ”§\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Tool Result: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m\"city\"\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m\"tokyo\"\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m\"population_millions\"\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m13.9\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m\"timezone\"\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m\"Asia/Tokyo\"\u001b[0m\u001b[1;32m}\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "\u001b[2m         â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m5\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.tool_execute'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m005s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "        \n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m4\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.reason_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m1.\u001b[0m\u001b[2m777s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m6\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.reason_llm_call'\u001b[0m\n",
      "        \u001b[3;90mThought: \u001b[0m\u001b[3;90mI \u001b[0m\u001b[3;90mnow \u001b[0m\u001b[3;90mhave \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mpopulation \u001b[0m\u001b[3;90mdata \u001b[0m\u001b[3;90mfor \u001b[0m\u001b[3;90mboth \u001b[0m\u001b[3;90mParis \u001b[0m\u001b[3;90mand \u001b[0m\u001b[3;90mTokyo. \u001b[0m\u001b[3;90mThe \u001b[0m\u001b[3;90mnext \u001b[0m\u001b[3;90mstep \u001b[0m\u001b[3;90mis \u001b[0m\u001b[3;90mto \u001b[0m\u001b[3;90mcalculate \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mdifference \u001b[0m\u001b[3;90min \u001b[0m\u001b[3;90mpopulation \u001b[0m\u001b[3;90mbetween \u001b[0m\u001b[3;90mTokyo \u001b[0m\u001b[3;90mand \u001b[0m\u001b[3;90mParis \u001b[0m\u001b[3;90mto \u001b[0m\u001b[3;90mdetermine \u001b[0m\u001b[3;90mhow \u001b[0m\u001b[3;90mmany \u001b[0m\u001b[3;90mmillions \u001b[0m\u001b[3;90mmore \u001b[0m\u001b[3;90mpeople \u001b[0m\u001b[3;90mlive \u001b[0m\u001b[3;90min \u001b[0m\u001b[3;90mTokyo \u001b[0m\u001b[3;90mthan \u001b[0m\u001b[3;90min \u001b[0m\u001b[3;90mParis.\u001b[0m\n",
      "        \u001b[3;90mAction: \u001b[0m\u001b[1;3;90m{\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"tool\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[3;90m\"calculator\"\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"inputs\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[1;3;90m{\u001b[0m\n",
      "        \u001b[3;90m   \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"num1\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[1;3;90m13.9\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m   \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"num2\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[1;3;90m2.1\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m   \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"operation\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[3;90m\"subtract\"\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[1;3;90m}\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"memory_keys\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[1;3;90m[\u001b[0m\u001b[1;3;90m]\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"final_answer\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[3;90mfalse\u001b[0m\n",
      "        \u001b[1;3;90m}\u001b[0m        \n",
      "        \u001b[1;32m[\u001b[0m\u001b[1;32mðŸ”§\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Tool Selected: calculator\u001b[0m\n",
      "\u001b[2m         â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m7\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.tool_execute'\u001b[0m\n",
      "            \u001b[1;32mINFO: \u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32mðŸ”§\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Executing Tool: calculator, Attempt: \u001b[0m\u001b[1;32m0\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32mðŸ“¤\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Inputs: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'num1'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m13.9\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'num2'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m2.1\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'operation'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m'subtract'\u001b[0m\u001b[1;32m}\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "            \n",
      "            \u001b[1;32m[\u001b[0m\u001b[1;32mðŸ”§\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Tool Result: \u001b[0m\u001b[1;32m11.8\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "\u001b[2m         â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m7\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.tool_execute'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m004s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "        \n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m6\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.reason_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m2.\u001b[0m\u001b[2m691s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m8\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.reason_llm_call'\u001b[0m\n",
      "        \u001b[3;90mThought: \u001b[0m\u001b[3;90mI \u001b[0m\u001b[3;90mnow \u001b[0m\u001b[3;90mhave \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mpopulation \u001b[0m\u001b[3;90mdifference \u001b[0m\u001b[3;90mbetween \u001b[0m\u001b[3;90mTokyo \u001b[0m\u001b[3;90mand \u001b[0m\u001b[3;90mParis. \u001b[0m\u001b[3;90mThe \u001b[0m\u001b[3;90mnext \u001b[0m\u001b[3;90mstep \u001b[0m\u001b[3;90mis \u001b[0m\u001b[3;90mto \u001b[0m\u001b[3;90mprovide \u001b[0m\u001b[3;90ma \u001b[0m\u001b[3;90mfinal \u001b[0m\u001b[3;90mrecommendation \u001b[0m\u001b[3;90mbased \u001b[0m\u001b[3;90mon \u001b[0m\u001b[3;90mthis \u001b[0m\u001b[3;90mnumber.\u001b[0m\n",
      "        \u001b[3;90mAction: \u001b[0m\u001b[3;90mNone\u001b[0m        \u001b[1;32mINFO: Generating final answer with \u001b[0m\u001b[1;32mlanguage\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;3;32mNone\u001b[0m\u001b[1;32m \u001b[0m\u001b[1;32mlength\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;3;32mNone\u001b[0m\u001b[1;32m \u001b[0m\u001b[1;32mhistory_len\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m7\u001b[0m\n",
      "\n",
      "        \u001b[1;32mFinal Response:\u001b[0m\n",
      "        \u001b[1;37mÙ¹ÙˆÚ©ÛŒÙˆ \u001b[0m\u001b[1;37mÙ…ÛŒÚº \u001b[0m\u001b[1;37m13.9\u001b[0m\u001b[1;37m \u001b[0m\u001b[1;37mÙ…Ù„ÛŒÙ† \u001b[0m\u001b[1;37mØ§ÙˆØ± \u001b[0m\u001b[1;37mÙ¾Ø§Ø±Ø³ \u001b[0m\u001b[1;37mÙ…ÛŒÚº \u001b[0m\u001b[1;37m2.1\u001b[0m\u001b[1;37m \u001b[0m\u001b[1;37mÙ…Ù„ÛŒÙ† \u001b[0m\u001b[1;37mÙ„ÙˆÚ¯ \u001b[0m\u001b[1;37mØ±ÛØªÛ’ \u001b[0m\u001b[1;37mÛÛŒÚºÛ” \u001b[0m\u001b[1;37mÙ¹ÙˆÚ©ÛŒÙˆ \u001b[0m\u001b[1;37mÙ…ÛŒÚº \u001b[0m\u001b[1;37mÙ¾Ø§Ø±Ø³ \u001b[0m\u001b[1;37mØ³Û’ \u001b[0m\u001b[1;37m11.8\u001b[0m\u001b[1;37m \u001b[0m\u001b[1;37mÙ…Ù„ÛŒÙ† \u001b[0m\u001b[1;37mØ²ÛŒØ§Ø¯Û \u001b[0m\u001b[1;37mÙ„ÙˆÚ¯ \u001b[0m\u001b[1;37mØ±ÛØªÛ’ \u001b[0m\u001b[1;37mÛÛŒÚºÛ” \u001b[0m\u001b[1;37mØ§Ø³ \u001b[0m\u001b[1;37mÙ†ØªÛŒØ¬Û’ \u001b[0m\u001b[1;37mÚ©Û’ \u001b[0m\u001b[1;37mÙ¾ÛŒØ´ \u001b[0m\u001b[1;37mÙ†Ø¸Ø±ØŒ \u001b[0m\u001b[1;37mÙ¹ÙˆÚ©ÛŒÙˆ \u001b[0m\u001b[1;37mÚ©ÛŒ \u001b[0m\u001b[1;37mØ¢Ø¨Ø§Ø¯ÛŒ \u001b[0m\u001b[1;37mÙ…ÛŒÚº \u001b[0m\u001b[1;37mØ¨ÛØª \u001b[0m\u001b[1;37mØ²ÛŒØ§Ø¯Û \u001b[0m\u001b[1;37mÙØ±Ù‚ \u001b[0m\u001b[1;37mÛÛ’ØŒ \u001b[0m\u001b[1;37mØ§Ø³ \u001b[0m\u001b[1;37mÙ„ÛŒÛ’ \u001b[0m\u001b[1;37mØ§Ú¯Ø± \u001b[0m\u001b[1;37mØ¢Ù¾ \u001b[0m\u001b[1;37mÚ©ÛŒ \u001b[0m\u001b[1;37mØ§ØµÙ„ \u001b[0m\u001b[1;37mØªØ±Ø¬ÛŒØ­ \u001b[0m\u001b[1;37mØ¨Ú‘ÛŒ \u001b[0m\u001b[1;37mØ¢Ø¨Ø§Ø¯ÛŒ \u001b[0m\u001b[1;37mÙˆØ§Ù„Û’ \u001b[0m\u001b[1;37mØ´ÛØ± \u001b[0m\u001b[1;37mÚ©Ø§ \u001b[0m\u001b[1;37mØªØ¬Ø±Ø¨Û \u001b[0m\u001b[1;37mÚ©Ø±Ù†Ø§ \u001b[0m\u001b[1;37mÛÛ’ \u001b[0m\u001b[1;37mØªÙˆ \u001b[0m\u001b[1;37mÙ¹ÙˆÚ©ÛŒÙˆ \u001b[0m\u001b[1;37mØ§Ù†ØªØ®Ø§Ø¨ \u001b[0m\u001b[1;37mÚ©Ø±ÛŒÚºÛ” \u001b[0m\u001b[1;37mØ§Ú¯Ø± \u001b[0m\u001b[1;37mØ¢Ù¾ \u001b[0m\u001b[1;37mÚ©Ùˆ \u001b[0m\u001b[1;37mØ§ÛŒÚ© \u001b[0m\u001b[1;37mÚ©Ù… \u001b[0m\u001b[1;37mØ¢Ø¨Ø§Ø¯ÛŒ \u001b[0m\u001b[1;37mÙˆØ§Ù„Û’ \u001b[0m\u001b[1;37mØ´ÛØ± \u001b[0m\u001b[1;37mÚ©Ø§ \u001b[0m\u001b[1;37mØ³ÙØ± \u001b[0m\u001b[1;37mÙ¾Ø³Ù†Ø¯ \u001b[0m\u001b[1;37mÛÛ’ \u001b[0m\u001b[1;37mØªÙˆ \u001b[0m\u001b[1;37mÙ¾Ø§Ø±Ø³ \u001b[0m\u001b[1;37mÚ©Ø§ \u001b[0m\u001b[1;37mØ§Ù†ØªØ®Ø§Ø¨ \u001b[0m\u001b[1;37mÚ©Ø±ÛŒÚºÛ”\u001b[0m        \n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m8\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.reason_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m6.\u001b[0m\u001b[2m239s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.react_agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent_main'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m12.\u001b[0m\u001b[2m536s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\n",
      "\u001b[1;36m Tracing End!\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from neurosurfer.agents.react.agent import ReActAgent, ReActConfig\n",
    "from neurosurfer.models.chat_models.base import BaseChatModel\n",
    "\n",
    "\n",
    "react_config = ReActConfig(\n",
    "    mode=\"delegate_final\",\n",
    "    skip_special_tokens=True,\n",
    "    max_new_tokens=4096,\n",
    "    temperature=0.7,\n",
    "    log_internal_thoughts=True,\n",
    "    final_answer_language=\"Urdu\",\n",
    "    final_answer_length=\"detailed\",\n",
    ")\n",
    "\n",
    "react_agent = ReActAgent(\n",
    "    id=\"react_agent_main\",\n",
    "    llm=LLM,\n",
    "    toolkit=toolkit,\n",
    "    config=react_config,\n",
    "    log_traces=True,\n",
    ")\n",
    "\n",
    "query = (\n",
    "    \"I'm deciding between visiting Paris or Tokyo. \"\n",
    "    \"Use your tools to: \"\n",
    "    \"1) look up the population of each city in millions, \"\n",
    "    \"2) compute how many millions more people live in Tokyo than in Paris, \"\n",
    "    \"3) then give me a final recommendation in natural language using that number.\"\n",
    ")\n",
    "\n",
    "res = react_agent.run(query=query, stream=False)\n",
    "\n",
    "\n",
    "# # For streaming Response\n",
    "# res = react_agent.run(query=query, stream=True)\n",
    "# for chunk in res.response: \n",
    "#     # print(chunk, end=\"\", flush=True)\n",
    "#     pass\n",
    "\n",
    "# traces\n",
    "# trace_dict = res.traces.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5383323",
   "metadata": {},
   "source": [
    "## ðŸ§ª CodeAgent â€” Python Execution via ReAct\n",
    "\n",
    "This example shows how the `CodeAgent` can analyze a user request, plan the required steps, and then execute real Python code through the built-in `python_execute` tool. The task here is to generate synthetic Lorenz-attractor data and produce a 3D plot.\n",
    "\n",
    "### ðŸ”§ What this cell demonstrates\n",
    "- Initializing the `CodeAgent` with analysis-first mode (`analysis_only`)\n",
    "- Allowing the agent to think step-by-step before calling the Python execution tool\n",
    "- Automatically generating Python code, running it, and retrieving the results\n",
    "- Viewing the generated plot file in the working directory\n",
    "- Inspecting detailed traces showing:\n",
    "  - reasoning thoughts,\n",
    "  - tool selection,\n",
    "  - executed Python code,\n",
    "  - generated artifacts,\n",
    "  - and the final explanatory output\n",
    "\n",
    "### ðŸ§­ Why the CodeAgent is useful\n",
    "The CodeAgent is designed for tasks that require:\n",
    "- data analysis,\n",
    "- mathematical simulations,\n",
    "- plotting and visualization,\n",
    "- execution of multi-step logic,\n",
    "- or any workflow where Python code is needed.\n",
    "\n",
    "It combines ReAct reasoning with safe, sandboxed code execution, making it ideal for scientific, analytical, or exploratory notebooks.\n",
    "\n",
    "### ðŸ“Œ Outcome\n",
    "You will see:\n",
    "- the generated Python code,\n",
    "- the saved `lorenz_attractor_3d.png` plot,\n",
    "- and a final descriptive summary of what the code produced.\n",
    "\n",
    "This marks the transition from pure LLM reasoning into full programmatic capabilities within the agent framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf4eb609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Agent Cofig:\n",
      "CodeAgentConfig(mode='analysis_only', temperature=0.7, max_new_tokens=4096, allow_input_pruning=True, repair_with_llm=True, retry=RetryPolicy(max_parse_retries=2, max_tool_errors=2, backoff_sec=0.8), skip_special_tokens=False, return_stream_by_default=False, log_internal_thoughts=True, return_internal_thoughts=False, final_answer_language='english', final_answer_length='detailed', final_answer_max_history_chars=12000, forced_memory_keys={'python_execute': ['python_last_result_summary', 'python_last_error']}, agent_name='CodeAgent', enable_post_processing=False, default_workdir='.', encourage_multistep_planning=True, default_return_raw=False)\n",
      "\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 19:58:41\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: python_execute\n",
      "\u001b[1;33mðŸ§  Thinking\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mCodeAgent\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.react_agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'CodeAgent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.run'\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'CodeAgent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.reason_llm_call'\u001b[0m\n",
      "        \u001b[3;90mThought: \u001b[0m\u001b[3;90mI \u001b[0m\u001b[3;90mneed \u001b[0m\u001b[3;90mto \u001b[0m\u001b[3;90mgenerate \u001b[0m\u001b[3;90msynthetic \u001b[0m\u001b[3;90mdata \u001b[0m\u001b[3;90mfor \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mLorenz \u001b[0m\u001b[3;90mattractor \u001b[0m\u001b[3;90musing \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mgiven \u001b[0m\u001b[3;90mparameters \u001b[0m\u001b[1;3;90m(\u001b[0m\u001b[3;90mÏƒ\u001b[0m\u001b[3;90m=\u001b[0m\u001b[1;3;90m10\u001b[0m\u001b[3;90m, \u001b[0m\u001b[3;90mÏ\u001b[0m\u001b[3;90m=\u001b[0m\u001b[1;3;90m28\u001b[0m\u001b[3;90m, \u001b[0m\u001b[3;90mÎ²\u001b[0m\u001b[3;90m=\u001b[0m\u001b[1;3;90m8\u001b[0m\u001b[3;90m/\u001b[0m\u001b[1;3;90m3\u001b[0m\u001b[1;3;90m)\u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90mand \u001b[0m\u001b[3;90mthen \u001b[0m\u001b[3;90mcreate \u001b[0m\u001b[3;90ma \u001b[0m\u001b[3;90m3D \u001b[0m\u001b[3;90mplot \u001b[0m\u001b[3;90mof \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mresults. \u001b[0m\u001b[3;90mI \u001b[0m\u001b[3;90mwill \u001b[0m\u001b[3;90mfirst \u001b[0m\u001b[3;90mwrite \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mPython \u001b[0m\u001b[3;90mcode \u001b[0m\u001b[3;90mto \u001b[0m\u001b[3;90mgenerate \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mdata \u001b[0m\u001b[3;90mand \u001b[0m\u001b[3;90mthen \u001b[0m\u001b[3;90mplot \u001b[0m\u001b[3;90mit.\u001b[0m\n",
      "        \n",
      "        \u001b[3;90mAction: \u001b[0m\u001b[1;3;90m{\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"tool\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[3;90m\"python_execute\"\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"inputs\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[1;3;90m{\u001b[0m\n",
      "        \u001b[3;90m   \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"task\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[3;90m\"Generate \u001b[0m\u001b[3;90msynthetic \u001b[0m\u001b[3;90mdata \u001b[0m\u001b[3;90mfor \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mLorenz \u001b[0m\u001b[3;90mattractor \u001b[0m\u001b[3;90mwith \u001b[0m\u001b[3;90mparameters \u001b[0m\u001b[3;90mÏƒ\u001b[0m\u001b[3;90m=\u001b[0m\u001b[1;3;90m10\u001b[0m\u001b[3;90m, \u001b[0m\u001b[3;90mÏ\u001b[0m\u001b[3;90m=\u001b[0m\u001b[1;3;90m28\u001b[0m\u001b[3;90m, \u001b[0m\u001b[3;90mÎ²\u001b[0m\u001b[3;90m=\u001b[0m\u001b[1;3;90m8\u001b[0m\u001b[3;90m/\u001b[0m\u001b[1;3;90m3\u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90mand \u001b[0m\u001b[3;90mplot \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90m3D \u001b[0m\u001b[3;90mtrajectory.\"\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[1;3;90m}\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"memory_keys\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[1;3;90m[\u001b[0m\u001b[1;3;90m]\u001b[0m\u001b[3;90m,\u001b[0m\n",
      "        \u001b[3;90m \u001b[0m\u001b[3;90m \u001b[0m\u001b[3;90m\"final_answer\"\u001b[0m\u001b[3;90m: \u001b[0m\u001b[3;90mfalse\u001b[0m\n",
      "        \u001b[1;3;90m}\u001b[0m        \n",
      "        \u001b[1;32m[\u001b[0m\u001b[1;32mðŸ”§\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Tool Selected: python_execute\u001b[0m\n",
      "\u001b[2m         â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'CodeAgent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.tool_execute'\u001b[0m\n",
      "            \u001b[1;32mINFO: \u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32mðŸ”§\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Executing Tool: python_execute, Attempt: \u001b[0m\u001b[1;32m0\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32mðŸ“¤\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Inputs: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'task'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m'Generate synthetic data for the Lorenz attractor with parameters \u001b[0m\u001b[1;32mÏƒ\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m10\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32mÏ\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m28\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32mÎ²\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m8\u001b[0m\u001b[1;32m/3 and plot the 3D trajectory.'\u001b[0m\u001b[1;32m}\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "            \n",
      "            \u001b[1;32m[\u001b[0m\u001b[1;32mðŸ”§\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m Tool Result: \u001b[0m\n",
      "            \u001b[1;32m<\u001b[0m\u001b[1;32m__code__\u001b[0m\u001b[1;32m>\u001b[0m\n",
      "            \u001b[1;32m```python\u001b[0m\n",
      "            \u001b[1;32mimport numpy as np\u001b[0m\n",
      "            \u001b[1;32mimport matplotlib.pyplot as plt\u001b[0m\n",
      "            \u001b[1;32mfrom mpl_toolkits.mplot3d import Axes3D\u001b[0m\n",
      "            \n",
      "            \u001b[1;32m# Lorenz attractor parameters\u001b[0m\n",
      "            \u001b[1;32msigma = \u001b[0m\u001b[1;32m10\u001b[0m\n",
      "            \u001b[1;32mrho = \u001b[0m\u001b[1;32m28\u001b[0m\n",
      "            \u001b[1;32mbeta = \u001b[0m\u001b[1;32m8\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m3\u001b[0m\n",
      "            \n",
      "            \u001b[1;32m# Time settings\u001b[0m\n",
      "            \u001b[1;32mdt = \u001b[0m\u001b[1;32m0.01\u001b[0m\n",
      "            \u001b[1;32mt = \u001b[0m\u001b[1;32mnp.arange\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m0\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m100\u001b[0m\u001b[1;32m, dt\u001b[0m\u001b[1;32m)\u001b[0m\n",
      "            \n",
      "            \u001b[1;32m# Initial conditions\u001b[0m\n",
      "            \u001b[1;32mx0, y0, z0 = \u001b[0m\u001b[1;32m0.1\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m0.1\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m0.1\u001b[0m\n",
      "            \n",
      "            \u001b[1;32m# Generate synthetic data\u001b[0m\n",
      "            \u001b[1;32mx, y, z = \u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m, \u001b[0m\n",
      "            \u001b[1;32mfor ti in t\u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m:\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m:\u001b[0m\n",
      "            \u001b[1;32m    x_next = x\u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m-1\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m + sigma * \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32my\u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m-1\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m - x\u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m-1\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m * dt\u001b[0m\n",
      "            \u001b[1;32m    y_next = y\u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m-1\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m + \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mx\u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m-1\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m * rho - z\u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m-1\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m * y\u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m-1\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m * dt\u001b[0m\n",
      "            \u001b[1;32m    z_next = z\u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m-1\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m + \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mx\u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m-1\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m * y\u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m-1\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m - beta * z\u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m-1\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m * dt\u001b[0m\n",
      "            \u001b[1;32m    \u001b[0m\u001b[1;32mx.append\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mx_next\u001b[0m\u001b[1;32m)\u001b[0m\n",
      "            \u001b[1;32m    \u001b[0m\u001b[1;32my.append\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32my_next\u001b[0m\u001b[1;32m)\u001b[0m\n",
      "            \u001b[1;32m    \u001b[0m\u001b[1;32mz.append\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mz_next\u001b[0m\u001b[1;32m)\u001b[0m\n",
      "            \n",
      "            \u001b[1;32m# Convert to numpy arrays\u001b[0m\n",
      "            \u001b[1;32mx, y, z = \u001b[0m\u001b[1;32mnp.array\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mx\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32mnp.array\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32my\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32mnp.array\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mz\u001b[0m\u001b[1;32m)\u001b[0m\n",
      "            \n",
      "            \u001b[1;32m# Plot the 3D trajectory\u001b[0m\n",
      "            \u001b[1;32mfig = \u001b[0m\u001b[1;32mplt.figure\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mfigsize\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m10\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m7\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m)\u001b[0m\n",
      "            \u001b[1;32max = \u001b[0m\u001b[1;32mfig.add_subplot\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m111\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32mprojection\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m'3d'\u001b[0m\u001b[1;32m)\u001b[0m\n",
      "            \u001b[1;32max.plot\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mx, y, z, \u001b[0m\u001b[1;32mlw\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m0\u001b[0m\u001b[1;32m.5\u001b[0m\u001b[1;32m)\u001b[0m\n",
      "            \u001b[1;32max.set_title\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m'Lorenz Attractor 3D Trajectory'\u001b[0m\u001b[1;32m)\u001b[0m\n",
      "            \u001b[1;32max.set_xlabel\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m'X Axis'\u001b[0m\u001b[1;32m)\u001b[0m\n",
      "            \u001b[1;32max.set_ylabel\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m'Y Axis'\u001b[0m\u001b[1;32m)\u001b[0m\n",
      "            \u001b[1;32max.set_zlabel\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m'Z Axis'\u001b[0m\u001b[1;32m)\u001b[0m\n",
      "            \u001b[1;32mplt.savefig\u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m'lorenz_attractor_3d.png'\u001b[0m\u001b[1;32m)\u001b[0m\n",
      "            \u001b[1;32mgenerated_plots = \u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m\"lorenz_attractor_3d.png\"\u001b[0m\u001b[1;32m]\u001b[0m\n",
      "            \u001b[1;32mresult = generated_plots\u001b[0m\n",
      "            \u001b[1;32m```\u001b[0m\n",
      "            \u001b[1;32m<\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m__code__\u001b[0m\u001b[1;32m>\u001b[0m\n",
      "            \n",
      "            \u001b[1;32mHere are the results from executing Python code for your task:\u001b[0m\n",
      "            \n",
      "            \u001b[1;32m[\u001b[0m\n",
      "            \u001b[1;32m  \u001b[0m\u001b[1;32m\"lorenz_attractor_3d.png\"\u001b[0m\n",
      "            \u001b[1;32m]\u001b[0m\n",
      "            \n",
      "            \u001b[1;32mGenerated plots \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32msaved in this session's working directory\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m:\u001b[0m\n",
      "            \u001b[1;32m- lorenz_attractor_3d.png\u001b[0m\n",
      "            \u001b[1;32m...\u001b[0m\n",
      "\u001b[2m         â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'CodeAgent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.tool_execute'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m8.\u001b[0m\u001b[2m373s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "        \n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'CodeAgent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.reason_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m11.\u001b[0m\u001b[2m828s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m4\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'CodeAgent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.reason_llm_call'\u001b[0m\n",
      "        \u001b[3;90mThought: \u001b[0m\u001b[3;90mThe \u001b[0m\u001b[3;90msynthetic \u001b[0m\u001b[3;90mdata \u001b[0m\u001b[3;90mfor \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mLorenz \u001b[0m\u001b[3;90mattractor \u001b[0m\u001b[3;90mhas \u001b[0m\u001b[3;90mbeen \u001b[0m\u001b[3;90mgenerated \u001b[0m\u001b[3;90mand \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90m3D \u001b[0m\u001b[3;90mplot \u001b[0m\u001b[3;90mhas \u001b[0m\u001b[3;90mbeen \u001b[0m\u001b[3;90mcreated \u001b[0m\u001b[3;90mand \u001b[0m\u001b[3;90msaved \u001b[0m\u001b[3;90mas \u001b[0m\u001b[3;90m\"lorenz_attractor_3d.png\"\u001b[0m\u001b[3;90m. \u001b[0m\u001b[3;90mThe \u001b[0m\u001b[3;90mtask \u001b[0m\u001b[3;90mis \u001b[0m\u001b[3;90mnow \u001b[0m\u001b[3;90mcomplete, \u001b[0m\u001b[3;90mand \u001b[0m\u001b[3;90mthe \u001b[0m\u001b[3;90mfinal \u001b[0m\u001b[3;90manswer \u001b[0m\u001b[3;90mcan \u001b[0m\u001b[3;90mbe \u001b[0m\u001b[3;90mprovided.\u001b[0m\n",
      "        \n",
      "        \u001b[3;90mAction: \u001b[0m\u001b[3;90mNone\u001b[0m\u001b[1;32m No further tool selected; returning control to parent agent.\u001b[0m\n",
      "        \n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m4\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'CodeAgent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.loop.reason_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m1.\u001b[0m\u001b[2m700s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.react_agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'CodeAgent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'react_agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m13.\u001b[0m\u001b[2m531s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mCodeAgent\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing End!\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from neurosurfer.agents.code import CodeAgent, CodeAgentConfig\n",
    "\n",
    "code_agent = CodeAgent(\n",
    "    llm=LLM,\n",
    "    config=CodeAgentConfig(\n",
    "        mode=\"analysis_only\",\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=4096,\n",
    "        log_internal_thoughts=True,\n",
    "        default_workdir=\".\",\n",
    "    ),\n",
    "    log_traces=True,\n",
    ")\n",
    "\n",
    "user_query = \"\"\"Generate a 3D plot of the Lorenz attractor using synthetic data with the classic (Ïƒ=10, Ï=28, Î²=8/3) parameters.\"\"\"\n",
    "agent_restuls = code_agent.run(query=user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89dcfc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(agent_restuls.tool_calls[0].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b1381",
   "metadata": {},
   "source": [
    "## ðŸ“š Retrieval-Augmented Generation (RAG) â€” Indexing & Querying a Document\n",
    "\n",
    "This example demonstrates how to use the RAGAgent to ingest documents, build a searchable vector store, and answer queries using retrieved context. It showcases the full RAG pipeline: file reading â†’ chunking â†’ embedding â†’ storage â†’ retrieval â†’ generation.\n",
    "\n",
    "### ðŸ”§ What this cell sets up\n",
    "- A **Chunker** to split large documents into semantically meaningful segments.\n",
    "- A **FileReader** capable of loading DOCX, PDF, text files, and more.\n",
    "- A **SentenceTransformerEmbedder** to generate vector embeddings (`e5-small-v2`).\n",
    "- A **RAGAgent** configured with:\n",
    "  - persistent vector storage,\n",
    "  - configurable retrieval depth,\n",
    "  - deduplication,\n",
    "  - multi-worker ingestion for speed.\n",
    "\n",
    "### ðŸ“ Ingesting a document\n",
    "The agent reads the target file, chunks it, computes embeddings, and stores them on disk.  \n",
    "A summary of ingestion statisticsâ€”chunk count, file size, processing timeâ€”is printed afterward.\n",
    "\n",
    "### ðŸ” Querying with RAG\n",
    "We then pose a natural-language question and enable `stream=True` to visualize the answer as itâ€™s generated.  \n",
    "Under the hood, the RAGAgent:\n",
    "1. retrieves the most relevant chunks,\n",
    "2. injects them into the model as context,\n",
    "3. produces a grounded, reference-aware answer.\n",
    "\n",
    "### ðŸ“Œ Why this matters\n",
    "RAG is essential when working with:\n",
    "- proprietary documents,\n",
    "- long technical files,\n",
    "- research papers,\n",
    "- project documentation,\n",
    "- meeting notes and knowledge bases.\n",
    "\n",
    "This example demonstrates a complete retrieval-augmented workflow integrated directly with the Neurosurfer agent framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35db287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 20:01:49\u001b[0m | \u001b[96mSentenceTransformer.py:__init__\u001b[0m | Use pytorch device_name: cuda:0\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 20:01:49\u001b[0m | \u001b[96msentence_transformer.py:__init__\u001b[0m | SentenceTransformer embedding model initialized.\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-12-11 20:01:49\u001b[0m | \u001b[96magent.py:__init__\u001b[0m | No vectorstore provided to RAGAgent, using default ChromaVectorStore. Initializing default collection `neurosurfer-rag-agent`\n",
      "[Init] ChromaVectorStore initialized with collection: neurosurfer-rag-agent\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf95ffa1433240858d45ae520ed69cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of ingestion:\n",
      "status: ok\n",
      "sources: 1\n",
      "chunks: 9\n",
      "unique_chunks: 9\n",
      "added: 9\n",
      "finished_at: 1765468909.6063807\n",
      "accepted_sources: 1\n",
      "total_docs_in_collection: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scripts/index_repo_for_rag.py\n",
    "from pathlib import Path\n",
    "from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n",
    "from neurosurfer.agents.rag.chunker import Chunker\n",
    "from neurosurfer.agents.rag.filereader import FileReader\n",
    "from neurosurfer.agents.rag import RAGAgent, RAGAgentConfig, RAGIngestorConfig\n",
    "\n",
    "chunker = Chunker()\n",
    "file_reader = FileReader()\n",
    "\n",
    "embedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\n",
    "rag_agent = RAGAgent(\n",
    "    llm=LLM,\n",
    "    embedder=embedder,\n",
    "    file_reader=file_reader,\n",
    "    chunker=chunker,\n",
    "    config=RAGAgentConfig(\n",
    "        top_k=5,\n",
    "        fixed_max_new_tokens=2048,\n",
    "        clear_collection_on_init=True,\n",
    "        persist_directory=\"/home/nomi/rag-storage\",\n",
    "    ),\n",
    "    ingestor_config=RAGIngestorConfig(\n",
    "        batch_size=64,\n",
    "        max_workers=4,\n",
    "        deduplicate=True,\n",
    "        normalize_embeddings=True,\n",
    "        default_metadata=None,\n",
    "        tmp_dir=\"/home/nomi/rag-storage\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "dir_path = \"./temp/AI Demonstration Proposal_Final_Draft.docx\"\n",
    "summary = rag_agent.ingest(sources=dir_path)\n",
    "\n",
    "print(\"\\nSummary of ingestion:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print()\n",
    "\n",
    "# retrival_results = rag_agent.retrieve(user_query=\"Explain how graph agent is initialized\", top_k=10)\n",
    "# print(\"max_new_tokens\", retrival_results.max_new_tokens)\n",
    "# print()\n",
    "# print(retrival_results.context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebe5c60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 20:01:54\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGAgent.retrieve] Retrieval plan: RetrievalPlan(mode='smart', scope='wide', answer_breadth='single_fact', top_k=20, notes=None, extra=None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74cb38ec8a841c2b7dd7bcd2dd0c56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 20:01:54\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGAgent.retrieve] Retrieved 9 documents\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 20:01:54\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGAgent.retrieve] Untrimmed context: 15423 chars\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 20:01:54\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGAgent.retrieve] Trimmed context: 15423 chars\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The MOM LLM Project is an advanced AI initiative designed to automate the documentation of meetings through the integration of multimodal AI technologies. The project combines speech recognition and natural language understanding to transcribe audio recordings and generate comprehensive Minutes of Meeting (MOMs). Here is a detailed explanation of the project:\n",
       "\n",
       "### 1. **Objective**\n",
       "The primary objective of the MOM LLM Project is to automate the documentation of meetings by leveraging AI models to transcribe audio inputs and generate structured meeting minutes. This reduces the manual effort required for meeting documentation and enhances the accuracy of meeting reports.\n",
       "\n",
       "### 2. **AI Workflow (Step-by-Step)**\n",
       "- **Input Processing**: The system accepts either raw meeting text or audio files (MP3, WAV, M4A). Audio inputs are split into 30-second chunks and converted into log-Mel spectrograms for optimal model processing.\n",
       "- **Audio Transcription**: The Whisper model performs Automatic Speech Recognition (ASR) with speaker diarization, producing timestamped transcripts with punctuation and speaker labels.\n",
       "- **Content Analysis & Generation**: The transcribed text is processed by the Llama 3.1 8B Instruct model, which executes the following tasks:\n",
       "  - Participant and role identification\n",
       "  - Discussion summarization with contextual analysis\n",
       "  - Action item extraction and assignment mapping\n",
       "  - Decision documentation with rationale generation\n",
       "- **Resource Optimization**: When system resources are limited, model quantization techniques (INT8/INT4) are applied to compress model weights, reducing memory usage from ~16GB to as low as ~2-4GB, while maintaining reliable performance for both CPU and GPU environments.\n",
       "\n",
       "### 3. **Architecture Flow**\n",
       "- **System Components**:\n",
       "  - **Input Layer**: API Gateway (FastAPI) handles file uploads, validation, and authentication.\n",
       "  - **Processing Pipeline**: Audio preprocessing, Whisper transcription, Llama inference with quantization, and structured output generation.\n",
       "  - **Storage Layer**: Temporary audio storage, transcript cache, and MOM result repository.\n",
       "  - **Response Layer**: RESTful endpoints returning structured outputs with meeting metadata and optional PDF export.\n",
       "- **Data Flow Sequence**:\n",
       "  - Client uploads audio or text input.\n",
       "  - The system validates format and size (up to 1GB).\n",
       "  - For audio input, Whisper transcribes speech into text; text input skips this stage.\n",
       "  - Llama 8B Instruct analyzes content to extract participants, summary, and actions.\n",
       "  - The system compiles results into a structured MOM format (participants, summary, key points, action items).\n",
       "  - Output is returned as structured data, optionally exported as a formatted PDF.\n",
       "\n",
       "### 4. **Model Specifications**\n",
       "- **Whisper Model**: Used for audio transcription with speaker diarization.\n",
       "- **Llama 3.1 8B Instruct**: Utilized for content analysis and structured output generation.\n",
       "- **Quantization Techniques**: Applied for resource optimization, reducing memory usage and enabling deployment on varied computational environments.\n",
       "\n",
       "### 5. **Benefits**\n",
       "- **Scalability**: The system is designed to handle varied computational environments, ensuring performance on limited hardware resources.\n",
       "- **Cost Efficiency**: The use of quantization and efficient model processing reduces computational costs.\n",
       "- **Accuracy**: The integration of advanced AI models ensures high accuracy in meeting documentation.\n",
       "- **Real-Time Intelligence**: The system provides real-time meeting intelligence and documentation, enhancing productivity and decision-making.\n",
       "\n",
       "### 6. **Demo Deliverables**\n",
       "- **AI-driven MOM generator**: Supports both text and audio inputs for meeting documentation.\n",
       "- **Transcription dashboard**: Provides diarization logs and transcription details.\n",
       "- **Configurable quantization**: Enables low-resource environments for deployment.\n",
       "- **Structured MOM export**: Offers outputs in JSON and PDF formats for easy sharing and archiving.\n",
       "\n",
       "### 7. **Summary**\n",
       "The MOM LLM Project demonstrates a powerful application of multimodal AI, combining speech recognition and language understanding to automate meeting documentation. By integrating Whisper ASR for transcription and Llama 3.1 8B Instruct for structured output generation, it offers a scalable, cost-efficient, and accurate solution for real-time meeting intelligence and enterprise documentation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# streaming response example\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "user_query = \"\"\"Explain MOM LLM Project in details.\"\"\"\n",
    "streaming_response = rag_agent.run(\n",
    "    user_query, \n",
    "    retrieval_mode=\"smart\", \n",
    "    stream=True\n",
    ")\n",
    "\n",
    "md_display = display(Markdown(\"\"), display_id=True)\n",
    "response = \"\"\n",
    "for chunk in streaming_response:\n",
    "    response += chunk.choices[0].delta.content or \"\"\n",
    "    md_display.update(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4c8c2",
   "metadata": {},
   "source": [
    "## ðŸ§© GraphAgent â€” Multi-Node Workflow with Web Search, Outlining, Drafting, and Review\n",
    "\n",
    "This section demonstrates how to use the `GraphAgent` to run a complete multi-stage writing workflow.  \n",
    "A YAML graph defines the nodes (â€œresearchâ€, â€œoutlineâ€, â€œdraftâ€, â€œreviewâ€), their tool access, dependencies, and output modes.  \n",
    "The agent then orchestrates all nodes in sequence using the same LLM, Toolkit, and tracing system.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ What this cell sets up\n",
    "- **WebSearchTool** configured with SerpAPI and optional crawling.\n",
    "- A **Toolkit** containing web search, enabling the research node to gather external information.\n",
    "- A **GraphAgent** configured with:\n",
    "  - a YAML workflow (`blog_workflow.yml`),\n",
    "  - per-node policies (temperature, max tokens, structured/text modes),\n",
    "  - a manager LLM that composes prompts and coordinates all nodes,\n",
    "  - full tracing for transparency.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§­ What happens during execution\n",
    "The workflow runs node-by-node:\n",
    "\n",
    "1. **research**  \n",
    "   - Uses the web search tool to gather key information.  \n",
    "   - Produces a structured summary exported as JSON.\n",
    "\n",
    "2. **outline**  \n",
    "   - Reads the research output.  \n",
    "   - Generates a structured blog outline in markdown.  \n",
    "   - Exported as an `.md` file.\n",
    "\n",
    "3. **draft**  \n",
    "   - Consumes both the outline and research data.  \n",
    "   - Produces a long-form article draft (~3000 words).  \n",
    "   - Exported as markdown.\n",
    "\n",
    "4. **review**  \n",
    "   - Reads the draft + research.  \n",
    "   - Produces a structured technical/editorial review.  \n",
    "   - Exported as markdown.\n",
    "\n",
    "The manager LLM composes prompts for each node, ensuring that every tool, dependency, and policy is correctly applied.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ Why GraphAgent is powerful\n",
    "- It allows you to define entire multi-step, multi-agent workflows declaratively via YAML.  \n",
    "- Each node can have different modes: free-text, structured, tool-enabled, or special policies.  \n",
    "- Tools are selectively available only to nodes that require them.  \n",
    "- Every step is traced, logged, and exportable for inspection or reuse.  \n",
    "- Output artifacts (research, outline, draft, review) are automatically saved to the `exports/` folder.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ What you see in the console\n",
    "- Detailed traces for each node  \n",
    "- Web search execution logs  \n",
    "- LLM calls and tool routing decisions  \n",
    "- Final responses for each stage  \n",
    "- Automatic export notifications\n",
    "\n",
    "This example showcases how to build full production-grade, multi-agent pipelines using Neurosurferâ€™s GraphAgent system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec7eb2",
   "metadata": {},
   "source": [
    "### YAML Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d42ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key:  f443633b...\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-12-11 20:11:44\u001b[0m | \u001b[96magent.py:__init__\u001b[0m | No vectorstore provided to RAGAgent, using default ChromaVectorStore. Initializing default collection `neurosurfer-rag-agent`\n",
      "[Init] ChromaVectorStore initialized with collection: neurosurfer-rag-agent\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 20:11:46\u001b[0m | \u001b[96mSentenceTransformer.py:__init__\u001b[0m | Use pytorch device_name: cuda:0\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 20:11:46\u001b[0m | \u001b[96msentence_transformer.py:__init__\u001b[0m | SentenceTransformer embedding model initialized.\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 20:11:46\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: web_search\n",
      "{'web_search': <neurosurfer.tools.websearch.tool.WebSearchTool object at 0x77503639f5f0>}\n"
     ]
    }
   ],
   "source": [
    "# test web search tool\n",
    "from neurosurfer.tools.websearch import WebSearchTool, WebSearchConfig\n",
    "from neurosurfer.tools.toolkit import Toolkit\n",
    "\n",
    "api_key = os.getenv(\"SERPAPI_KEY\", \"API Key not found...\")\n",
    "print(\"API Key: \", f\"{api_key[:8]}...\")\n",
    "\n",
    "web_search_tool = WebSearchTool(\n",
    "    config=WebSearchConfig(\n",
    "        engine=\"serpapi\",\n",
    "        engine_kwargs={\"api_key\": api_key},\n",
    "        max_results=3,\n",
    "        enable_crawl=True,\n",
    "        max_crawl_results=2,\n",
    "        content_words_limit=2000,\n",
    "        content_limit_strategy=\"distributive\",\n",
    "        summarize=False,\n",
    "        top_k=10,\n",
    "    ),\n",
    "    llm=LLM,\n",
    ")\n",
    "\n",
    "toolkit = Toolkit(tools=[web_search_tool])\n",
    "print(toolkit.registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf1ccb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 21:14:38\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: web_search\n",
      "\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmanager\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m2.\u001b[0m\u001b[2m441s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmanager\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;33mðŸ§  Thinking\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mresearch\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\n",
      "        \u001b[1;32mINFO: Selected tool: web_search\u001b[0m\n",
      "        \u001b[1;32mINFO: Raw inputs: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'query'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m'Stop Treating LMs as Black Boxes: The Case for Observability-First AI Systems'\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'hl'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m'en'\u001b[0m\u001b[1;32m}\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m945s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-12-11 21:14:48\u001b[0m | \u001b[96mingestor.py:ingest\u001b[0m | Some sources were skipped as unsupported: [None, None]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ee1be9bfc346c7adf4522422ed8a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 21:14:48\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGAgent.retrieve] Retrieval plan: RetrievalPlan(mode='classic', scope=None, answer_breadth=None, top_k=10, notes=None, extra=None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c769ec64da514b4c9f513849a1112d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 21:14:48\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGAgent.retrieve] Retrieved 10 documents\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 21:14:48\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGAgent.retrieve] Untrimmed context: 10397 chars\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 21:14:48\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGAgent.retrieve] Trimmed context: 10397 chars\n",
      "        \u001b[1;32mINFO: Tool \u001b[0m\u001b[1;32m'web_search'\u001b[0m\u001b[1;32m Tool Return: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'query'\u001b[0m\u001b[1;32m: 'Compose a \u001b[0m\u001b[1;32m1000\u001b[0m\u001b[1;32m-\u001b[0m\u001b[1;32m1500\u001b[0m\u001b[1;32m word blog arguing why observability layers are essential in modern LL\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m6.\u001b[0m\u001b[2m204s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m7.\u001b[0m\u001b[2m151s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mresearch\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing End!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmanager\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m3.\u001b[0m\u001b[2m151s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmanager\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;33mðŸ§  Thinking\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36moutline\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'outline'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'outline'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'outline'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m11.\u001b[0m\u001b[2m306s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'outline'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m11.\u001b[0m\u001b[2m307s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36moutline\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;32mFinal response:\u001b[0m\n",
      "**Title:**  \n",
      "*Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems*\n",
      "\n",
      "**Description:**  \n",
      "This blog post explores the critical need for observability in large language model (LLM) systems. As LLMs become more integrated into production applications, the lack of visibility into their internal workings has led to significant challenges, including hallucinations, performance bottlenecks, and compliance risks. By adopting an observability-first approach, organizations can gain real-time insights into model behavior, trace requests end-to-end, measure performance metrics, and create feedback loops for continuous improvement. This article outlines the key components of an observability-first strategy, discusses common failure patterns in LLM systems, and highlights the role of tools and best practices in building transparent, reliable, and ethical AI systems.\n",
      "\n",
      "---\n",
      "\n",
      "### **Outline of the Blog Post**\n",
      "\n",
      "#### **1. Introduction: The Rise of LLMs and the Black Box Problem**\n",
      "- **Overview of LLMs and Their Growing Role**  \n",
      "  - Explain the rapid adoption of LLMs across industries.  \n",
      "  - Highlight their complexity and the challenges of deploying them at scale.  \n",
      "\n",
      "- **The Black Box Conundrum**  \n",
      "  - Define the \"black box\" problem in LLMs.  \n",
      "  - Discuss the limitations of traditional monitoring and debugging approaches.  \n",
      "  - Introduce the concept of **observability** as a solution.  \n",
      "\n",
      "- **Why Observability Matters Now**  \n",
      "  - Link the increasing complexity of LLM systems to the need for deeper visibility.  \n",
      "  - Preview the key components of observability: tracing, metrics, feedback loops, and failure analysis.  \n",
      "\n",
      "---\n",
      "\n",
      "#### **2. Understanding Observability in LLM Systems**\n",
      "- **What is Observability?**  \n",
      "  - Define observability in the context of LLMs.  \n",
      "  - Differentiate between **observability** and **monitoring**.  \n",
      "  - Emphasize the importance of **comprehensive visibility** into model behavior.  \n",
      "\n",
      "- **The Observability Stack for LLMs**  \n",
      "  - Break down the key elements of an observability layer:  \n",
      "    - **Tracing**: Tracking requests through the system.  \n",
      "    - **Metrics**: Measuring performance and resource usage.  \n",
      "    - **Feedback Loops**: Using data to improve model behavior.  \n",
      "    - **Content Safety and Compliance**: Ensuring ethical and legal standards.  \n",
      "\n",
      "- **The Role of Observability in AI Ethics and Trust**  \n",
      "  - Discuss how observability supports transparency\n",
      "\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmanager\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m2.\u001b[0m\u001b[2m999s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmanager\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;33mðŸ§  Thinking\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mdraft\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'draft'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'draft'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'draft'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m52.\u001b[0m\u001b[2m274s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'draft'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m52.\u001b[0m\u001b[2m276s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mdraft\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;32mFinal response:\u001b[0m\n",
      "# Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems\n",
      "\n",
      "## Introduction: The Rise of LLMs and the Black Box Problem\n",
      "\n",
      "Large language models (LLMs) have rapidly transformed the landscape of artificial intelligence, becoming integral to applications across industries such as finance, healthcare, customer service, and content creation. Their ability to understand and generate human-like text has made them a cornerstone of modern AI systems. However, as these models scale in complexity and usage, a critical challenge has emerged: the **black box problem**.\n",
      "\n",
      "The black box problem refers to the lack of transparency in how LLMs process inputs, generate outputs, and make decisions. Unlike traditional machine learning models, which often have more interpretable structures, LLMs are inherently complex and opaque. This opacity makes it difficult for developers and stakeholders to understand how the model behaves, especially in production environments where reliability, safety, and compliance are paramount.\n",
      "\n",
      "Traditional monitoring and debugging approaches, which are well-suited for conventional software systems, fall short when applied to LLMs. These approaches typically focus on system-level metrics such as CPU usage, memory consumption, and response times. However, they fail to capture the internal behavior of the model itselfâ€”what the model is thinking, how it processes data, and whether it is generating safe, accurate, or ethical outputs.\n",
      "\n",
      "This is where **observability** comes in. Observability is not just about monitoring; it is about **understanding** the behavior of a system in real time. For LLMs, observability means having the ability to track requests end-to-end, measure performance across all layers, and create feedback loops to continuously improve the model's behavior.\n",
      "\n",
      "In this blog post, we will explore the concept of observability in the context of LLM systems. We will break down its key componentsâ€”tracing, metrics, feedback loops, and content safetyâ€”and discuss how they contribute to building transparent, reliable, and ethical AI systems. We will also examine common failure patterns in LLM systems and how observability can help address them.\n",
      "\n",
      "By the end of this post, you will understand why observability is not just a nice-to-haveâ€”it is essential for the future of AI.\n",
      "\n",
      "---\n",
      "\n",
      "## Understanding Observability in LLM Systems\n",
      "\n",
      "### What is Observability?\n",
      "\n",
      "Observability is the ability to understand the internal state of a system based on its external outputs. In the context of LLM systems, observability goes beyond traditional monitoring. It involves **tracking the flow of requests through the model**, **measuring performance at every layer**, and **analyzing the model's behavior in real time**.\n",
      "\n",
      "Unlike monitoring, which is reactive and focuses on predefined metrics, observability is **proactive and holistic**. It provides a **comprehensive view of the system**, enabling developers to not only detect issues but also understand why they are happening.\n",
      "\n",
      "For example, while monitoring might tell you that a model is returning incorrect answers, observability would tell you **why** the model is generating those answersâ€”whether it's due to a faulty prompt, a data bias, or a misalignment in the training data.\n",
      "\n",
      "### The Observability Stack for LLMs\n",
      "\n",
      "An effective observability strategy for LLM systems consists of several key components. These components work together to provide a complete picture of the model's behavior and performance. Let's explore each of them:\n",
      "\n",
      "#### 1. Tracing\n",
      "\n",
      "**Tracing** is the process of tracking a request through the entire system, from the initial user input to the final output. In LLM systems, tracing is especially important because it allows developers to understand how the model processes inputs, interacts with external tools, and generates outputs.\n",
      "\n",
      "For example, consider an application that uses an LLM to answer customer questions. The request might start with a user query, then pass through a database lookup, an API call, and finally the LLM itself. Tracing would allow developers to see the entire journey of the request, including any delays or errors that occurred at each step.\n",
      "\n",
      "Tools like **LangSmith** and **W&B Weave** provide end-to-end tracing capabilities, allowing developers to visualize the flow of requests and identify performance bottlenecks.\n",
      "\n",
      "#### 2. Metrics\n",
      "\n",
      "**Metrics** are quantitative measures of system performance and behavior. In the context of LLM systems, metrics can include:\n",
      "\n",
      "- **Response latency**: How long it takes the model to generate a response.\n",
      "- **Token usage**: The number of tokens processed by the model.\n",
      "- **Error rates**: The percentage of requests that result in errors.\n",
      "- **Throughput**: The number of requests handled per second.\n",
      "- **Resource usage**: CPU, memory, and GPU usage.\n",
      "\n",
      "These metrics provide valuable insights into the performance of the model and help developers identify issues such as latency spikes, resource exhaustion, or inefficient token usage.\n",
      "\n",
      "For instance, if a model starts to take longer than expected to generate responses, a developer might use metrics to determine whether the issue is due to a slow API call, a misconfigured model, or an overload of incoming requests.\n",
      "\n",
      "#### 3. Feedback Loops\n",
      "\n",
      "**Feedback loops** are mechanisms that allow the model to learn from its own behavior. In the context of LLM systems, feedback loops involve **collecting data on model performance** and using it to **improve the model over time**.\n",
      "\n",
      "For example, if a model generates incorrect answers, developers can use feedback loops to **retrain the model on the incorrect outputs**, helping it to avoid making the same mistakes in the future. This process is often referred to as **model fine-tuning** or **reinforcement learning from human feedback (RLHF)**.\n",
      "\n",
      "Tools like **W&B Weave** and **LangSmith** provide features that enable developers to collect and analyze feedback data, making it easier to iterate on model improvements.\n",
      "\n",
      "#### 4. Content Safety and Compliance\n",
      "\n",
      "As LLMs become more integrated into production applications, **content safety and compliance** have become a major concern. These systems must ensure that the model's outputs are **accurate, safe, and aligned with ethical and legal standards**.\n",
      "\n",
      "Observability plays a crucial role in addressing these concerns. By **monitoring the model's outputs in real time**, developers can detect and flag potentially harmful or unethical content. This includes identifying **hallucinations**, **bias**, **profanity**, and **inappropriate content**.\n",
      "\n",
      "For example, a healthcare application using an LLM to generate patient summaries must ensure that the model does not disclose sensitive information or make false claims. Observability tools can help detect such issues by **scanning model outputs for PII (Personally Identifiable Information)** or **violations of ethical guidelines**.\n",
      "\n",
      "In regulated industries, **observability also means maintaining an audit trail**. This ensures that if there is ever a question or incident, there is a record to review. Features like **PII redaction** and **access control** are essential for maintaining compliance in such environments.\n",
      "\n",
      "---\n",
      "\n",
      "## The Role of Observability in AI Ethics and Trust\n",
      "\n",
      "### Building Trust Through Transparency\n",
      "\n",
      "Trust is a fundamental component of any AI system. For users, developers, and stakeholders, trust is built through **transparency, reliability, and accountability**. Observability plays a crucial role in building trust by providing **visibility into the model's behavior**.\n",
      "\n",
      "When users know that an AI system is **transparent and accountable**, they are more likely to trust its outputs. This is especially important in high-stakes applications such as **healthcare, finance, and legal services**, where the consequences of incorrect outputs can be severe.\n",
      "\n",
      "Observability also helps build **trust among developers and data scientists**. By providing **real-time insights into model behavior**, observability enables developers to **debug issues more effectively** and **improve the model's performance over time**.\n",
      "\n",
      "### Ensuring Ethical AI\n",
      "\n",
      "Ethical AI is not just about avoiding harm; it's also about **ensuring that AI systems are fair, just, and aligned with societal values**. Observability supports ethical AI by providing **insights into model behavior**, **detecting biases**, and **ensuring compliance with ethical guidelines**.\n",
      "\n",
      "For example, if an LLM is used in a hiring application, it must be **free from biases that could disadvantage certain groups**. Observability tools can help detect such biases by **analyzing the model's outputs for fairness** and **flagging any discriminatory patterns**.\n",
      "\n",
      "In addition, observability helps ensure that **AI systems are aligned with ethical guidelines**. For instance, if an LLM is used to generate content for a news website, it must be **free from misinformation and bias**. Observability tools can help detect such issues by **scanning model outputs for factual accuracy** and **flagging any potential misinformation**.\n",
      "\n",
      "---\n",
      "\n",
      "## Common Failure Patterns in LLM Systems\n",
      "\n",
      "### Hallucinations\n",
      "\n",
      "One of the most common failure patterns in LLM systems is **hallucination**, where the model generates **false or made-up information**. This can be a significant issue, especially in applications where **accuracy is critical**.\n",
      "\n",
      "For example, an LLM used in a customer support application might generate **incorrect information about product features**, leading to **confusion and dissatisfaction** among users. Observability tools can help detect hallucinations by **monitoring model outputs** and **flagging any inconsistencies**.\n",
      "\n",
      "### Performance Bottlenecks\n",
      "\n",
      "As LLMs scale in complexity and usage, **performance bottlenecks** can emerge. These bottlenecks can be due to a variety of factors, including **slow API calls**, **inefficient token usage**, or **resource exhaustion**.\n",
      "\n",
      "Observability tools can help identify these bottlenecks by **tracking performance metrics** and **analyzing the flow of requests**. For example, if a model starts to take longer than expected to generate responses, a developer might use observability tools to **identify the slow API call** or **optimize token usage**.\n",
      "\n",
      "### Cost Overruns\n",
      "\n",
      "Many LLM services charge based on **token usage or compute time**, making **cost overruns** a significant concern. Without observability, it is difficult to **track and optimize costs**.\n",
      "\n",
      "Observability tools can help prevent cost overruns by **tracking token usage in real time** and **identifying any excessive usage**. For example, if a model is generating **long or unnecessary prompts**, observability tools can **flag these prompts** and **suggest optimizations**.\n",
      "\n",
      "---\n",
      "\n",
      "## The Future of Observability in AI\n",
      "\n",
      "As LLMs continue to evolve and become more integrated into production systems, **observability will play an increasingly important role** in building **transparent, reliable, and ethical AI**.\n",
      "\n",
      "Tools like **W&B Weave**, **LangSmith**, and **Lunary** are at the forefront of this movement, providing **comprehensive observability solutions** for LLM systems. These tools not only help developers **monitor and debug their models** but also enable them to **improve model performance over time**.\n",
      "\n",
      "In the future, we can expect to see **more advanced observability tools** that provide **even deeper insights into model behavior**, **automated feedback loops**, and **real-time ethical compliance checks**. These tools will be essential for **building AI systems that are not only powerful but also trustworthy**.\n",
      "\n",
      "---\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "In conclusion, **observability is not just a technical necessityâ€”it is a strategic imperative** for the future of AI. As LLMs become more complex and widely used, **the need for transparency and accountability has never been greater**.\n",
      "\n",
      "By adopting an **observability-first approach**, organizations can **gain real-time insights into model behavior**, **detect and fix issues quickly**, and **ensure that their AI systems are safe, reliable, and ethical**.\n",
      "\n",
      "In the next section, we will explore **practical examples of observability in action**, including **how to implement observability in your own LLM projects** and **best practices for building observability-first AI systems**. Stay tuned for more insights on this critical topic.\n",
      "\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmanager\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m4\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m4\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m3.\u001b[0m\u001b[2m301s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mmanager\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;33mðŸ§  Thinking\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mreview\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'review'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'review'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'review'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m31.\u001b[0m\u001b[2m051s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'review'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m31.\u001b[0m\u001b[2m052s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[1;36mreview\u001b[0m\u001b[1;36m]\u001b[0m\u001b[1;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;32mFinal response:\u001b[0m\n",
      "# Review of the Blog Post: \"Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems\"\n",
      "\n",
      "---\n",
      "\n",
      "## âœ… Strengths\n",
      "\n",
      "- **Clear structure**: The blog post is well-organized with logical sections and a clear progression from introduction to conclusion.\n",
      "- **Relevant content**: It effectively covers the core aspects of observability in LLM systems, including tracing, metrics, feedback loops, and content safety.\n",
      "- **Practical tone**: The tone is practical and slightly opinionated, as requested, making it accessible for intermediate ML engineers.\n",
      "- **Use of examples**: The post uses concrete examples (e.g., hallucinations, performance bottlenecks, cost overruns) to illustrate key points.\n",
      "- **Actionable insights**: The post provides actionable insights for developers, such as using tools like LangSmith, W&B Weave, and Lunary.\n",
      "- **Alignment with query**: The content thoroughly addresses the importance of observability in LLM systems and covers all requested components (tracing, metrics, feedback loops, failure patterns).\n",
      "\n",
      "---\n",
      "\n",
      "## ðŸš¨ Issues and Suggestions\n",
      "\n",
      "### 1. **Factual Inaccuracies and Missing Explanations**\n",
      "\n",
      "- **Tool Mentioned Without Context**: The post mentions tools like **LangSmith**, **W&B Weave**, and **Lunary**, but it doesnâ€™t explain what they are or how they are used in the context of LLM observability. This could confuse readers who are unfamiliar with these tools.  \n",
      "  **Suggestion**: Add brief descriptions of each tool and its role in LLM observability, or provide links to their documentation for further reading.\n",
      "\n",
      "- **Overgeneralization of Observability**: The post uses the term \"observability\" broadly, but it doesnâ€™t clearly differentiate between **observability**, **monitoring**, and **debugging**. This can lead to confusion.  \n",
      "  **Suggestion**: Clarify the distinction between these concepts, especially in the section on \"LLM Observability vs. LLM Monitoring.\"\n",
      "\n",
      "- **Limited Discussion on Feedback Loops**: While the post mentions feedback loops, it doesnâ€™t go into detail about how they are implemented or their impact on model behavior.  \n",
      "  **Suggestion**: Expand on the concept of feedback loops, including examples like **reinforcement learning from human feedback (RLHF)** and **active learning**.\n",
      "\n",
      "- **Ambiguity Around \"Common Failure Patterns\"**: The post lists failure patterns (e.g., hallucinations, performance bottlenecks, cost overruns) but doesnâ€™t provide enough depth on how observability helps detect or mitigate these issues.  \n",
      "  **Suggestion**: Include specific examples of how observability tools detect and address these failure patterns (e.g., how a tool identifies a hallucination and triggers a model retraining).\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Stylistic and Clarity Issues**\n",
      "\n",
      "- **Repetition of Terms**: The term \"observability\" is used repeatedly, which can make the post feel redundant.  \n",
      "  **Suggestion**: Vary the language to avoid repetition. For example, use synonyms like \"visibility,\" \"diagnostic insights,\" or \"behavioral analysis.\"\n",
      "\n",
      "- **Unclear Transition Between Sections**: The transition from the section on \"Observability Stack for LLMs\" to \"The Role of Observability in AI Ethics and Trust\" is abrupt.  \n",
      "  **Suggestion**: Add a brief paragraph or sentence that connects these sections, explaining how observability supports both technical and ethical aspects of AI.\n",
      "\n",
      "- **Lack of Visual Aids**: The post is text-heavy and lacks visual aids such as diagrams or charts that could help explain complex concepts like tracing or feedback loops.  \n",
      "  **Suggestion**: Suggest including visual aids (e.g., flowcharts of a request lifecycle, graphs of performance metrics) in future versions of the post.\n",
      "\n",
      "- **Some Sentences Are Too Long**: Some sentences are long and complex, making them difficult to parse.  \n",
      "  **Suggestion**: Break up long sentences into shorter ones for better readability. For example:\n",
      "  > \"While monitoring might tell you that a model is returning incorrect answers, observability would tell you why the model is generating those answersâ€”whether it's due to a faulty prompt, a data bias, or a misalignment in the training data.\"\n",
      "  â†’\n",
      "  > \"While monitoring might tell you that a model is returning incorrect answers, observability goes further. It tells you why the model is generating those answers. The cause could be a faulty prompt, data bias, or a misalignment in the training data.\"\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Unclarified Concepts**\n",
      "\n",
      "- **\"Feedback Loops\"**: The term is used without a clear definition. The post mentions that feedback loops involve collecting data on model performance and using it to improve the model, but it doesnâ€™t explain how this process works in practice.  \n",
      "  **Suggestion**: Define \"feedback loops\" more clearly, perhaps with an example such as:\n",
      "  > \"A feedback loop in the context of LLMs involves collecting data on model performance (e.g., incorrect outputs, bias) and using that data to retrain or fine-tune the model. This process is often referred to as reinforcement learning from human feedback (RLHF).\"\n",
      "\n",
      "- **\"Content Safety and Compliance\"**: The post touches on content safety and compliance but doesnâ€™t provide enough detail on how observability tools enforce these standards.  \n",
      "  **Suggestion**: Expand on the role of observability in content safety, including examples of how tools detect and flag problematic content (e.g., profanity, bias, hallucinations).\n",
      "\n",
      "---\n",
      "\n",
      "## âœ… Summary of Actionable Edits\n",
      "\n",
      "| Issue | Suggestion |\n",
      "|------|------------|\n",
      "| Tool descriptions are too vague | Add brief descriptions of tools like LangSmith, W&B Weave, and Lunary |\n",
      "| Overgeneralization of observability | Clarify the difference between observability, monitoring, and debugging |\n",
      "| Limited discussion on feedback loops | Expand on how feedback loops are implemented and their impact |\n",
      "| Ambiguity around failure patterns | Provide specific examples of how observability detects and mitigates failure patterns |\n",
      "| Repetition of terms | Vary the language to avoid repetition |\n",
      "| Unclear transitions between sections | Add connecting sentences between sections |\n",
      "| Lack of visual aids | Suggest including diagrams or charts for better clarity |\n",
      "| Long sentences | Break up long sentences for better readability |\n",
      "| Unclarified concepts | Define \"feedback loops\" and \"content safety\" more clearly |\n",
      "\n",
      "---\n",
      "\n",
      "## âœ… Final Assessment\n",
      "\n",
      "The blog post is well-written, informative, and aligned with the original query. It provides a clear argument for the importance of observability in LLM systems and covers the key components (tracing, metrics, feedback loops, content safety). However, there are opportunities for improvement in terms of clarity, structure, and depth of explanation. With the suggested edits, the post can be made even more effective for its target audience of intermediate ML engineers.\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 21:16:32\u001b[0m | \u001b[96mexport.py:export_single_node\u001b[0m | Exported node research output to exports/research_20251211_211441.json\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 21:16:32\u001b[0m | \u001b[96mexport.py:export_single_node\u001b[0m | Exported node outline output to exports/outline_20251211_211451.md\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 21:16:32\u001b[0m | \u001b[96mexport.py:export_single_node\u001b[0m | Exported node draft output to exports/draft_20251211_211505.md\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-12-11 21:16:32\u001b[0m | \u001b[96mexport.py:export_single_node\u001b[0m | Exported node review output to exports/review_20251211_211601.md\n"
     ]
    }
   ],
   "source": [
    "from neurosurfer.models.chat_models.base import BaseChatModel\n",
    "from neurosurfer.agents.graph import GraphAgent, ManagerConfig\n",
    "\n",
    "graph_agent = GraphAgent(\n",
    "    llm=LLM,\n",
    "    graph_yaml=\"blog_workflow.yml\",\n",
    "    toolkit=toolkit,\n",
    "    manager_config=ManagerConfig(\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=4096,\n",
    "    ),\n",
    "    manager_llm=LLM,\n",
    "    log_traces=True\n",
    ")\n",
    "\n",
    "# Run workflow\n",
    "# graph_inputs = {\n",
    "#     \"topic_title\": \"The Missing Middle Layer: Why LLM Systems Need Tool Routers, Not Bigger Models\",\n",
    "#     \"query\": \"Compose a 1000-1500 word blog on why tool-routing layers matter more than scaling LLM size, covering practical design patterns, examples, and tradeoffs.\",\n",
    "#     \"audience\": \"Intermediate ML engineers\",\n",
    "#     \"tone\": \"Practical and slightly opinionated\",\n",
    "# }\n",
    "\n",
    "graph_inputs = {\n",
    "    \"topic_title\": \"Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems\",\n",
    "    \"query\": \"Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.\",\n",
    "    \"audience\": \"Intermediate ML engineers\",\n",
    "    \"tone\": \"Practical and slightly opinionated\",\n",
    "}\n",
    "\n",
    "results = graph_agent.run(inputs=graph_inputs)\n",
    "# result = await run_async(executor.run(inputs=graph_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1205b51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'graph': {'name': 'blog_workflow',\n",
       "  'description': 'Example multi-agent workflow for writing and reviewing a technical blog using multiple specialized nodes (each node uses an Agent under the hood).\\n',\n",
       "  'inputs': [{'name': 'topic_title',\n",
       "    'type': 'string',\n",
       "    'required': True,\n",
       "    'description': None},\n",
       "   {'name': 'query', 'type': 'string', 'required': True, 'description': None},\n",
       "   {'name': 'audience',\n",
       "    'type': 'string',\n",
       "    'required': True,\n",
       "    'description': None},\n",
       "   {'name': 'tone', 'type': 'string', 'required': True, 'description': None}],\n",
       "  'nodes': [{'id': 'research',\n",
       "    'description': None,\n",
       "    'kind': 'base',\n",
       "    'purpose': 'Perform focused research on the requested topic titled {topic_title}.',\n",
       "    'goal': 'Collect key facts, terminology, and references that are directly useful for writing a technical blog post.',\n",
       "    'expected_result': \"A compact, structured summary with sections for 'key_points', 'sources', and 'risks_or_caveats'.\",\n",
       "    'tools': ['web_search'],\n",
       "    'depends_on': [],\n",
       "    'mode': <NodeMode.AUTO: 'auto'>,\n",
       "    'output_schema': None,\n",
       "    'model': None,\n",
       "    'policy': None,\n",
       "    'export': True,\n",
       "    'export_path': None},\n",
       "   {'id': 'outline',\n",
       "    'description': None,\n",
       "    'kind': 'base',\n",
       "    'purpose': 'Design a clear structure for the article.',\n",
       "    'goal': 'Turn the research summary into a logical outline suitable for a 2000-2500 word blog post.',\n",
       "    'expected_result': 'A title, a detailed description, and an ordered list of sections with headings and bullet points.',\n",
       "    'tools': [],\n",
       "    'depends_on': ['research'],\n",
       "    'mode': <NodeMode.STRUCTURED: 'structured'>,\n",
       "    'output_schema': None,\n",
       "    'model': None,\n",
       "    'policy': None,\n",
       "    'export': True,\n",
       "    'export_path': None},\n",
       "   {'id': 'draft',\n",
       "    'description': None,\n",
       "    'kind': 'base',\n",
       "    'purpose': 'Write the first full draft of the article of about 3000 words including every detail.',\n",
       "    'goal': 'Use the outline and research to produce a readable, coherent blog draft with clear headings, examples, and smooth transitions.\\n',\n",
       "    'expected_result': 'A complete draft in markdown, including title, headings, and paragraphs.',\n",
       "    'tools': [],\n",
       "    'depends_on': ['outline', 'research'],\n",
       "    'mode': <NodeMode.TEXT: 'text'>,\n",
       "    'output_schema': None,\n",
       "    'model': None,\n",
       "    'policy': {'max_new_tokens': 16000,\n",
       "     'temperature': 0.7,\n",
       "     'retries': None,\n",
       "     'timeout_s': None,\n",
       "     'allow_input_pruning': None,\n",
       "     'repair_with_llm': True,\n",
       "     'strict_tool_call': None,\n",
       "     'strict_json': None,\n",
       "     'max_json_repair_attempts': None,\n",
       "     'skip_special_tokens': None,\n",
       "     'return_stream_by_default': None,\n",
       "     'log_internal_thoughts': None},\n",
       "    'export': True,\n",
       "    'export_path': None},\n",
       "   {'id': 'review',\n",
       "    'description': None,\n",
       "    'kind': 'base',\n",
       "    'purpose': 'Perform technical and editorial review of the draft.',\n",
       "    'goal': 'Identify factual issues, missing explanations, and stylistic problems. Suggest actionable edits and mark any unclear sections.\\n',\n",
       "    'expected_result': 'A structured review with strengths, issues, and concrete suggestions.',\n",
       "    'tools': [],\n",
       "    'depends_on': ['draft', 'research'],\n",
       "    'mode': <NodeMode.STRUCTURED: 'structured'>,\n",
       "    'output_schema': None,\n",
       "    'model': None,\n",
       "    'policy': {'max_new_tokens': 16000,\n",
       "     'temperature': 0.7,\n",
       "     'retries': None,\n",
       "     'timeout_s': None,\n",
       "     'allow_input_pruning': None,\n",
       "     'repair_with_llm': True,\n",
       "     'strict_tool_call': None,\n",
       "     'strict_json': None,\n",
       "     'max_json_repair_attempts': None,\n",
       "     'skip_special_tokens': None,\n",
       "     'return_stream_by_default': None,\n",
       "     'log_internal_thoughts': None},\n",
       "    'export': True,\n",
       "    'export_path': None}],\n",
       "  'outputs': ['draft', 'review']},\n",
       " 'nodes': {'research': {'node_id': 'research',\n",
       "   'mode': <NodeMode.AUTO: 'auto'>,\n",
       "   'raw_output': {'query': 'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.',\n",
       "    'summary': \"Top 3 results out of ~113 results for: 'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.'\\n1. LLM observability tools: Monitoring, debugging, and ... â€” https://medium.com/online-inference/llm-observability-tools-monitoring-debugging-and-improving-ai-systems-5af769796266\\n2. LLM Observability Explained: Prevent Hallucinations, ... â€” https://www.splunk.com/en_us/blog/learn/llm-observability.html\\n3. What Is LLM Observability? Use Cases and Best Practices â€” https://www.tredence.com/blog/llm-observability\",\n",
       "    'provider': 'serpapi',\n",
       "    'elapsed_ms': 6055,\n",
       "    'rag_content': 'Source: de17dd0dd7eabff1:cc35f490\\nLLM observability tools: Monitoring, debugging, and improving AI systems Dave Davies 24 min read Â· Aug 22, 2025 -- Listen Share LLM observability is crucial for maintaining the operational health of applications powered by large language models. As AI systems become more complex and widely deployed, having visibility into their behavior is increasingly important. Observability tools provide comprehensive insight into all layers of an LLM-powered application, helping teams monitor performance, debug issues, and ensure outputs remain safe and reliable. Press enter or click to view image in full size By capturing everything from system metrics to model inputs and outputs, ... anomalies. This gives engineers actionable insights to fix inaccuracies, optimize performance, and uphold safety standards. In short, LLM observability creates a feedback loop that turns raw model behavior into useful information for improving your application. LLM observability vs. LLM monitoring Itâ€™s important to d\\n\\n---\\n\\nSource: 09ad15b0683646b8:cc35f490\\nck loop that turns raw model behavior into useful information for improving your application. LLM observability vs. LLM monitoring Itâ€™s important to distinguish observability from monitoring in the context of LLM applications. LLM monitoring typically focuses on the â€œwhatâ€ â€” it tracks key performance metrics and system health indicators to ensure the service is up and responsive. For example, monitoring will measure metrics such as API response time, error rates, request throughput, and token usage counts. If something goes ... wrong; observability helps you understand exactly what happened inside the system and how to fix it. Both are necessary â€” monitoring provides early warnings, while observability provides deep diagnosis â€” but observability offers the comprehensive visibility that LLM applications especially need. Challenges addressed by LLM observability tools Large language model applications face a range of challenges that traditional software doesnâ€™t, and LLM observability too\\n\\n---\\n\\nSource: a11a9e62cab8284b:cc35f490\\nt overruns are another issue addressed by observability. Many LLM services (such as OpenAIâ€™s API) charge based on token usage or compute time. Without ... observability solutions To solve the challenges above, an effective LLM observability solution provides several key capabilities. At the core is real-time performance monitoring across the entire system. This means the tool tracks metrics such as response latency, throughput of requests, error rates, and resource usage (CPU, memory, GPU, or API tokens) in real time. With these insights, teams can immediately spot performance degradation â€” for example, if the average response time of the model starts creeping up or if the error rate exceeds a threshold. Real-time monitoring ensures that any latency spikes, timeouts, or bottlenecks in the LLM or its ... tools integrate checks for content safety and compliance. This may involve scanning model outputs with profanity or hate speech detectors, bias evaluators, or using AI-driven content mo\\n\\n---\\n\\nSource: 6b08b29bc2b9891d:cc35f490\\nd by LLM observability tools Large language model applications face a range of challenges that traditional software doesnâ€™t, and LLM observability tools are designed to address these issues. One major problem is hallucinations â€” when an LLM generates answers that sound plausible but are factually incorrect or completely made-up. For example, an LLM might confidently provide a wrong ... external API call holding things up? LLM observability ties together data from all components, so teams can pinpoint if a particular step (like a database search for context or a third-party API call) is causing the delay. Similarly, failures or errors in a pipeline can be tricky to reproduce given the complexity of LLM systems. Observability tools trace each step of a request, making it easier to debug when something goes wrong in a multi-step process. Cost overruns are another issue addressed by observability. Many LLM services (such as OpenAIâ€™s API) charge based on token usage or compute time. Without\\n\\n---\\n\\nSource: 724c404f6a885694:cc35f490\\nsafety and compliance. This may involve scanning model outputs with profanity or hate speech detectors, bias evaluators, or using AI-driven content moderation APIs. A good observability platform might, for example, automatically tag any response that seems to contain personally identifiable information or that violates certain ethical guidelines. In regulated industries, observability also means keeping an audit trail â€” the tool should log all prompts and responses securely so that if thereâ€™s ever a question or incident, thereâ€™s a record to review. Features like PII redaction (masking sensitive user data in logs) and access control become ... LLM observability tools As the demand for LLM observability has grown, a number of tools and platforms have emerged to help developers monitor and debug their language model applications. Each tool has its own focus and strengths, but all aim to shed light on the â€œblack boxâ€ of LLMs in production. Some of the leading observability tools in this sp\\n\\n---\\n\\nSource: 06f031e91c9c0236:cc35f490\\nheir applications in just a few lines of code and immediately gain end-to-end visibility and confidence in their modelâ€™s behavior. The tutorial illustrated how easy it is to get started with Weave and how it can be leveraged for tasks ranging from real-time monitoring to quality assurance and ethical compliance. Looking ahead, LLM observability tools will likely ... know they can catch mistakes and iterate quickly. Tools like W&B Weave are at the forefront of this effort, providing the means to monitor, debug, and improve LLM applications in one unified environment. By adopting LLM observability practices now, developers and organizations set themselves up for success â€” ensuring that their AI systems remain performant, trustworthy, and aligned with user needs as both the models and usage grow. As these observability tools evolve, they will undoubtedly play a central role in the future of AI deployments, helping us harness the full potential of LLMs while keeping a watchful eye on\\n\\n---\\n\\nSource: 9758ab006e75cff3:cc35f490\\n Weave, instrumented our LLM call with one line ( @weave.op ), ran the application to log data, optionally added an evaluation, and then ... bottlenecks in your LLM application. Suppose your app has higher-than-desired latency; by looking at traces, you might discover that a database lookup or a particular model call is the slow part. With that insight, you could try caching results, using a smaller model for that step, or otherwise improving the architecture. Likewise, Weaveâ€™s token usage tracking can show if certain prompts are excessively long (costly) â€” prompting you to optimize prompt formats to reduce token counts. Over time, you can use Weave metrics to verify that optimizations are effective (e.g., see average latency drop after a change, or a reduction ... when you have a robust observability tool in place. Continuous evaluation and model improvement: You can use Weave not just for monitoring but for ongoing evaluation even after deployment. Letâ€™s say you regularly update your\\n\\n---\\n\\nSource: d7c061743dc2e1ee:cc35f490\\n on LLM deployments. On top of all that, W&Bâ€™s focus on enterprise readiness (with secure cloud hosting, or on-prem deployment if needed, and compliance features) gives confidence to companies that need to monitor LLMs in production while meeting security requirements. In summary, W&B Weave is a preferred observability tool because it combines comprehensive tracing, real-time performance monitoring, automated output evaluation, and an intuitive UI in one platform. It effectively bridges the gap between model development ... enter your API key Once installed and authenticated, initialize Weave in your script or notebook by calling weave.init() at the start of your program. This sets up a new Weave project where all your logs and traces will be stored. For example: import weave # Initialize a Weave project (give a name for this observability run) weave.init(project=\"llm-observability-demo\") After this initialization, Weave is ready to capture data. It will associate all logs with the pro\\n\\n---\\n\\nSource: 2d3c24ba61c5ba18:cc35f490\\nas its own focus and strengths, but all aim to shed light on the â€œblack boxâ€ of LLMs in production. Some of the leading observability tools in this space include Lunary, LangSmith, and W&B Weave, among others. Lunary Lunary is an open-source platform that provides analytics and tracing for LLM-powered apps. Itâ€™s designed to be model-agnostic, meaning it can work with any LLM provider or architecture. Lunary focuses on ... and intermediate actions (like calls to tools or external APIs). This provides end-to-end tracing for chain-of-thought style applications. LangSmith also emphasizes prompt evaluation and testing. It allows developers to systematically evaluate their prompts and model outputs by using datasets of queries and expected answers. In the LangSmith dashboard, you can see how often your modelâ€™s responses match the expected results or where it might be failing. It supports scoring of outputs for accuracy, completeness, or custom metrics you define. Additionally, LangSmith prov\\n\\n---\\n\\nSource: e5ee1e1a6d3f0816:cc35f490\\nthen to an LLM, and then into some post-processing, ... means W&B Weave not only tells you that an answer was given, but also gives an indication of whether that answer was good or potentially problematic â€” all in real time. Few other observability tools have this level of integrated quality checking. Seamless integration and agnostic support are also key features of W&B Weave. It is designed to be framework-agnostic and model-agnostic: whether youâ€™re using OpenAIâ€™s GPT-4 via API, an open-source model on Hugging Face, or a LangChain application, Weave can plug in. The tool provides simple SDK methods and decorators to instrument your code. For instance, by adding a ... in a W&B report format) to communicate your findings. The collaborative features (permissions, team projects, comments) make it a strong choice for enterprise teams working on LLM deployments. On top of all that, W&Bâ€™s focus on enterprise readiness (with secure cloud hosting, or on-prem deployment if needed, and complian'},\n",
       "   'structured_output': None,\n",
       "   'tool_call_output': {'selected_tool': 'web_search',\n",
       "    'inputs': {'query': 'Stop Treating LMs as Black Boxes: The Case for Observability-First AI Systems',\n",
       "     'hl': 'en'},\n",
       "    'returns': {'query': 'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.',\n",
       "     'summary': \"Top 3 results out of ~113 results for: 'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.'\\n1. LLM observability tools: Monitoring, debugging, and ... â€” https://medium.com/online-inference/llm-observability-tools-monitoring-debugging-and-improving-ai-systems-5af769796266\\n2. LLM Observability Explained: Prevent Hallucinations, ... â€” https://www.splunk.com/en_us/blog/learn/llm-observability.html\\n3. What Is LLM Observability? Use Cases and Best Practices â€” https://www.tredence.com/blog/llm-observability\",\n",
       "     'provider': 'serpapi',\n",
       "     'elapsed_ms': 6055,\n",
       "     'rag_content': 'Source: de17dd0dd7eabff1:cc35f490\\nLLM observability tools: Monitoring, debugging, and improving AI systems Dave Davies 24 min read Â· Aug 22, 2025 -- Listen Share LLM observability is crucial for maintaining the operational health of applications powered by large language models. As AI systems become more complex and widely deployed, having visibility into their behavior is increasingly important. Observability tools provide comprehensive insight into all layers of an LLM-powered application, helping teams monitor performance, debug issues, and ensure outputs remain safe and reliable. Press enter or click to view image in full size By capturing everything from system metrics to model inputs and outputs, ... anomalies. This gives engineers actionable insights to fix inaccuracies, optimize performance, and uphold safety standards. In short, LLM observability creates a feedback loop that turns raw model behavior into useful information for improving your application. LLM observability vs. LLM monitoring Itâ€™s important to d\\n\\n---\\n\\nSource: 09ad15b0683646b8:cc35f490\\nck loop that turns raw model behavior into useful information for improving your application. LLM observability vs. LLM monitoring Itâ€™s important to distinguish observability from monitoring in the context of LLM applications. LLM monitoring typically focuses on the â€œwhatâ€ â€” it tracks key performance metrics and system health indicators to ensure the service is up and responsive. For example, monitoring will measure metrics such as API response time, error rates, request throughput, and token usage counts. If something goes ... wrong; observability helps you understand exactly what happened inside the system and how to fix it. Both are necessary â€” monitoring provides early warnings, while observability provides deep diagnosis â€” but observability offers the comprehensive visibility that LLM applications especially need. Challenges addressed by LLM observability tools Large language model applications face a range of challenges that traditional software doesnâ€™t, and LLM observability too\\n\\n---\\n\\nSource: a11a9e62cab8284b:cc35f490\\nt overruns are another issue addressed by observability. Many LLM services (such as OpenAIâ€™s API) charge based on token usage or compute time. Without ... observability solutions To solve the challenges above, an effective LLM observability solution provides several key capabilities. At the core is real-time performance monitoring across the entire system. This means the tool tracks metrics such as response latency, throughput of requests, error rates, and resource usage (CPU, memory, GPU, or API tokens) in real time. With these insights, teams can immediately spot performance degradation â€” for example, if the average response time of the model starts creeping up or if the error rate exceeds a threshold. Real-time monitoring ensures that any latency spikes, timeouts, or bottlenecks in the LLM or its ... tools integrate checks for content safety and compliance. This may involve scanning model outputs with profanity or hate speech detectors, bias evaluators, or using AI-driven content mo\\n\\n---\\n\\nSource: 6b08b29bc2b9891d:cc35f490\\nd by LLM observability tools Large language model applications face a range of challenges that traditional software doesnâ€™t, and LLM observability tools are designed to address these issues. One major problem is hallucinations â€” when an LLM generates answers that sound plausible but are factually incorrect or completely made-up. For example, an LLM might confidently provide a wrong ... external API call holding things up? LLM observability ties together data from all components, so teams can pinpoint if a particular step (like a database search for context or a third-party API call) is causing the delay. Similarly, failures or errors in a pipeline can be tricky to reproduce given the complexity of LLM systems. Observability tools trace each step of a request, making it easier to debug when something goes wrong in a multi-step process. Cost overruns are another issue addressed by observability. Many LLM services (such as OpenAIâ€™s API) charge based on token usage or compute time. Without\\n\\n---\\n\\nSource: 724c404f6a885694:cc35f490\\nsafety and compliance. This may involve scanning model outputs with profanity or hate speech detectors, bias evaluators, or using AI-driven content moderation APIs. A good observability platform might, for example, automatically tag any response that seems to contain personally identifiable information or that violates certain ethical guidelines. In regulated industries, observability also means keeping an audit trail â€” the tool should log all prompts and responses securely so that if thereâ€™s ever a question or incident, thereâ€™s a record to review. Features like PII redaction (masking sensitive user data in logs) and access control become ... LLM observability tools As the demand for LLM observability has grown, a number of tools and platforms have emerged to help developers monitor and debug their language model applications. Each tool has its own focus and strengths, but all aim to shed light on the â€œblack boxâ€ of LLMs in production. Some of the leading observability tools in this sp\\n\\n---\\n\\nSource: 06f031e91c9c0236:cc35f490\\nheir applications in just a few lines of code and immediately gain end-to-end visibility and confidence in their modelâ€™s behavior. The tutorial illustrated how easy it is to get started with Weave and how it can be leveraged for tasks ranging from real-time monitoring to quality assurance and ethical compliance. Looking ahead, LLM observability tools will likely ... know they can catch mistakes and iterate quickly. Tools like W&B Weave are at the forefront of this effort, providing the means to monitor, debug, and improve LLM applications in one unified environment. By adopting LLM observability practices now, developers and organizations set themselves up for success â€” ensuring that their AI systems remain performant, trustworthy, and aligned with user needs as both the models and usage grow. As these observability tools evolve, they will undoubtedly play a central role in the future of AI deployments, helping us harness the full potential of LLMs while keeping a watchful eye on\\n\\n---\\n\\nSource: 9758ab006e75cff3:cc35f490\\n Weave, instrumented our LLM call with one line ( @weave.op ), ran the application to log data, optionally added an evaluation, and then ... bottlenecks in your LLM application. Suppose your app has higher-than-desired latency; by looking at traces, you might discover that a database lookup or a particular model call is the slow part. With that insight, you could try caching results, using a smaller model for that step, or otherwise improving the architecture. Likewise, Weaveâ€™s token usage tracking can show if certain prompts are excessively long (costly) â€” prompting you to optimize prompt formats to reduce token counts. Over time, you can use Weave metrics to verify that optimizations are effective (e.g., see average latency drop after a change, or a reduction ... when you have a robust observability tool in place. Continuous evaluation and model improvement: You can use Weave not just for monitoring but for ongoing evaluation even after deployment. Letâ€™s say you regularly update your\\n\\n---\\n\\nSource: d7c061743dc2e1ee:cc35f490\\n on LLM deployments. On top of all that, W&Bâ€™s focus on enterprise readiness (with secure cloud hosting, or on-prem deployment if needed, and compliance features) gives confidence to companies that need to monitor LLMs in production while meeting security requirements. In summary, W&B Weave is a preferred observability tool because it combines comprehensive tracing, real-time performance monitoring, automated output evaluation, and an intuitive UI in one platform. It effectively bridges the gap between model development ... enter your API key Once installed and authenticated, initialize Weave in your script or notebook by calling weave.init() at the start of your program. This sets up a new Weave project where all your logs and traces will be stored. For example: import weave # Initialize a Weave project (give a name for this observability run) weave.init(project=\"llm-observability-demo\") After this initialization, Weave is ready to capture data. It will associate all logs with the pro\\n\\n---\\n\\nSource: 2d3c24ba61c5ba18:cc35f490\\nas its own focus and strengths, but all aim to shed light on the â€œblack boxâ€ of LLMs in production. Some of the leading observability tools in this space include Lunary, LangSmith, and W&B Weave, among others. Lunary Lunary is an open-source platform that provides analytics and tracing for LLM-powered apps. Itâ€™s designed to be model-agnostic, meaning it can work with any LLM provider or architecture. Lunary focuses on ... and intermediate actions (like calls to tools or external APIs). This provides end-to-end tracing for chain-of-thought style applications. LangSmith also emphasizes prompt evaluation and testing. It allows developers to systematically evaluate their prompts and model outputs by using datasets of queries and expected answers. In the LangSmith dashboard, you can see how often your modelâ€™s responses match the expected results or where it might be failing. It supports scoring of outputs for accuracy, completeness, or custom metrics you define. Additionally, LangSmith prov\\n\\n---\\n\\nSource: e5ee1e1a6d3f0816:cc35f490\\nthen to an LLM, and then into some post-processing, ... means W&B Weave not only tells you that an answer was given, but also gives an indication of whether that answer was good or potentially problematic â€” all in real time. Few other observability tools have this level of integrated quality checking. Seamless integration and agnostic support are also key features of W&B Weave. It is designed to be framework-agnostic and model-agnostic: whether youâ€™re using OpenAIâ€™s GPT-4 via API, an open-source model on Hugging Face, or a LangChain application, Weave can plug in. The tool provides simple SDK methods and decorators to instrument your code. For instance, by adding a ... in a W&B report format) to communicate your findings. The collaborative features (permissions, team projects, comments) make it a strong choice for enterprise teams working on LLM deployments. On top of all that, W&Bâ€™s focus on enterprise readiness (with secure cloud hosting, or on-prem deployment if needed, and complian'},\n",
       "    'final': False,\n",
       "    'extras': {}},\n",
       "   'started_at': 1765473281.1212542,\n",
       "   'duration_ms': 7155,\n",
       "   'error': None,\n",
       "   'traces': {'steps': [{'step_id': 1,\n",
       "      'kind': 'agent',\n",
       "      'label': 'agent.run',\n",
       "      'node_id': None,\n",
       "      'agent_id': 'research',\n",
       "      'started_at': 1765473281.1237457,\n",
       "      'duration_ms': 7151,\n",
       "      'inputs': {'agent_type': 'Agent',\n",
       "       'has_toolkit': True,\n",
       "       'structured': False,\n",
       "       'stream': False,\n",
       "       'strict_tool_call': False},\n",
       "      'outputs': {},\n",
       "      'meta': {},\n",
       "      'ok': True,\n",
       "      'error': None,\n",
       "      'logs': []},\n",
       "     {'step_id': 2,\n",
       "      'kind': 'llm.call',\n",
       "      'label': 'agent.route_and_call.router_llm_call',\n",
       "      'node_id': None,\n",
       "      'agent_id': 'research',\n",
       "      'started_at': 1765473281.1247797,\n",
       "      'duration_ms': 944,\n",
       "      'inputs': {'attempt': 1,\n",
       "       'strict_tool_call': False,\n",
       "       'system_prompt_len': 1316,\n",
       "       'user_prompt_len': 528,\n",
       "       'user_prompt': 'Perform focused research on the topic \"Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems\". Collect key facts, terminology, and references that are directly useful for writing a technical blog post. Focus on observability layers, tracing, metrics, feedback loops, and common failure patterns in modern LLM systems. Use web search to gather relevant sources and ensure the information is up-to-date and credible. Organize the findings into sections for \\'key_points\\', \\'sources\\', and \\'risks_or_caveats\\'.',\n",
       "       'system_prompt': 'You are a stateless tool router. \\nYour task is to decide whether to call a tool or not, and respond with STRICT JSON.\\n\\nAlways respond with a single one-line valid JSON object:\\n{\"tool\": \"<tool_name>\", \"inputs\": {<param>: <value>}}\\n\\nRules:\\n- Choose at most ONE tool per request.\\n- If no tool fits the request or inputs are ambiguous, output:\\n  {\"tool\": \"none\", \"inputs\": {}}\\n- Use only explicit parameters defined by that tool. Do NOT invent or rename parameters.\\n- Include only required parameters unless an optional one is clearly implied.\\n- Do NOT produce natural language answers. Emit JSON only.\\n\\nTOOLS CATALOG:\\nAvailable tools:\\nTool Name: `web_search`\\nDescription: Search the web using a pluggable backend (e.g. SerpAPI). Optionally crawls the top results, extracts page content, and summarizes it with an LLM.\\nWhen to use: Use this tool when you need up-to-date information, external web content, or detailed summaries combining multiple sources. The tool can return raw results or a refined LLM summary.\\nTool Inputs:\\n- `query`: string (required) â€” The web search query.\\n- `hl`: string (optional) â€” Interface language (e.g. \\'en\\'). Defaults to \\'en\\'.\\nTool Return: object â€” JSON object with keys: `query`, `summary`, `results`, `provider`, `elapsed_ms`, and optionally `llm_summary` if summarization is enabled.\\n\\n\\n',\n",
       "       'temperature': 0.7,\n",
       "       'max_new_tokens': 512,\n",
       "       'stream': False},\n",
       "      'outputs': {'model_response': '{\"tool\": \"web_search\", \"inputs\": {\"query\": \"Stop Treating LMs as Black Boxes: The Case for Observability-First AI Systems\", \"hl\": \"en\"}}',\n",
       "       'model_response_len': 136},\n",
       "      'meta': {},\n",
       "      'ok': True,\n",
       "      'error': None,\n",
       "      'logs': [{'ts': 1765473282.0682263,\n",
       "        'message': 'Selected tool: web_search',\n",
       "        'data': {},\n",
       "        'type': 'info'},\n",
       "       {'ts': 1765473282.0690644,\n",
       "        'message': \"Raw inputs: {'query': 'Stop Treating LMs as Black Boxes: The Case for Observability-First AI Systems', 'hl': 'en'}\",\n",
       "        'data': {},\n",
       "        'type': 'info'}]},\n",
       "     {'step_id': 3,\n",
       "      'kind': 'tool.execute',\n",
       "      'label': 'agent.route_and_call.tool_execute',\n",
       "      'node_id': None,\n",
       "      'agent_id': 'research',\n",
       "      'started_at': 1765473282.070135,\n",
       "      'duration_ms': 6203,\n",
       "      'inputs': {'tool_name': 'web_search',\n",
       "       'payload': {'query': 'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.',\n",
       "        'hl': 'en',\n",
       "        'dependencies': {},\n",
       "        'topic_title': 'Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems',\n",
       "        'audience': 'Intermediate ML engineers',\n",
       "        'tone': 'Practical and slightly opinionated'}},\n",
       "      'outputs': {'tool_return': {'query': 'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.',\n",
       "        'summary': \"Top 3 results out of ~113 results for: 'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.'\\n1. LLM observability tools: Monitoring, debugging, and ... â€” https://medium.com/online-inference/llm-observability-tools-monitoring-debugging-and-improving-ai-systems-5af769796266\\n2. LLM Observability Explained: Prevent Hallucinations, ... â€” https://www.splunk.com/en_us/blog/learn/llm-observability.html\\n3. What Is LLM Observability? Use Cases and Best Practices â€” https://www.tredence.com/blog/llm-observability\",\n",
       "        'provider': 'serpapi',\n",
       "        'elapsed_ms': 6055,\n",
       "        'rag_content': 'Source: de17dd0dd7eabff1:cc35f490\\nLLM observability tools: Monitoring, debugging, and improving AI systems Dave Davies 24 min read Â· Aug 22, 2025 -- Listen Share LLM observability is crucial for maintaining the operational health of applications powered by large language models. As AI systems become more complex and widely deployed, having visibility into their behavior is increasingly important. Observability tools provide comprehensive insight into all layers of an LLM-powered application, helping teams monitor performance, debug issues, and ensure outputs remain safe and reliable. Press enter or click to view image in full size By capturing everything from system metrics to model inputs and outputs, ... anomalies. This gives engineers actionable insights to fix inaccuracies, optimize performance, and uphold safety standards. In short, LLM observability creates a feedback loop that turns raw model behavior into useful information for improving your application. LLM observability vs. LLM monitoring Itâ€™s important to d\\n\\n---\\n\\nSource: 09ad15b0683646b8:cc35f490\\nck loop that turns raw model behavior into useful information for improving your application. LLM observability vs. LLM monitoring Itâ€™s important to distinguish observability from monitoring in the context of LLM applications. LLM monitoring typically focuses on the â€œwhatâ€ â€” it tracks key performance metrics and system health indicators to ensure the service is up and responsive. For example, monitoring will measure metrics such as API response time, error rates, request throughput, and token usage counts. If something goes ... wrong; observability helps you understand exactly what happened inside the system and how to fix it. Both are necessary â€” monitoring provides early warnings, while observability provides deep diagnosis â€” but observability offers the comprehensive visibility that LLM applications especially need. Challenges addressed by LLM observability tools Large language model applications face a range of challenges that traditional software doesnâ€™t, and LLM observability too\\n\\n---\\n\\nSource: a11a9e62cab8284b:cc35f490\\nt overruns are another issue addressed by observability. Many LLM services (such as OpenAIâ€™s API) charge based on token usage or compute time. Without ... observability solutions To solve the challenges above, an effective LLM observability solution provides several key capabilities. At the core is real-time performance monitoring across the entire system. This means the tool tracks metrics such as response latency, throughput of requests, error rates, and resource usage (CPU, memory, GPU, or API tokens) in real time. With these insights, teams can immediately spot performance degradation â€” for example, if the average response time of the model starts creeping up or if the error rate exceeds a threshold. Real-time monitoring ensures that any latency spikes, timeouts, or bottlenecks in the LLM or its ... tools integrate checks for content safety and compliance. This may involve scanning model outputs with profanity or hate speech detectors, bias evaluators, or using AI-driven content mo\\n\\n---\\n\\nSource: 6b08b29bc2b9891d:cc35f490\\nd by LLM observability tools Large language model applications face a range of challenges that traditional software doesnâ€™t, and LLM observability tools are designed to address these issues. One major problem is hallucinations â€” when an LLM generates answers that sound plausible but are factually incorrect or completely made-up. For example, an LLM might confidently provide a wrong ... external API call holding things up? LLM observability ties together data from all components, so teams can pinpoint if a particular step (like a database search for context or a third-party API call) is causing the delay. Similarly, failures or errors in a pipeline can be tricky to reproduce given the complexity of LLM systems. Observability tools trace each step of a request, making it easier to debug when something goes wrong in a multi-step process. Cost overruns are another issue addressed by observability. Many LLM services (such as OpenAIâ€™s API) charge based on token usage or compute time. Without\\n\\n---\\n\\nSource: 724c404f6a885694:cc35f490\\nsafety and compliance. This may involve scanning model outputs with profanity or hate speech detectors, bias evaluators, or using AI-driven content moderation APIs. A good observability platform might, for example, automatically tag any response that seems to contain personally identifiable information or that violates certain ethical guidelines. In regulated industries, observability also means keeping an audit trail â€” the tool should log all prompts and responses securely so that if thereâ€™s ever a question or incident, thereâ€™s a record to review. Features like PII redaction (masking sensitive user data in logs) and access control become ... LLM observability tools As the demand for LLM observability has grown, a number of tools and platforms have emerged to help developers monitor and debug their language model applications. Each tool has its own focus and strengths, but all aim to shed light on the â€œblack boxâ€ of LLMs in production. Some of the leading observability tools in this sp\\n\\n---\\n\\nSource: 06f031e91c9c0236:cc35f490\\nheir applications in just a few lines of code and immediately gain end-to-end visibility and confidence in their modelâ€™s behavior. The tutorial illustrated how easy it is to get started with Weave and how it can be leveraged for tasks ranging from real-time monitoring to quality assurance and ethical compliance. Looking ahead, LLM observability tools will likely ... know they can catch mistakes and iterate quickly. Tools like W&B Weave are at the forefront of this effort, providing the means to monitor, debug, and improve LLM applications in one unified environment. By adopting LLM observability practices now, developers and organizations set themselves up for success â€” ensuring that their AI systems remain performant, trustworthy, and aligned with user needs as both the models and usage grow. As these observability tools evolve, they will undoubtedly play a central role in the future of AI deployments, helping us harness the full potential of LLMs while keeping a watchful eye on\\n\\n---\\n\\nSource: 9758ab006e75cff3:cc35f490\\n Weave, instrumented our LLM call with one line ( @weave.op ), ran the application to log data, optionally added an evaluation, and then ... bottlenecks in your LLM application. Suppose your app has higher-than-desired latency; by looking at traces, you might discover that a database lookup or a particular model call is the slow part. With that insight, you could try caching results, using a smaller model for that step, or otherwise improving the architecture. Likewise, Weaveâ€™s token usage tracking can show if certain prompts are excessively long (costly) â€” prompting you to optimize prompt formats to reduce token counts. Over time, you can use Weave metrics to verify that optimizations are effective (e.g., see average latency drop after a change, or a reduction ... when you have a robust observability tool in place. Continuous evaluation and model improvement: You can use Weave not just for monitoring but for ongoing evaluation even after deployment. Letâ€™s say you regularly update your\\n\\n---\\n\\nSource: d7c061743dc2e1ee:cc35f490\\n on LLM deployments. On top of all that, W&Bâ€™s focus on enterprise readiness (with secure cloud hosting, or on-prem deployment if needed, and compliance features) gives confidence to companies that need to monitor LLMs in production while meeting security requirements. In summary, W&B Weave is a preferred observability tool because it combines comprehensive tracing, real-time performance monitoring, automated output evaluation, and an intuitive UI in one platform. It effectively bridges the gap between model development ... enter your API key Once installed and authenticated, initialize Weave in your script or notebook by calling weave.init() at the start of your program. This sets up a new Weave project where all your logs and traces will be stored. For example: import weave # Initialize a Weave project (give a name for this observability run) weave.init(project=\"llm-observability-demo\") After this initialization, Weave is ready to capture data. It will associate all logs with the pro\\n\\n---\\n\\nSource: 2d3c24ba61c5ba18:cc35f490\\nas its own focus and strengths, but all aim to shed light on the â€œblack boxâ€ of LLMs in production. Some of the leading observability tools in this space include Lunary, LangSmith, and W&B Weave, among others. Lunary Lunary is an open-source platform that provides analytics and tracing for LLM-powered apps. Itâ€™s designed to be model-agnostic, meaning it can work with any LLM provider or architecture. Lunary focuses on ... and intermediate actions (like calls to tools or external APIs). This provides end-to-end tracing for chain-of-thought style applications. LangSmith also emphasizes prompt evaluation and testing. It allows developers to systematically evaluate their prompts and model outputs by using datasets of queries and expected answers. In the LangSmith dashboard, you can see how often your modelâ€™s responses match the expected results or where it might be failing. It supports scoring of outputs for accuracy, completeness, or custom metrics you define. Additionally, LangSmith prov\\n\\n---\\n\\nSource: e5ee1e1a6d3f0816:cc35f490\\nthen to an LLM, and then into some post-processing, ... means W&B Weave not only tells you that an answer was given, but also gives an indication of whether that answer was good or potentially problematic â€” all in real time. Few other observability tools have this level of integrated quality checking. Seamless integration and agnostic support are also key features of W&B Weave. It is designed to be framework-agnostic and model-agnostic: whether youâ€™re using OpenAIâ€™s GPT-4 via API, an open-source model on Hugging Face, or a LangChain application, Weave can plug in. The tool provides simple SDK methods and decorators to instrument your code. For instance, by adding a ... in a W&B report format) to communicate your findings. The collaborative features (permissions, team projects, comments) make it a strong choice for enterprise teams working on LLM deployments. On top of all that, W&Bâ€™s focus on enterprise readiness (with secure cloud hosting, or on-prem deployment if needed, and complian'},\n",
       "       'extras': {}},\n",
       "      'meta': {},\n",
       "      'ok': True,\n",
       "      'error': None,\n",
       "      'logs': [{'ts': 1765473288.2731433,\n",
       "        'message': \"Tool 'web_search' Tool Return: {'query': 'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LL...\",\n",
       "        'data': {},\n",
       "        'type': 'info'}]}],\n",
       "    'meta': {'agent_type': 'generic_agent',\n",
       "     'agent_config': {'allow_input_pruning': True,\n",
       "      'repair_with_llm': True,\n",
       "      'strict_tool_call': False,\n",
       "      'temperature': 0.7,\n",
       "      'max_new_tokens': 512,\n",
       "      'return_stream_by_default': False,\n",
       "      'retry': {'max_route_retries': 2,\n",
       "       'max_tool_retries': 1,\n",
       "       'backoff_sec': 0.7},\n",
       "      'strict_json': True,\n",
       "      'max_json_repair_attempts': 1},\n",
       "     'model': '/home/nomi/workspace/Model_Weights/Qwen3-8B-unsloth-bnb-4bit',\n",
       "     'toolkit': True,\n",
       "     'log_steps': True}}},\n",
       "  'outline': {'node_id': 'outline',\n",
       "   'mode': <NodeMode.STRUCTURED: 'structured'>,\n",
       "   'raw_output': '**Title:**  \\n*Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems*\\n\\n**Description:**  \\nThis blog post explores the critical need for observability in large language model (LLM) systems. As LLMs become more integrated into production applications, the lack of visibility into their internal workings has led to significant challenges, including hallucinations, performance bottlenecks, and compliance risks. By adopting an observability-first approach, organizations can gain real-time insights into model behavior, trace requests end-to-end, measure performance metrics, and create feedback loops for continuous improvement. This article outlines the key components of an observability-first strategy, discusses common failure patterns in LLM systems, and highlights the role of tools and best practices in building transparent, reliable, and ethical AI systems.\\n\\n---\\n\\n### **Outline of the Blog Post**\\n\\n#### **1. Introduction: The Rise of LLMs and the Black Box Problem**\\n- **Overview of LLMs and Their Growing Role**  \\n  - Explain the rapid adoption of LLMs across industries.  \\n  - Highlight their complexity and the challenges of deploying them at scale.  \\n\\n- **The Black Box Conundrum**  \\n  - Define the \"black box\" problem in LLMs.  \\n  - Discuss the limitations of traditional monitoring and debugging approaches.  \\n  - Introduce the concept of **observability** as a solution.  \\n\\n- **Why Observability Matters Now**  \\n  - Link the increasing complexity of LLM systems to the need for deeper visibility.  \\n  - Preview the key components of observability: tracing, metrics, feedback loops, and failure analysis.  \\n\\n---\\n\\n#### **2. Understanding Observability in LLM Systems**\\n- **What is Observability?**  \\n  - Define observability in the context of LLMs.  \\n  - Differentiate between **observability** and **monitoring**.  \\n  - Emphasize the importance of **comprehensive visibility** into model behavior.  \\n\\n- **The Observability Stack for LLMs**  \\n  - Break down the key elements of an observability layer:  \\n    - **Tracing**: Tracking requests through the system.  \\n    - **Metrics**: Measuring performance and resource usage.  \\n    - **Feedback Loops**: Using data to improve model behavior.  \\n    - **Content Safety and Compliance**: Ensuring ethical and legal standards.  \\n\\n- **The Role of Observability in AI Ethics and Trust**  \\n  - Discuss how observability supports transparency',\n",
       "   'structured_output': None,\n",
       "   'tool_call_output': None,\n",
       "   'started_at': 1765473291.4309845,\n",
       "   'duration_ms': 11311,\n",
       "   'error': None,\n",
       "   'traces': {'steps': [{'step_id': 1,\n",
       "      'kind': 'agent',\n",
       "      'label': 'agent.run',\n",
       "      'node_id': None,\n",
       "      'agent_id': 'outline',\n",
       "      'started_at': 1765473291.431669,\n",
       "      'duration_ms': 11307,\n",
       "      'inputs': {'agent_type': 'Agent',\n",
       "       'has_toolkit': False,\n",
       "       'structured': False,\n",
       "       'stream': False,\n",
       "       'strict_tool_call': False},\n",
       "      'outputs': {},\n",
       "      'meta': {},\n",
       "      'ok': True,\n",
       "      'error': None,\n",
       "      'logs': []},\n",
       "     {'step_id': 2,\n",
       "      'kind': 'llm.call',\n",
       "      'label': 'agent.free_text_call',\n",
       "      'node_id': None,\n",
       "      'agent_id': 'outline',\n",
       "      'started_at': 1765473291.431949,\n",
       "      'duration_ms': 11305,\n",
       "      'inputs': {'system_prompt_len': 562,\n",
       "       'user_prompt_len': 11831,\n",
       "       'user_prompt': 'Based on the research summary provided, create a clear and logical outline for a 2000-2500 word blog post titled \"Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems.\" The outline should include a title, a detailed description, and an ordered list of sections with headings and bullet points. Focus on the importance of observability in LLM systems, including tracing, metrics, feedback loops, and common failure patterns.\\n\\n# Context from dependency nodes:\\n## research\\n{\\'query\\': \\'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.\\', \\'summary\\': \"Top 3 results out of ~113 results for: \\'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.\\'\\\\n1. LLM observability tools: Monitoring, debugging, and ... â€” https://medium.com/online-inference/llm-observability-tools-monitoring-debugging-and-improving-ai-systems-5af769796266\\\\n2. LLM Observability Explained: Prevent Hallucinations, ... â€” https://www.splunk.com/en_us/blog/learn/llm-observability.html\\\\n3. What Is LLM Observability? Use Cases and Best Practices â€” https://www.tredence.com/blog/llm-observability\", \\'provider\\': \\'serpapi\\', \\'elapsed_ms\\': 6055, \\'rag_content\\': \\'Source: de17dd0dd7eabff1:cc35f490\\\\nLLM observability tools: Monitoring, debugging, and improving AI systems Dave Davies 24 min read Â· Aug 22, 2025 -- Listen Share LLM observability is crucial for maintaining the operational health of applications powered by large language models. As AI systems become more complex and widely deployed, having visibility into their behavior is increasingly important. Observability tools provide comprehensive insight into all layers of an LLM-powered application, helping teams monitor performance, debug issues, and ensure outputs remain safe and reliable. Press enter or click to view image in full size By capturing everything from system metrics to model inputs and outputs, ... anomalies. This gives engineers actionable insights to fix inaccuracies, optimize performance, and uphold safety standards. In short, LLM observability creates a feedback loop that turns raw model behavior into useful information for improving your application. LLM observability vs. LLM monitoring Itâ€™s important to d\\\\n\\\\n---\\\\n\\\\nSource: 09ad15b0683646b8:cc35f490\\\\nck loop that turns raw model behavior into useful information for improving your application. LLM observability vs. LLM monitoring Itâ€™s important to distinguish observability from monitoring in the context of LLM applications. LLM monitoring typically focuses on the â€œwhatâ€ â€” it tracks key performance metrics and system health indicators to ensure the service is up and responsive. For example, monitoring will measure metrics such as API response time, error rates, request throughput, and token usage counts. If something goes ... wrong; observability helps you understand exactly what happened inside the system and how to fix it. Both are necessary â€” monitoring provides early warnings, while observability provides deep diagnosis â€” but observability offers the comprehensive visibility that LLM applications especially need. Challenges addressed by LLM observability tools Large language model applications face a range of challenges that traditional software doesnâ€™t, and LLM observability too\\\\n\\\\n---\\\\n\\\\nSource: a11a9e62cab8284b:cc35f490\\\\nt overruns are another issue addressed by observability. Many LLM services (such as OpenAIâ€™s API) charge based on token usage or compute time. Without ... observability solutions To solve the challenges above, an effective LLM observability solution provides several key capabilities. At the core is real-time performance monitoring across the entire system. This means the tool tracks metrics such as response latency, throughput of requests, error rates, and resource usage (CPU, memory, GPU, or API tokens) in real time. With these insights, teams can immediately spot performance degradation â€” for example, if the average response time of the model starts creeping up or if the error rate exceeds a threshold. Real-time monitoring ensures that any latency spikes, timeouts, or bottlenecks in the LLM or its ... tools integrate checks for content safety and compliance. This may involve scanning model outputs with profanity or hate speech detectors, bias evaluators, or using AI-driven content mo\\\\n\\\\n---\\\\n\\\\nSource: 6b08b29bc2b9891d:cc35f490\\\\nd by LLM observability tools Large language model applications face a range of challenges that traditional software doesnâ€™t, and LLM observability tools are designed to address these issues. One major problem is hallucinations â€” when an LLM generates answers that sound plausible but are factually incorrect or completely made-up. For example, an LLM might confidently provide a wrong ... external API call holding things up? LLM observability ties together data from all components, so teams can pinpoint if a particular step (like a database search for context or a third-party API call) is causing the delay. Similarly, failures or errors in a pipeline can be tricky to reproduce given the complexity of LLM systems. Observability tools trace each step of a request, making it easier to debug when something goes wrong in a multi-step process. Cost overruns are another issue addressed by observability. Many LLM services (such as OpenAIâ€™s API) charge based on token usage or compute time. Without\\\\n\\\\n---\\\\n\\\\nSource: 724c404f6a885694:cc35f490\\\\nsafety and compliance. This may involve scanning model outputs with profanity or hate speech detectors, bias evaluators, or using AI-driven content moderation APIs. A good observability platform might, for example, automatically tag any response that seems to contain personally identifiable information or that violates certain ethical guidelines. In regulated industries, observability also means keeping an audit trail â€” the tool should log all prompts and responses securely so that if thereâ€™s ever a question or incident, thereâ€™s a record to review. Features like PII redaction (masking sensitive user data in logs) and access control become ... LLM observability tools As the demand for LLM observability has grown, a number of tools and platforms have emerged to help developers monitor and debug their language model applications. Each tool has its own focus and strengths, but all aim to shed light on the â€œblack boxâ€ of LLMs in production. Some of the leading observability tools in this sp\\\\n\\\\n---\\\\n\\\\nSource: 06f031e91c9c0236:cc35f490\\\\nheir applications in just a few lines of code and immediately gain end-to-end visibility and confidence in their modelâ€™s behavior. The tutorial illustrated how easy it is to get started with Weave and how it can be leveraged for tasks ranging from real-time monitoring to quality assurance and ethical compliance. Looking ahead, LLM observability tools will likely ... know they can catch mistakes and iterate quickly. Tools like W&B Weave are at the forefront of this effort, providing the means to monitor, debug, and improve LLM applications in one unified environment. By adopting LLM observability practices now, developers and organizations set themselves up for success â€” ensuring that their AI systems remain performant, trustworthy, and aligned with user needs as both the models and usage grow. As these observability tools evolve, they will undoubtedly play a central role in the future of AI deployments, helping us harness the full potential of LLMs while keeping a watchful eye on\\\\n\\\\n---\\\\n\\\\nSource: 9758ab006e75cff3:cc35f490\\\\n Weave, instrumented our LLM call with one line ( @weave.op ), ran the application to log data, optionally added an evaluation, and then ... bottlenecks in your LLM application. Suppose your app has higher-than-desired latency; by looking at traces, you might discover that a database lookup or a particular model call is the slow part. With that insight, you could try caching results, using a smaller model for that step, or otherwise improving the architecture. Likewise, Weaveâ€™s token usage tracking can show if certain prompts are excessively long (costly) â€” prompting you to optimize prompt formats to reduce token counts. Over time, you can use Weave metrics to verify that optimizations are effective (e.g., see average latency drop after a change, or a reduction ... when you have a robust observability tool in place. Continuous evaluation and model improvement: You can use Weave not just for monitoring but for ongoing evaluation even after deployment. Letâ€™s say you regularly update your\\\\n\\\\n---\\\\n\\\\nSource: d7c061743dc2e1ee:cc35f490\\\\n on LLM deployments. On top of all that, W&Bâ€™s focus on enterprise readiness (with secure cloud hosting, or on-prem deployment if needed, and compliance features) gives confidence to companies that need to monitor LLMs in production while meeting security requirements. In summary, W&B Weave is a preferred observability tool because it combines comprehensive tracing, real-time performance monitoring, automated output evaluation, and an intuitive UI in one platform. It effectively bridges the gap between model development ... enter your API key Once installed and authenticated, initialize Weave in your script or notebook by calling weave.init() at the start of your program. This sets up a new Weave project where all your logs and traces will be stored. For example: import weave # Initialize a Weave project (give a name for this observability run) weave.init(project=\"llm-observability-demo\") After this initialization, Weave is ready to capture data. It will associate all logs with the pro\\\\n\\\\n---\\\\n\\\\nSource: 2d3c24ba61c5ba18:cc35f490\\\\nas its own focus and strengths, but all aim to shed light on the â€œblack boxâ€ of LLMs in production. Some of the leading observability tools in this space include Lunary, LangSmith, and W&B Weave, among others. Lunary Lunary is an open-source platform that provides analytics and tracing for LLM-powered apps. Itâ€™s designed to be model-agnostic, meaning it can work with any LLM provider or architecture. Lunary focuses on ... and intermediate actions (like calls to tools or external APIs). This provides end-to-end tracing for chain-of-thought style applications. LangSmith also emphasizes prompt evaluation and testing. It allows developers to systematically evaluate their prompts and model outputs by using datasets of queries and expected answers. In the LangSmith dashboard, you can see how often your modelâ€™s responses match the expected results or where it might be failing. It supports scoring of outputs for accuracy, completeness, or custom metrics you define. Additionally, LangSmith prov\\\\n\\\\n---\\\\n\\\\nSource: e5ee1e1a6d3f0816:cc35f490\\\\nthen to an LLM, and then into some post-processing, ... means W&B Weave not only tells you that an answer was given, but also gives an indication of whether that answer was good or potentially problematic â€” all in real time. Few other observability tools have this level of integrated quality checking. Seamless integration and agnostic support are also key features of W&B Weave. It is designed to be framework-agnostic and model-agnostic: whether youâ€™re using OpenAIâ€™s GPT-4 via API, an open-source model on Hugging Face, or a LangChain application, Weave can plug in. The tool provides simple SDK methods and decorators to instrument your code. For instance, by adding a ... in a W&B report format) to communicate your findings. The collaborative features (permissions, team projects, comments) make it a strong choice for enterprise teams working on LLM deployments. On top of all that, W&Bâ€™s focus on enterprise readiness (with secure cloud hosting, or on-prem deployment if needed, and complian\\'}',\n",
       "       'system_prompt': 'You are a specialized agent in a larger workflow.\\n\\nYour role:\\n- PURPOSE: Design a clear structure for the article.\\n- GOAL: Turn the research summary into a logical outline suitable for a 2000-2500 word blog post.\\n- EXPECTED_RESULT: A title, a detailed description, and an ordered list of sections with headings and bullet points.\\n\\nGeneral behaviour:\\n- Be precise and concise unless the task requires extended output.\\n- Use clear structure (headings/bullets) when helpful.\\n- If you are calling tools, interpret their outputs carefully and explain your reasoning.\\n',\n",
       "       'temperature': 0.7,\n",
       "       'max_new_tokens': 512,\n",
       "       'stream': False},\n",
       "      'outputs': {'output': '**Title:**  \\n*Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems*\\n\\n**Description:**  \\nThis blog post explores the critical need for observability in large language model (LLM) systems. As LLMs become more integrated into production applications, the lack of visibility into their internal workings has led to significant challenges, including hallucinations, performance bottlenecks, and compliance risks. By adopting an observability-first approach, organizations can gain real-time insights into model behavior, trace requests end-to-end, measure performance metrics, and create feedback loops for continuous improvement. This article outlines the key components of an observability-first strategy, discusses common failure patterns in LLM systems, and highlights the role of tools and best practices in building transparent, reliable, and ethical AI systems.\\n\\n---\\n\\n### **Outline of the Blog Post**\\n\\n#### **1. Introduction: The Rise of LLMs and the Black Box Problem**\\n- **Overview of LLMs and Their Growing Role**  \\n  - Explain the rapid adoption of LLMs across industries.  \\n  - Highlight their complexity and the challenges of deploying them at scale.  \\n\\n- **The Black Box Conundrum**  \\n  - Define the \"black box\" problem in LLMs.  \\n  - Discuss the limitations of traditional monitoring and debugging approaches.  \\n  - Introduce the concept of **observability** as a solution.  \\n\\n- **Why Observability Matters Now**  \\n  - Link the increasing complexity of LLM systems to the need for deeper visibility.  \\n  - Preview the key components of observability: tracing, metrics, feedback loops, and failure analysis.  \\n\\n---\\n\\n#### **2. Understanding Observability in LLM Systems**\\n- **What is Observability?**  \\n  - Define observability in the context of LLMs.  \\n  - Differentiate between **observability** and **monitoring**.  \\n  - Emphasize the importance of **comprehensive visibility** into model behavior.  \\n\\n- **The Observability Stack for LLMs**  \\n  - Break down the key elements of an observability layer:  \\n    - **Tracing**: Tracking requests through the system.  \\n    - **Metrics**: Measuring performance and resource usage.  \\n    - **Feedback Loops**: Using data to improve model behavior.  \\n    - **Content Safety and Compliance**: Ensuring ethical and legal standards.  \\n\\n- **The Role of Observability in AI Ethics and Trust**  \\n  - Discuss how observability supports transparency'},\n",
       "      'meta': {},\n",
       "      'ok': True,\n",
       "      'error': None,\n",
       "      'logs': []}],\n",
       "    'meta': {'agent_type': 'generic_agent',\n",
       "     'agent_config': {'allow_input_pruning': True,\n",
       "      'repair_with_llm': True,\n",
       "      'strict_tool_call': False,\n",
       "      'temperature': 0.7,\n",
       "      'max_new_tokens': 512,\n",
       "      'return_stream_by_default': False,\n",
       "      'retry': {'max_route_retries': 2,\n",
       "       'max_tool_retries': 1,\n",
       "       'backoff_sec': 0.7},\n",
       "      'strict_json': True,\n",
       "      'max_json_repair_attempts': 1},\n",
       "     'model': '/home/nomi/workspace/Model_Weights/Qwen3-8B-unsloth-bnb-4bit',\n",
       "     'toolkit': False,\n",
       "     'log_steps': True}}},\n",
       "  'draft': {'node_id': 'draft',\n",
       "   'mode': <NodeMode.TEXT: 'text'>,\n",
       "   'raw_output': \"# Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems\\n\\n## Introduction: The Rise of LLMs and the Black Box Problem\\n\\nLarge language models (LLMs) have rapidly transformed the landscape of artificial intelligence, becoming integral to applications across industries such as finance, healthcare, customer service, and content creation. Their ability to understand and generate human-like text has made them a cornerstone of modern AI systems. However, as these models scale in complexity and usage, a critical challenge has emerged: the **black box problem**.\\n\\nThe black box problem refers to the lack of transparency in how LLMs process inputs, generate outputs, and make decisions. Unlike traditional machine learning models, which often have more interpretable structures, LLMs are inherently complex and opaque. This opacity makes it difficult for developers and stakeholders to understand how the model behaves, especially in production environments where reliability, safety, and compliance are paramount.\\n\\nTraditional monitoring and debugging approaches, which are well-suited for conventional software systems, fall short when applied to LLMs. These approaches typically focus on system-level metrics such as CPU usage, memory consumption, and response times. However, they fail to capture the internal behavior of the model itselfâ€”what the model is thinking, how it processes data, and whether it is generating safe, accurate, or ethical outputs.\\n\\nThis is where **observability** comes in. Observability is not just about monitoring; it is about **understanding** the behavior of a system in real time. For LLMs, observability means having the ability to track requests end-to-end, measure performance across all layers, and create feedback loops to continuously improve the model's behavior.\\n\\nIn this blog post, we will explore the concept of observability in the context of LLM systems. We will break down its key componentsâ€”tracing, metrics, feedback loops, and content safetyâ€”and discuss how they contribute to building transparent, reliable, and ethical AI systems. We will also examine common failure patterns in LLM systems and how observability can help address them.\\n\\nBy the end of this post, you will understand why observability is not just a nice-to-haveâ€”it is essential for the future of AI.\\n\\n---\\n\\n## Understanding Observability in LLM Systems\\n\\n### What is Observability?\\n\\nObservability is the ability to understand the internal state of a system based on its external outputs. In the context of LLM systems, observability goes beyond traditional monitoring. It involves **tracking the flow of requests through the model**, **measuring performance at every layer**, and **analyzing the model's behavior in real time**.\\n\\nUnlike monitoring, which is reactive and focuses on predefined metrics, observability is **proactive and holistic**. It provides a **comprehensive view of the system**, enabling developers to not only detect issues but also understand why they are happening.\\n\\nFor example, while monitoring might tell you that a model is returning incorrect answers, observability would tell you **why** the model is generating those answersâ€”whether it's due to a faulty prompt, a data bias, or a misalignment in the training data.\\n\\n### The Observability Stack for LLMs\\n\\nAn effective observability strategy for LLM systems consists of several key components. These components work together to provide a complete picture of the model's behavior and performance. Let's explore each of them:\\n\\n#### 1. Tracing\\n\\n**Tracing** is the process of tracking a request through the entire system, from the initial user input to the final output. In LLM systems, tracing is especially important because it allows developers to understand how the model processes inputs, interacts with external tools, and generates outputs.\\n\\nFor example, consider an application that uses an LLM to answer customer questions. The request might start with a user query, then pass through a database lookup, an API call, and finally the LLM itself. Tracing would allow developers to see the entire journey of the request, including any delays or errors that occurred at each step.\\n\\nTools like **LangSmith** and **W&B Weave** provide end-to-end tracing capabilities, allowing developers to visualize the flow of requests and identify performance bottlenecks.\\n\\n#### 2. Metrics\\n\\n**Metrics** are quantitative measures of system performance and behavior. In the context of LLM systems, metrics can include:\\n\\n- **Response latency**: How long it takes the model to generate a response.\\n- **Token usage**: The number of tokens processed by the model.\\n- **Error rates**: The percentage of requests that result in errors.\\n- **Throughput**: The number of requests handled per second.\\n- **Resource usage**: CPU, memory, and GPU usage.\\n\\nThese metrics provide valuable insights into the performance of the model and help developers identify issues such as latency spikes, resource exhaustion, or inefficient token usage.\\n\\nFor instance, if a model starts to take longer than expected to generate responses, a developer might use metrics to determine whether the issue is due to a slow API call, a misconfigured model, or an overload of incoming requests.\\n\\n#### 3. Feedback Loops\\n\\n**Feedback loops** are mechanisms that allow the model to learn from its own behavior. In the context of LLM systems, feedback loops involve **collecting data on model performance** and using it to **improve the model over time**.\\n\\nFor example, if a model generates incorrect answers, developers can use feedback loops to **retrain the model on the incorrect outputs**, helping it to avoid making the same mistakes in the future. This process is often referred to as **model fine-tuning** or **reinforcement learning from human feedback (RLHF)**.\\n\\nTools like **W&B Weave** and **LangSmith** provide features that enable developers to collect and analyze feedback data, making it easier to iterate on model improvements.\\n\\n#### 4. Content Safety and Compliance\\n\\nAs LLMs become more integrated into production applications, **content safety and compliance** have become a major concern. These systems must ensure that the model's outputs are **accurate, safe, and aligned with ethical and legal standards**.\\n\\nObservability plays a crucial role in addressing these concerns. By **monitoring the model's outputs in real time**, developers can detect and flag potentially harmful or unethical content. This includes identifying **hallucinations**, **bias**, **profanity**, and **inappropriate content**.\\n\\nFor example, a healthcare application using an LLM to generate patient summaries must ensure that the model does not disclose sensitive information or make false claims. Observability tools can help detect such issues by **scanning model outputs for PII (Personally Identifiable Information)** or **violations of ethical guidelines**.\\n\\nIn regulated industries, **observability also means maintaining an audit trail**. This ensures that if there is ever a question or incident, there is a record to review. Features like **PII redaction** and **access control** are essential for maintaining compliance in such environments.\\n\\n---\\n\\n## The Role of Observability in AI Ethics and Trust\\n\\n### Building Trust Through Transparency\\n\\nTrust is a fundamental component of any AI system. For users, developers, and stakeholders, trust is built through **transparency, reliability, and accountability**. Observability plays a crucial role in building trust by providing **visibility into the model's behavior**.\\n\\nWhen users know that an AI system is **transparent and accountable**, they are more likely to trust its outputs. This is especially important in high-stakes applications such as **healthcare, finance, and legal services**, where the consequences of incorrect outputs can be severe.\\n\\nObservability also helps build **trust among developers and data scientists**. By providing **real-time insights into model behavior**, observability enables developers to **debug issues more effectively** and **improve the model's performance over time**.\\n\\n### Ensuring Ethical AI\\n\\nEthical AI is not just about avoiding harm; it's also about **ensuring that AI systems are fair, just, and aligned with societal values**. Observability supports ethical AI by providing **insights into model behavior**, **detecting biases**, and **ensuring compliance with ethical guidelines**.\\n\\nFor example, if an LLM is used in a hiring application, it must be **free from biases that could disadvantage certain groups**. Observability tools can help detect such biases by **analyzing the model's outputs for fairness** and **flagging any discriminatory patterns**.\\n\\nIn addition, observability helps ensure that **AI systems are aligned with ethical guidelines**. For instance, if an LLM is used to generate content for a news website, it must be **free from misinformation and bias**. Observability tools can help detect such issues by **scanning model outputs for factual accuracy** and **flagging any potential misinformation**.\\n\\n---\\n\\n## Common Failure Patterns in LLM Systems\\n\\n### Hallucinations\\n\\nOne of the most common failure patterns in LLM systems is **hallucination**, where the model generates **false or made-up information**. This can be a significant issue, especially in applications where **accuracy is critical**.\\n\\nFor example, an LLM used in a customer support application might generate **incorrect information about product features**, leading to **confusion and dissatisfaction** among users. Observability tools can help detect hallucinations by **monitoring model outputs** and **flagging any inconsistencies**.\\n\\n### Performance Bottlenecks\\n\\nAs LLMs scale in complexity and usage, **performance bottlenecks** can emerge. These bottlenecks can be due to a variety of factors, including **slow API calls**, **inefficient token usage**, or **resource exhaustion**.\\n\\nObservability tools can help identify these bottlenecks by **tracking performance metrics** and **analyzing the flow of requests**. For example, if a model starts to take longer than expected to generate responses, a developer might use observability tools to **identify the slow API call** or **optimize token usage**.\\n\\n### Cost Overruns\\n\\nMany LLM services charge based on **token usage or compute time**, making **cost overruns** a significant concern. Without observability, it is difficult to **track and optimize costs**.\\n\\nObservability tools can help prevent cost overruns by **tracking token usage in real time** and **identifying any excessive usage**. For example, if a model is generating **long or unnecessary prompts**, observability tools can **flag these prompts** and **suggest optimizations**.\\n\\n---\\n\\n## The Future of Observability in AI\\n\\nAs LLMs continue to evolve and become more integrated into production systems, **observability will play an increasingly important role** in building **transparent, reliable, and ethical AI**.\\n\\nTools like **W&B Weave**, **LangSmith**, and **Lunary** are at the forefront of this movement, providing **comprehensive observability solutions** for LLM systems. These tools not only help developers **monitor and debug their models** but also enable them to **improve model performance over time**.\\n\\nIn the future, we can expect to see **more advanced observability tools** that provide **even deeper insights into model behavior**, **automated feedback loops**, and **real-time ethical compliance checks**. These tools will be essential for **building AI systems that are not only powerful but also trustworthy**.\\n\\n---\\n\\n## Conclusion\\n\\nIn conclusion, **observability is not just a technical necessityâ€”it is a strategic imperative** for the future of AI. As LLMs become more complex and widely used, **the need for transparency and accountability has never been greater**.\\n\\nBy adopting an **observability-first approach**, organizations can **gain real-time insights into model behavior**, **detect and fix issues quickly**, and **ensure that their AI systems are safe, reliable, and ethical**.\\n\\nIn the next section, we will explore **practical examples of observability in action**, including **how to implement observability in your own LLM projects** and **best practices for building observability-first AI systems**. Stay tuned for more insights on this critical topic.\",\n",
       "   'structured_output': None,\n",
       "   'tool_call_output': None,\n",
       "   'started_at': 1765473305.7463915,\n",
       "   'duration_ms': 52281,\n",
       "   'error': None,\n",
       "   'traces': {'steps': [{'step_id': 1,\n",
       "      'kind': 'agent',\n",
       "      'label': 'agent.run',\n",
       "      'node_id': None,\n",
       "      'agent_id': 'draft',\n",
       "      'started_at': 1765473305.748451,\n",
       "      'duration_ms': 52276,\n",
       "      'inputs': {'agent_type': 'Agent',\n",
       "       'has_toolkit': False,\n",
       "       'structured': False,\n",
       "       'stream': False,\n",
       "       'strict_tool_call': False},\n",
       "      'outputs': {},\n",
       "      'meta': {},\n",
       "      'ok': True,\n",
       "      'error': None,\n",
       "      'logs': []},\n",
       "     {'step_id': 2,\n",
       "      'kind': 'llm.call',\n",
       "      'label': 'agent.free_text_call',\n",
       "      'node_id': None,\n",
       "      'agent_id': 'draft',\n",
       "      'started_at': 1765473305.7492301,\n",
       "      'duration_ms': 52274,\n",
       "      'inputs': {'system_prompt_len': 619,\n",
       "       'user_prompt_len': 14362,\n",
       "       'user_prompt': 'Write a full draft of the blog post based on the provided outline and research, ensuring it is approximately 3000 words. Include clear headings, subheadings, and paragraphs that flow smoothly. Cover the introduction, the concept of observability, its components (tracing, metrics, feedback loops, and content safety), and the role of observability in AI ethics and trust. Use practical examples and insights from the research to support your arguments. Maintain a practical and slightly opinionated tone for an intermediate ML engineering audience.\\n\\n# Context from dependency nodes:\\n## outline\\n**Title:**  \\n*Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems*\\n\\n**Description:**  \\nThis blog post explores the critical need for observability in large language model (LLM) systems. As LLMs become more integrated into production applications, the lack of visibility into their internal workings has led to significant challenges, including hallucinations, performance bottlenecks, and compliance risks. By adopting an observability-first approach, organizations can gain real-time insights into model behavior, trace requests end-to-end, measure performance metrics, and create feedback loops for continuous improvement. This article outlines the key components of an observability-first strategy, discusses common failure patterns in LLM systems, and highlights the role of tools and best practices in building transparent, reliable, and ethical AI systems.\\n\\n---\\n\\n### **Outline of the Blog Post**\\n\\n#### **1. Introduction: The Rise of LLMs and the Black Box Problem**\\n- **Overview of LLMs and Their Growing Role**  \\n  - Explain the rapid adoption of LLMs across industries.  \\n  - Highlight their complexity and the challenges of deploying them at scale.  \\n\\n- **The Black Box Conundrum**  \\n  - Define the \"black box\" problem in LLMs.  \\n  - Discuss the limitations of traditional monitoring and debugging approaches.  \\n  - Introduce the concept of **observability** as a solution.  \\n\\n- **Why Observability Matters Now**  \\n  - Link the increasing complexity of LLM systems to the need for deeper visibility.  \\n  - Preview the key components of observability: tracing, metrics, feedback loops, and failure analysis.  \\n\\n---\\n\\n#### **2. Understanding Observability in LLM Systems**\\n- **What is Observability?**  \\n  - Define observability in the context of LLMs.  \\n  - Differentiate between **observability** and **monitoring**.  \\n  - Emphasize the importance of **comprehensive visibility** into model behavior.  \\n\\n- **The Observability Stack for LLMs**  \\n  - Break down the key elements of an observability layer:  \\n    - **Tracing**: Tracking requests through the system.  \\n    - **Metrics**: Measuring performance and resource usage.  \\n    - **Feedback Loops**: Using data to improve model behavior.  \\n    - **Content Safety and Compliance**: Ensuring ethical and legal standards.  \\n\\n- **The Role of Observability in AI Ethics and Trust**  \\n  - Discuss how observability supports transparency\\n\\n## research\\n{\\'query\\': \\'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.\\', \\'summary\\': \"Top 3 results out of ~113 results for: \\'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.\\'\\\\n1. LLM observability tools: Monitoring, debugging, and ... â€” https://medium.com/online-inference/llm-observability-tools-monitoring-debugging-and-improving-ai-systems-5af769796266\\\\n2. LLM Observability Explained: Prevent Hallucinations, ... â€” https://www.splunk.com/en_us/blog/learn/llm-observability.html\\\\n3. What Is LLM Observability? Use Cases and Best Practices â€” https://www.tredence.com/blog/llm-observability\", \\'provider\\': \\'serpapi\\', \\'elapsed_ms\\': 6055, \\'rag_content\\': \\'Source: de17dd0dd7eabff1:cc35f490\\\\nLLM observability tools: Monitoring, debugging, and improving AI systems Dave Davies 24 min read Â· Aug 22, 2025 -- Listen Share LLM observability is crucial for maintaining the operational health of applications powered by large language models. As AI systems become more complex and widely deployed, having visibility into their behavior is increasingly important. Observability tools provide comprehensive insight into all layers of an LLM-powered application, helping teams monitor performance, debug issues, and ensure outputs remain safe and reliable. Press enter or click to view image in full size By capturing everything from system metrics to model inputs and outputs, ... anomalies. This gives engineers actionable insights to fix inaccuracies, optimize performance, and uphold safety standards. In short, LLM observability creates a feedback loop that turns raw model behavior into useful information for improving your application. LLM observability vs. LLM monitoring Itâ€™s important to d\\\\n\\\\n---\\\\n\\\\nSource: 09ad15b0683646b8:cc35f490\\\\nck loop that turns raw model behavior into useful information for improving your application. LLM observability vs. LLM monitoring Itâ€™s important to distinguish observability from monitoring in the context of LLM applications. LLM monitoring typically focuses on the â€œwhatâ€ â€” it tracks key performance metrics and system health indicators to ensure the service is up and responsive. For example, monitoring will measure metrics such as API response time, error rates, request throughput, and token usage counts. If something goes ... wrong; observability helps you understand exactly what happened inside the system and how to fix it. Both are necessary â€” monitoring provides early warnings, while observability provides deep diagnosis â€” but observability offers the comprehensive visibility that LLM applications especially need. Challenges addressed by LLM observability tools Large language model applications face a range of challenges that traditional software doesnâ€™t, and LLM observability too\\\\n\\\\n---\\\\n\\\\nSource: a11a9e62cab8284b:cc35f490\\\\nt overruns are another issue addressed by observability. Many LLM services (such as OpenAIâ€™s API) charge based on token usage or compute time. Without ... observability solutions To solve the challenges above, an effective LLM observability solution provides several key capabilities. At the core is real-time performance monitoring across the entire system. This means the tool tracks metrics such as response latency, throughput of requests, error rates, and resource usage (CPU, memory, GPU, or API tokens) in real time. With these insights, teams can immediately spot performance degradation â€” for example, if the average response time of the model starts creeping up or if the error rate exceeds a threshold. Real-time monitoring ensures that any latency spikes, timeouts, or bottlenecks in the LLM or its ... tools integrate checks for content safety and compliance. This may involve scanning model outputs with profanity or hate speech detectors, bias evaluators, or using AI-driven content mo\\\\n\\\\n---\\\\n\\\\nSource: 6b08b29bc2b9891d:cc35f490\\\\nd by LLM observability tools Large language model applications face a range of challenges that traditional software doesnâ€™t, and LLM observability tools are designed to address these issues. One major problem is hallucinations â€” when an LLM generates answers that sound plausible but are factually incorrect or completely made-up. For example, an LLM might confidently provide a wrong ... external API call holding things up? LLM observability ties together data from all components, so teams can pinpoint if a particular step (like a database search for context or a third-party API call) is causing the delay. Similarly, failures or errors in a pipeline can be tricky to reproduce given the complexity of LLM systems. Observability tools trace each step of a request, making it easier to debug when something goes wrong in a multi-step process. Cost overruns are another issue addressed by observability. Many LLM services (such as OpenAIâ€™s API) charge based on token usage or compute time. Without\\\\n\\\\n---\\\\n\\\\nSource: 724c404f6a885694:cc35f490\\\\nsafety and compliance. This may involve scanning model outputs with profanity or hate speech detectors, bias evaluators, or using AI-driven content moderation APIs. A good observability platform might, for example, automatically tag any response that seems to contain personally identifiable information or that violates certain ethical guidelines. In regulated industries, observability also means keeping an audit trail â€” the tool should log all prompts and responses securely so that if thereâ€™s ever a question or incident, thereâ€™s a record to review. Features like PII redaction (masking sensitive user data in logs) and access control become ... LLM observability tools As the demand for LLM observability has grown, a number of tools and platforms have emerged to help developers monitor and debug their language model applications. Each tool has its own focus and strengths, but all aim to shed light on the â€œblack boxâ€ of LLMs in production. Some of the leading observability tools in this sp\\\\n\\\\n---\\\\n\\\\nSource: 06f031e91c9c0236:cc35f490\\\\nheir applications in just a few lines of code and immediately gain end-to-end visibility and confidence in their modelâ€™s behavior. The tutorial illustrated how easy it is to get started with Weave and how it can be leveraged for tasks ranging from real-time monitoring to quality assurance and ethical compliance. Looking ahead, LLM observability tools will likely ... know they can catch mistakes and iterate quickly. Tools like W&B Weave are at the forefront of this effort, providing the means to monitor, debug, and improve LLM applications in one unified environment. By adopting LLM observability practices now, developers and organizations set themselves up for success â€” ensuring that their AI systems remain performant, trustworthy, and aligned with user needs as both the models and usage grow. As these observability tools evolve, they will undoubtedly play a central role in the future of AI deployments, helping us harness the full potential of LLMs while keeping a watchful eye on\\\\n\\\\n---\\\\n\\\\nSource: 9758ab006e75cff3:cc35f490\\\\n Weave, instrumented our LLM call with one line ( @weave.op ), ran the application to log data, optionally added an evaluation, and then ... bottlenecks in your LLM application. Suppose your app has higher-than-desired latency; by looking at traces, you might discover that a database lookup or a particular model call is the slow part. With that insight, you could try caching results, using a smaller model for that step, or otherwise improving the architecture. Likewise, Weaveâ€™s token usage tracking can show if certain prompts are excessively long (costly) â€” prompting you to optimize prompt formats to reduce token counts. Over time, you can use Weave metrics to verify that optimizations are effective (e.g., see average latency drop after a change, or a reduction ... when you have a robust observability tool in place. Continuous evaluation and model improvement: You can use Weave not just for monitoring but for ongoing evaluation even after deployment. Letâ€™s say you regularly update your\\\\n\\\\n---\\\\n\\\\nSource: d7c061743dc2e1ee:cc35f490\\\\n on LLM deployments. On top of all that, W&Bâ€™s focus on enterprise readiness (with secure cloud hosting, or on-prem deployment if needed, and compliance features) gives confidence to companies that need to monitor LLMs in production while meeting security requirements. In summary, W&B Weave is a preferred observability tool because it combines comprehensive tracing, real-time performance monitoring, automated output evaluation, and an intuitive UI in one platform. It effectively bridges the gap between model development ... enter your API key Once installed and authenticated, initialize Weave in your script or notebook by calling weave.init() at the start of your program. This sets up a new Weave project where all your logs and traces will be stored. For example: import weave # Initialize a Weave project (give a name for this observability run) weave.init(project=\"llm-observability-demo\") After this initialization, Weave is ready to capture data. It will associate all logs with the pro\\\\n\\\\n---\\\\n\\\\nSource: 2d3c24ba61c5ba18:cc35f490\\\\nas its own focus and strengths, but all aim to shed light on the â€œblack boxâ€ of LLMs in production. Some of the leading observability tools in this space include Lunary, LangSmith, and W&B Weave, among others. Lunary Lunary is an open-source platform that provides analytics and tracing for LLM-powered apps. Itâ€™s designed to be model-agnostic, meaning it can work with any LLM provider or architecture. Lunary focuses on ... and intermediate actions (like calls to tools or external APIs). This provides end-to-end tracing for chain-of-thought style applications. LangSmith also emphasizes prompt evaluation and testing. It allows developers to systematically evaluate their prompts and model outputs by using datasets of queries and expected answers. In the LangSmith dashboard, you can see how often your modelâ€™s responses match the expected results or where it might be failing. It supports scoring of outputs for accuracy, completeness, or custom metrics you define. Additionally, LangSmith prov\\\\n\\\\n---\\\\n\\\\nSource: e5ee1e1a6d3f0816:cc35f490\\\\nthen to an LLM, and then into some post-processing, ... means W&B Weave not only tells you that an answer was given, but also gives an indication of whether that answer was good or potentially problematic â€” all in real time. Few other observability tools have this level of integrated quality checking. Seamless integration and agnostic support are also key features of W&B Weave. It is designed to be framework-agnostic and model-agnostic: whether youâ€™re using OpenAIâ€™s GPT-4 via API, an open-source model on Hugging Face, or a LangChain application, Weave can plug in. The tool provides simple SDK methods and decorators to instrument your code. For instance, by adding a ... in a W&B report format) to communicate your findings. The collaborative features (permissions, team projects, comments) make it a strong choice for enterprise teams working on LLM deployments. On top of all that, W&Bâ€™s focus on enterprise readiness (with secure cloud hosting, or on-prem deployment if needed, and complian\\'}',\n",
       "       'system_prompt': 'You are a specialized agent in a larger workflow.\\n\\nYour role:\\n- PURPOSE: Write the first full draft of the article of about 3000 words including every detail.\\n- GOAL: Use the outline and research to produce a readable, coherent blog draft with clear headings, examples, and smooth transitions.\\n\\n- EXPECTED_RESULT: A complete draft in markdown, including title, headings, and paragraphs.\\n\\nGeneral behaviour:\\n- Be precise and concise unless the task requires extended output.\\n- Use clear structure (headings/bullets) when helpful.\\n- If you are calling tools, interpret their outputs carefully and explain your reasoning.\\n',\n",
       "       'temperature': 0.7,\n",
       "       'max_new_tokens': 16000,\n",
       "       'stream': False},\n",
       "      'outputs': {'output': \"# Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems\\n\\n## Introduction: The Rise of LLMs and the Black Box Problem\\n\\nLarge language models (LLMs) have rapidly transformed the landscape of artificial intelligence, becoming integral to applications across industries such as finance, healthcare, customer service, and content creation. Their ability to understand and generate human-like text has made them a cornerstone of modern AI systems. However, as these models scale in complexity and usage, a critical challenge has emerged: the **black box problem**.\\n\\nThe black box problem refers to the lack of transparency in how LLMs process inputs, generate outputs, and make decisions. Unlike traditional machine learning models, which often have more interpretable structures, LLMs are inherently complex and opaque. This opacity makes it difficult for developers and stakeholders to understand how the model behaves, especially in production environments where reliability, safety, and compliance are paramount.\\n\\nTraditional monitoring and debugging approaches, which are well-suited for conventional software systems, fall short when applied to LLMs. These approaches typically focus on system-level metrics such as CPU usage, memory consumption, and response times. However, they fail to capture the internal behavior of the model itselfâ€”what the model is thinking, how it processes data, and whether it is generating safe, accurate, or ethical outputs.\\n\\nThis is where **observability** comes in. Observability is not just about monitoring; it is about **understanding** the behavior of a system in real time. For LLMs, observability means having the ability to track requests end-to-end, measure performance across all layers, and create feedback loops to continuously improve the model's behavior.\\n\\nIn this blog post, we will explore the concept of observability in the context of LLM systems. We will break down its key componentsâ€”tracing, metrics, feedback loops, and content safetyâ€”and discuss how they contribute to building transparent, reliable, and ethical AI systems. We will also examine common failure patterns in LLM systems and how observability can help address them.\\n\\nBy the end of this post, you will understand why observability is not just a nice-to-haveâ€”it is essential for the future of AI.\\n\\n---\\n\\n## Understanding Observability in LLM Systems\\n\\n### What is Observability?\\n\\nObservability is the ability to understand the internal state of a system based on its external outputs. In the context of LLM systems, observability goes beyond traditional monitoring. It involves **tracking the flow of requests through the model**, **measuring performance at every layer**, and **analyzing the model's behavior in real time**.\\n\\nUnlike monitoring, which is reactive and focuses on predefined metrics, observability is **proactive and holistic**. It provides a **comprehensive view of the system**, enabling developers to not only detect issues but also understand why they are happening.\\n\\nFor example, while monitoring might tell you that a model is returning incorrect answers, observability would tell you **why** the model is generating those answersâ€”whether it's due to a faulty prompt, a data bias, or a misalignment in the training data.\\n\\n### The Observability Stack for LLMs\\n\\nAn effective observability strategy for LLM systems consists of several key components. These components work together to provide a complete picture of the model's behavior and performance. Let's explore each of them:\\n\\n#### 1. Tracing\\n\\n**Tracing** is the process of tracking a request through the entire system, from the initial user input to the final output. In LLM systems, tracing is especially important because it allows developers to understand how the model processes inputs, interacts with external tools, and generates outputs.\\n\\nFor example, consider an application that uses an LLM to answer customer questions. The request might start with a user query, then pass through a database lookup, an API call, and finally the LLM itself. Tracing would allow developers to see the entire journey of the request, including any delays or errors that occurred at each step.\\n\\nTools like **LangSmith** and **W&B Weave** provide end-to-end tracing capabilities, allowing developers to visualize the flow of requests and identify performance bottlenecks.\\n\\n#### 2. Metrics\\n\\n**Metrics** are quantitative measures of system performance and behavior. In the context of LLM systems, metrics can include:\\n\\n- **Response latency**: How long it takes the model to generate a response.\\n- **Token usage**: The number of tokens processed by the model.\\n- **Error rates**: The percentage of requests that result in errors.\\n- **Throughput**: The number of requests handled per second.\\n- **Resource usage**: CPU, memory, and GPU usage.\\n\\nThese metrics provide valuable insights into the performance of the model and help developers identify issues such as latency spikes, resource exhaustion, or inefficient token usage.\\n\\nFor instance, if a model starts to take longer than expected to generate responses, a developer might use metrics to determine whether the issue is due to a slow API call, a misconfigured model, or an overload of incoming requests.\\n\\n#### 3. Feedback Loops\\n\\n**Feedback loops** are mechanisms that allow the model to learn from its own behavior. In the context of LLM systems, feedback loops involve **collecting data on model performance** and using it to **improve the model over time**.\\n\\nFor example, if a model generates incorrect answers, developers can use feedback loops to **retrain the model on the incorrect outputs**, helping it to avoid making the same mistakes in the future. This process is often referred to as **model fine-tuning** or **reinforcement learning from human feedback (RLHF)**.\\n\\nTools like **W&B Weave** and **LangSmith** provide features that enable developers to collect and analyze feedback data, making it easier to iterate on model improvements.\\n\\n#### 4. Content Safety and Compliance\\n\\nAs LLMs become more integrated into production applications, **content safety and compliance** have become a major concern. These systems must ensure that the model's outputs are **accurate, safe, and aligned with ethical and legal standards**.\\n\\nObservability plays a crucial role in addressing these concerns. By **monitoring the model's outputs in real time**, developers can detect and flag potentially harmful or unethical content. This includes identifying **hallucinations**, **bias**, **profanity**, and **inappropriate content**.\\n\\nFor example, a healthcare application using an LLM to generate patient summaries must ensure that the model does not disclose sensitive information or make false claims. Observability tools can help detect such issues by **scanning model outputs for PII (Personally Identifiable Information)** or **violations of ethical guidelines**.\\n\\nIn regulated industries, **observability also means maintaining an audit trail**. This ensures that if there is ever a question or incident, there is a record to review. Features like **PII redaction** and **access control** are essential for maintaining compliance in such environments.\\n\\n---\\n\\n## The Role of Observability in AI Ethics and Trust\\n\\n### Building Trust Through Transparency\\n\\nTrust is a fundamental component of any AI system. For users, developers, and stakeholders, trust is built through **transparency, reliability, and accountability**. Observability plays a crucial role in building trust by providing **visibility into the model's behavior**.\\n\\nWhen users know that an AI system is **transparent and accountable**, they are more likely to trust its outputs. This is especially important in high-stakes applications such as **healthcare, finance, and legal services**, where the consequences of incorrect outputs can be severe.\\n\\nObservability also helps build **trust among developers and data scientists**. By providing **real-time insights into model behavior**, observability enables developers to **debug issues more effectively** and **improve the model's performance over time**.\\n\\n### Ensuring Ethical AI\\n\\nEthical AI is not just about avoiding harm; it's also about **ensuring that AI systems are fair, just, and aligned with societal values**. Observability supports ethical AI by providing **insights into model behavior**, **detecting biases**, and **ensuring compliance with ethical guidelines**.\\n\\nFor example, if an LLM is used in a hiring application, it must be **free from biases that could disadvantage certain groups**. Observability tools can help detect such biases by **analyzing the model's outputs for fairness** and **flagging any discriminatory patterns**.\\n\\nIn addition, observability helps ensure that **AI systems are aligned with ethical guidelines**. For instance, if an LLM is used to generate content for a news website, it must be **free from misinformation and bias**. Observability tools can help detect such issues by **scanning model outputs for factual accuracy** and **flagging any potential misinformation**.\\n\\n---\\n\\n## Common Failure Patterns in LLM Systems\\n\\n### Hallucinations\\n\\nOne of the most common failure patterns in LLM systems is **hallucination**, where the model generates **false or made-up information**. This can be a significant issue, especially in applications where **accuracy is critical**.\\n\\nFor example, an LLM used in a customer support application might generate **incorrect information about product features**, leading to **confusion and dissatisfaction** among users. Observability tools can help detect hallucinations by **monitoring model outputs** and **flagging any inconsistencies**.\\n\\n### Performance Bottlenecks\\n\\nAs LLMs scale in complexity and usage, **performance bottlenecks** can emerge. These bottlenecks can be due to a variety of factors, including **slow API calls**, **inefficient token usage**, or **resource exhaustion**.\\n\\nObservability tools can help identify these bottlenecks by **tracking performance metrics** and **analyzing the flow of requests**. For example, if a model starts to take longer than expected to generate responses, a developer might use observability tools to **identify the slow API call** or **optimize token usage**.\\n\\n### Cost Overruns\\n\\nMany LLM services charge based on **token usage or compute time**, making **cost overruns** a significant concern. Without observability, it is difficult to **track and optimize costs**.\\n\\nObservability tools can help prevent cost overruns by **tracking token usage in real time** and **identifying any excessive usage**. For example, if a model is generating **long or unnecessary prompts**, observability tools can **flag these prompts** and **suggest optimizations**.\\n\\n---\\n\\n## The Future of Observability in AI\\n\\nAs LLMs continue to evolve and become more integrated into production systems, **observability will play an increasingly important role** in building **transparent, reliable, and ethical AI**.\\n\\nTools like **W&B Weave**, **LangSmith**, and **Lunary** are at the forefront of this movement, providing **comprehensive observability solutions** for LLM systems. These tools not only help developers **monitor and debug their models** but also enable them to **improve model performance over time**.\\n\\nIn the future, we can expect to see **more advanced observability tools** that provide **even deeper insights into model behavior**, **automated feedback loops**, and **real-time ethical compliance checks**. These tools will be essential for **building AI systems that are not only powerful but also trustworthy**.\\n\\n---\\n\\n## Conclusion\\n\\nIn conclusion, **observability is not just a technical necessityâ€”it is a strategic imperative** for the future of AI. As LLMs become more complex and widely used, **the need for transparency and accountability has never been greater**.\\n\\nBy adopting an **observability-first approach**, organizations can **gain real-time insights into model behavior**, **detect and fix issues quickly**, and **ensure that their AI systems are safe, reliable, and ethical**.\\n\\nIn the next section, we will explore **practical examples of observability in action**, including **how to implement observability in your own LLM projects** and **best practices for building observability-first AI systems**. Stay tuned for more insights on this critical topic.\"},\n",
       "      'meta': {},\n",
       "      'ok': True,\n",
       "      'error': None,\n",
       "      'logs': []}],\n",
       "    'meta': {'agent_type': 'generic_agent',\n",
       "     'agent_config': {'allow_input_pruning': True,\n",
       "      'repair_with_llm': True,\n",
       "      'strict_tool_call': False,\n",
       "      'temperature': 0.7,\n",
       "      'max_new_tokens': 16000,\n",
       "      'return_stream_by_default': False,\n",
       "      'retry': {'max_route_retries': 2,\n",
       "       'max_tool_retries': 1,\n",
       "       'backoff_sec': 0.7},\n",
       "      'strict_json': True,\n",
       "      'max_json_repair_attempts': 1},\n",
       "     'model': '/home/nomi/workspace/Model_Weights/Qwen3-8B-unsloth-bnb-4bit',\n",
       "     'toolkit': False,\n",
       "     'log_steps': True}}},\n",
       "  'review': {'node_id': 'review',\n",
       "   'mode': <NodeMode.STRUCTURED: 'structured'>,\n",
       "   'raw_output': '# Review of the Blog Post: \"Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems\"\\n\\n---\\n\\n## âœ… Strengths\\n\\n- **Clear structure**: The blog post is well-organized with logical sections and a clear progression from introduction to conclusion.\\n- **Relevant content**: It effectively covers the core aspects of observability in LLM systems, including tracing, metrics, feedback loops, and content safety.\\n- **Practical tone**: The tone is practical and slightly opinionated, as requested, making it accessible for intermediate ML engineers.\\n- **Use of examples**: The post uses concrete examples (e.g., hallucinations, performance bottlenecks, cost overruns) to illustrate key points.\\n- **Actionable insights**: The post provides actionable insights for developers, such as using tools like LangSmith, W&B Weave, and Lunary.\\n- **Alignment with query**: The content thoroughly addresses the importance of observability in LLM systems and covers all requested components (tracing, metrics, feedback loops, failure patterns).\\n\\n---\\n\\n## ðŸš¨ Issues and Suggestions\\n\\n### 1. **Factual Inaccuracies and Missing Explanations**\\n\\n- **Tool Mentioned Without Context**: The post mentions tools like **LangSmith**, **W&B Weave**, and **Lunary**, but it doesnâ€™t explain what they are or how they are used in the context of LLM observability. This could confuse readers who are unfamiliar with these tools.  \\n  **Suggestion**: Add brief descriptions of each tool and its role in LLM observability, or provide links to their documentation for further reading.\\n\\n- **Overgeneralization of Observability**: The post uses the term \"observability\" broadly, but it doesnâ€™t clearly differentiate between **observability**, **monitoring**, and **debugging**. This can lead to confusion.  \\n  **Suggestion**: Clarify the distinction between these concepts, especially in the section on \"LLM Observability vs. LLM Monitoring.\"\\n\\n- **Limited Discussion on Feedback Loops**: While the post mentions feedback loops, it doesnâ€™t go into detail about how they are implemented or their impact on model behavior.  \\n  **Suggestion**: Expand on the concept of feedback loops, including examples like **reinforcement learning from human feedback (RLHF)** and **active learning**.\\n\\n- **Ambiguity Around \"Common Failure Patterns\"**: The post lists failure patterns (e.g., hallucinations, performance bottlenecks, cost overruns) but doesnâ€™t provide enough depth on how observability helps detect or mitigate these issues.  \\n  **Suggestion**: Include specific examples of how observability tools detect and address these failure patterns (e.g., how a tool identifies a hallucination and triggers a model retraining).\\n\\n---\\n\\n### 2. **Stylistic and Clarity Issues**\\n\\n- **Repetition of Terms**: The term \"observability\" is used repeatedly, which can make the post feel redundant.  \\n  **Suggestion**: Vary the language to avoid repetition. For example, use synonyms like \"visibility,\" \"diagnostic insights,\" or \"behavioral analysis.\"\\n\\n- **Unclear Transition Between Sections**: The transition from the section on \"Observability Stack for LLMs\" to \"The Role of Observability in AI Ethics and Trust\" is abrupt.  \\n  **Suggestion**: Add a brief paragraph or sentence that connects these sections, explaining how observability supports both technical and ethical aspects of AI.\\n\\n- **Lack of Visual Aids**: The post is text-heavy and lacks visual aids such as diagrams or charts that could help explain complex concepts like tracing or feedback loops.  \\n  **Suggestion**: Suggest including visual aids (e.g., flowcharts of a request lifecycle, graphs of performance metrics) in future versions of the post.\\n\\n- **Some Sentences Are Too Long**: Some sentences are long and complex, making them difficult to parse.  \\n  **Suggestion**: Break up long sentences into shorter ones for better readability. For example:\\n  > \"While monitoring might tell you that a model is returning incorrect answers, observability would tell you why the model is generating those answersâ€”whether it\\'s due to a faulty prompt, a data bias, or a misalignment in the training data.\"\\n  â†’\\n  > \"While monitoring might tell you that a model is returning incorrect answers, observability goes further. It tells you why the model is generating those answers. The cause could be a faulty prompt, data bias, or a misalignment in the training data.\"\\n\\n---\\n\\n### 3. **Unclarified Concepts**\\n\\n- **\"Feedback Loops\"**: The term is used without a clear definition. The post mentions that feedback loops involve collecting data on model performance and using it to improve the model, but it doesnâ€™t explain how this process works in practice.  \\n  **Suggestion**: Define \"feedback loops\" more clearly, perhaps with an example such as:\\n  > \"A feedback loop in the context of LLMs involves collecting data on model performance (e.g., incorrect outputs, bias) and using that data to retrain or fine-tune the model. This process is often referred to as reinforcement learning from human feedback (RLHF).\"\\n\\n- **\"Content Safety and Compliance\"**: The post touches on content safety and compliance but doesnâ€™t provide enough detail on how observability tools enforce these standards.  \\n  **Suggestion**: Expand on the role of observability in content safety, including examples of how tools detect and flag problematic content (e.g., profanity, bias, hallucinations).\\n\\n---\\n\\n## âœ… Summary of Actionable Edits\\n\\n| Issue | Suggestion |\\n|------|------------|\\n| Tool descriptions are too vague | Add brief descriptions of tools like LangSmith, W&B Weave, and Lunary |\\n| Overgeneralization of observability | Clarify the difference between observability, monitoring, and debugging |\\n| Limited discussion on feedback loops | Expand on how feedback loops are implemented and their impact |\\n| Ambiguity around failure patterns | Provide specific examples of how observability detects and mitigates failure patterns |\\n| Repetition of terms | Vary the language to avoid repetition |\\n| Unclear transitions between sections | Add connecting sentences between sections |\\n| Lack of visual aids | Suggest including diagrams or charts for better clarity |\\n| Long sentences | Break up long sentences for better readability |\\n| Unclarified concepts | Define \"feedback loops\" and \"content safety\" more clearly |\\n\\n---\\n\\n## âœ… Final Assessment\\n\\nThe blog post is well-written, informative, and aligned with the original query. It provides a clear argument for the importance of observability in LLM systems and covers the key components (tracing, metrics, feedback loops, content safety). However, there are opportunities for improvement in terms of clarity, structure, and depth of explanation. With the suggested edits, the post can be made even more effective for its target audience of intermediate ML engineers.',\n",
       "   'structured_output': None,\n",
       "   'tool_call_output': None,\n",
       "   'started_at': 1765473361.3331904,\n",
       "   'duration_ms': 31056,\n",
       "   'error': None,\n",
       "   'traces': {'steps': [{'step_id': 1,\n",
       "      'kind': 'agent',\n",
       "      'label': 'agent.run',\n",
       "      'node_id': None,\n",
       "      'agent_id': 'review',\n",
       "      'started_at': 1765473361.3351858,\n",
       "      'duration_ms': 31052,\n",
       "      'inputs': {'agent_type': 'Agent',\n",
       "       'has_toolkit': False,\n",
       "       'structured': False,\n",
       "       'stream': False,\n",
       "       'strict_tool_call': False},\n",
       "      'outputs': {},\n",
       "      'meta': {},\n",
       "      'ok': True,\n",
       "      'error': None,\n",
       "      'logs': []},\n",
       "     {'step_id': 2,\n",
       "      'kind': 'llm.call',\n",
       "      'label': 'agent.free_text_call',\n",
       "      'node_id': None,\n",
       "      'agent_id': 'review',\n",
       "      'started_at': 1765473361.335976,\n",
       "      'duration_ms': 31050,\n",
       "      'inputs': {'system_prompt_len': 583,\n",
       "       'user_prompt_len': 24253,\n",
       "       'user_prompt': 'Review the draft blog post for factual accuracy, clarity, and alignment with the original query. Identify any technical inaccuracies, missing explanations, or stylistic issues. Provide actionable suggestions for improvement and mark any unclear sections. Ensure the content thoroughly addresses the importance of observability in LLM systems, including tracing, metrics, feedback loops, and common failure patterns, while maintaining a practical and slightly opinionated tone for intermediate ML engineers.\\n\\n# Context from dependency nodes:\\n## draft\\n# Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems\\n\\n## Introduction: The Rise of LLMs and the Black Box Problem\\n\\nLarge language models (LLMs) have rapidly transformed the landscape of artificial intelligence, becoming integral to applications across industries such as finance, healthcare, customer service, and content creation. Their ability to understand and generate human-like text has made them a cornerstone of modern AI systems. However, as these models scale in complexity and usage, a critical challenge has emerged: the **black box problem**.\\n\\nThe black box problem refers to the lack of transparency in how LLMs process inputs, generate outputs, and make decisions. Unlike traditional machine learning models, which often have more interpretable structures, LLMs are inherently complex and opaque. This opacity makes it difficult for developers and stakeholders to understand how the model behaves, especially in production environments where reliability, safety, and compliance are paramount.\\n\\nTraditional monitoring and debugging approaches, which are well-suited for conventional software systems, fall short when applied to LLMs. These approaches typically focus on system-level metrics such as CPU usage, memory consumption, and response times. However, they fail to capture the internal behavior of the model itselfâ€”what the model is thinking, how it processes data, and whether it is generating safe, accurate, or ethical outputs.\\n\\nThis is where **observability** comes in. Observability is not just about monitoring; it is about **understanding** the behavior of a system in real time. For LLMs, observability means having the ability to track requests end-to-end, measure performance across all layers, and create feedback loops to continuously improve the model\\'s behavior.\\n\\nIn this blog post, we will explore the concept of observability in the context of LLM systems. We will break down its key componentsâ€”tracing, metrics, feedback loops, and content safetyâ€”and discuss how they contribute to building transparent, reliable, and ethical AI systems. We will also examine common failure patterns in LLM systems and how observability can help address them.\\n\\nBy the end of this post, you will understand why observability is not just a nice-to-haveâ€”it is essential for the future of AI.\\n\\n---\\n\\n## Understanding Observability in LLM Systems\\n\\n### What is Observability?\\n\\nObservability is the ability to understand the internal state of a system based on its external outputs. In the context of LLM systems, observability goes beyond traditional monitoring. It involves **tracking the flow of requests through the model**, **measuring performance at every layer**, and **analyzing the model\\'s behavior in real time**.\\n\\nUnlike monitoring, which is reactive and focuses on predefined metrics, observability is **proactive and holistic**. It provides a **comprehensive view of the system**, enabling developers to not only detect issues but also understand why they are happening.\\n\\nFor example, while monitoring might tell you that a model is returning incorrect answers, observability would tell you **why** the model is generating those answersâ€”whether it\\'s due to a faulty prompt, a data bias, or a misalignment in the training data.\\n\\n### The Observability Stack for LLMs\\n\\nAn effective observability strategy for LLM systems consists of several key components. These components work together to provide a complete picture of the model\\'s behavior and performance. Let\\'s explore each of them:\\n\\n#### 1. Tracing\\n\\n**Tracing** is the process of tracking a request through the entire system, from the initial user input to the final output. In LLM systems, tracing is especially important because it allows developers to understand how the model processes inputs, interacts with external tools, and generates outputs.\\n\\nFor example, consider an application that uses an LLM to answer customer questions. The request might start with a user query, then pass through a database lookup, an API call, and finally the LLM itself. Tracing would allow developers to see the entire journey of the request, including any delays or errors that occurred at each step.\\n\\nTools like **LangSmith** and **W&B Weave** provide end-to-end tracing capabilities, allowing developers to visualize the flow of requests and identify performance bottlenecks.\\n\\n#### 2. Metrics\\n\\n**Metrics** are quantitative measures of system performance and behavior. In the context of LLM systems, metrics can include:\\n\\n- **Response latency**: How long it takes the model to generate a response.\\n- **Token usage**: The number of tokens processed by the model.\\n- **Error rates**: The percentage of requests that result in errors.\\n- **Throughput**: The number of requests handled per second.\\n- **Resource usage**: CPU, memory, and GPU usage.\\n\\nThese metrics provide valuable insights into the performance of the model and help developers identify issues such as latency spikes, resource exhaustion, or inefficient token usage.\\n\\nFor instance, if a model starts to take longer than expected to generate responses, a developer might use metrics to determine whether the issue is due to a slow API call, a misconfigured model, or an overload of incoming requests.\\n\\n#### 3. Feedback Loops\\n\\n**Feedback loops** are mechanisms that allow the model to learn from its own behavior. In the context of LLM systems, feedback loops involve **collecting data on model performance** and using it to **improve the model over time**.\\n\\nFor example, if a model generates incorrect answers, developers can use feedback loops to **retrain the model on the incorrect outputs**, helping it to avoid making the same mistakes in the future. This process is often referred to as **model fine-tuning** or **reinforcement learning from human feedback (RLHF)**.\\n\\nTools like **W&B Weave** and **LangSmith** provide features that enable developers to collect and analyze feedback data, making it easier to iterate on model improvements.\\n\\n#### 4. Content Safety and Compliance\\n\\nAs LLMs become more integrated into production applications, **content safety and compliance** have become a major concern. These systems must ensure that the model\\'s outputs are **accurate, safe, and aligned with ethical and legal standards**.\\n\\nObservability plays a crucial role in addressing these concerns. By **monitoring the model\\'s outputs in real time**, developers can detect and flag potentially harmful or unethical content. This includes identifying **hallucinations**, **bias**, **profanity**, and **inappropriate content**.\\n\\nFor example, a healthcare application using an LLM to generate patient summaries must ensure that the model does not disclose sensitive information or make false claims. Observability tools can help detect such issues by **scanning model outputs for PII (Personally Identifiable Information)** or **violations of ethical guidelines**.\\n\\nIn regulated industries, **observability also means maintaining an audit trail**. This ensures that if there is ever a question or incident, there is a record to review. Features like **PII redaction** and **access control** are essential for maintaining compliance in such environments.\\n\\n---\\n\\n## The Role of Observability in AI Ethics and Trust\\n\\n### Building Trust Through Transparency\\n\\nTrust is a fundamental component of any AI system. For users, developers, and stakeholders, trust is built through **transparency, reliability, and accountability**. Observability plays a crucial role in building trust by providing **visibility into the model\\'s behavior**.\\n\\nWhen users know that an AI system is **transparent and accountable**, they are more likely to trust its outputs. This is especially important in high-stakes applications such as **healthcare, finance, and legal services**, where the consequences of incorrect outputs can be severe.\\n\\nObservability also helps build **trust among developers and data scientists**. By providing **real-time insights into model behavior**, observability enables developers to **debug issues more effectively** and **improve the model\\'s performance over time**.\\n\\n### Ensuring Ethical AI\\n\\nEthical AI is not just about avoiding harm; it\\'s also about **ensuring that AI systems are fair, just, and aligned with societal values**. Observability supports ethical AI by providing **insights into model behavior**, **detecting biases**, and **ensuring compliance with ethical guidelines**.\\n\\nFor example, if an LLM is used in a hiring application, it must be **free from biases that could disadvantage certain groups**. Observability tools can help detect such biases by **analyzing the model\\'s outputs for fairness** and **flagging any discriminatory patterns**.\\n\\nIn addition, observability helps ensure that **AI systems are aligned with ethical guidelines**. For instance, if an LLM is used to generate content for a news website, it must be **free from misinformation and bias**. Observability tools can help detect such issues by **scanning model outputs for factual accuracy** and **flagging any potential misinformation**.\\n\\n---\\n\\n## Common Failure Patterns in LLM Systems\\n\\n### Hallucinations\\n\\nOne of the most common failure patterns in LLM systems is **hallucination**, where the model generates **false or made-up information**. This can be a significant issue, especially in applications where **accuracy is critical**.\\n\\nFor example, an LLM used in a customer support application might generate **incorrect information about product features**, leading to **confusion and dissatisfaction** among users. Observability tools can help detect hallucinations by **monitoring model outputs** and **flagging any inconsistencies**.\\n\\n### Performance Bottlenecks\\n\\nAs LLMs scale in complexity and usage, **performance bottlenecks** can emerge. These bottlenecks can be due to a variety of factors, including **slow API calls**, **inefficient token usage**, or **resource exhaustion**.\\n\\nObservability tools can help identify these bottlenecks by **tracking performance metrics** and **analyzing the flow of requests**. For example, if a model starts to take longer than expected to generate responses, a developer might use observability tools to **identify the slow API call** or **optimize token usage**.\\n\\n### Cost Overruns\\n\\nMany LLM services charge based on **token usage or compute time**, making **cost overruns** a significant concern. Without observability, it is difficult to **track and optimize costs**.\\n\\nObservability tools can help prevent cost overruns by **tracking token usage in real time** and **identifying any excessive usage**. For example, if a model is generating **long or unnecessary prompts**, observability tools can **flag these prompts** and **suggest optimizations**.\\n\\n---\\n\\n## The Future of Observability in AI\\n\\nAs LLMs continue to evolve and become more integrated into production systems, **observability will play an increasingly important role** in building **transparent, reliable, and ethical AI**.\\n\\nTools like **W&B Weave**, **LangSmith**, and **Lunary** are at the forefront of this movement, providing **comprehensive observability solutions** for LLM systems. These tools not only help developers **monitor and debug their models** but also enable them to **improve model performance over time**.\\n\\nIn the future, we can expect to see **more advanced observability tools** that provide **even deeper insights into model behavior**, **automated feedback loops**, and **real-time ethical compliance checks**. These tools will be essential for **building AI systems that are not only powerful but also trustworthy**.\\n\\n---\\n\\n## Conclusion\\n\\nIn conclusion, **observability is not just a technical necessityâ€”it is a strategic imperative** for the future of AI. As LLMs become more complex and widely used, **the need for transparency and accountability has never been greater**.\\n\\nBy adopting an **observability-first approach**, organizations can **gain real-time insights into model behavior**, **detect and fix issues quickly**, and **ensure that their AI systems are safe, reliable, and ethical**.\\n\\nIn the next section, we will explore **practical examples of observability in action**, including **how to implement observability in your own LLM projects** and **best practices for building observability-first AI systems**. Stay tuned for more insights on this critical topic.\\n\\n## research\\n{\\'query\\': \\'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.\\', \\'summary\\': \"Top 3 results out of ~113 results for: \\'Compose a 1000-1500 word blog arguing why observability layers are essential in modern LLM systems, including tracing, metrics, feedback loops, and common failure patterns.\\'\\\\n1. LLM observability tools: Monitoring, debugging, and ... â€” https://medium.com/online-inference/llm-observability-tools-monitoring-debugging-and-improving-ai-systems-5af769796266\\\\n2. LLM Observability Explained: Prevent Hallucinations, ... â€” https://www.splunk.com/en_us/blog/learn/llm-observability.html\\\\n3. What Is LLM Observability? Use Cases and Best Practices â€” https://www.tredence.com/blog/llm-observability\", \\'provider\\': \\'serpapi\\', \\'elapsed_ms\\': 6055, \\'rag_content\\': \\'Source: de17dd0dd7eabff1:cc35f490\\\\nLLM observability tools: Monitoring, debugging, and improving AI systems Dave Davies 24 min read Â· Aug 22, 2025 -- Listen Share LLM observability is crucial for maintaining the operational health of applications powered by large language models. As AI systems become more complex and widely deployed, having visibility into their behavior is increasingly important. Observability tools provide comprehensive insight into all layers of an LLM-powered application, helping teams monitor performance, debug issues, and ensure outputs remain safe and reliable. Press enter or click to view image in full size By capturing everything from system metrics to model inputs and outputs, ... anomalies. This gives engineers actionable insights to fix inaccuracies, optimize performance, and uphold safety standards. In short, LLM observability creates a feedback loop that turns raw model behavior into useful information for improving your application. LLM observability vs. LLM monitoring Itâ€™s important to d\\\\n\\\\n---\\\\n\\\\nSource: 09ad15b0683646b8:cc35f490\\\\nck loop that turns raw model behavior into useful information for improving your application. LLM observability vs. LLM monitoring Itâ€™s important to distinguish observability from monitoring in the context of LLM applications. LLM monitoring typically focuses on the â€œwhatâ€ â€” it tracks key performance metrics and system health indicators to ensure the service is up and responsive. For example, monitoring will measure metrics such as API response time, error rates, request throughput, and token usage counts. If something goes ... wrong; observability helps you understand exactly what happened inside the system and how to fix it. Both are necessary â€” monitoring provides early warnings, while observability provides deep diagnosis â€” but observability offers the comprehensive visibility that LLM applications especially need. Challenges addressed by LLM observability tools Large language model applications face a range of challenges that traditional software doesnâ€™t, and LLM observability too\\\\n\\\\n---\\\\n\\\\nSource: a11a9e62cab8284b:cc35f490\\\\nt overruns are another issue addressed by observability. Many LLM services (such as OpenAIâ€™s API) charge based on token usage or compute time. Without ... observability solutions To solve the challenges above, an effective LLM observability solution provides several key capabilities. At the core is real-time performance monitoring across the entire system. This means the tool tracks metrics such as response latency, throughput of requests, error rates, and resource usage (CPU, memory, GPU, or API tokens) in real time. With these insights, teams can immediately spot performance degradation â€” for example, if the average response time of the model starts creeping up or if the error rate exceeds a threshold. Real-time monitoring ensures that any latency spikes, timeouts, or bottlenecks in the LLM or its ... tools integrate checks for content safety and compliance. This may involve scanning model outputs with profanity or hate speech detectors, bias evaluators, or using AI-driven content mo\\\\n\\\\n---\\\\n\\\\nSource: 6b08b29bc2b9891d:cc35f490\\\\nd by LLM observability tools Large language model applications face a range of challenges that traditional software doesnâ€™t, and LLM observability tools are designed to address these issues. One major problem is hallucinations â€” when an LLM generates answers that sound plausible but are factually incorrect or completely made-up. For example, an LLM might confidently provide a wrong ... external API call holding things up? LLM observability ties together data from all components, so teams can pinpoint if a particular step (like a database search for context or a third-party API call) is causing the delay. Similarly, failures or errors in a pipeline can be tricky to reproduce given the complexity of LLM systems. Observability tools trace each step of a request, making it easier to debug when something goes wrong in a multi-step process. Cost overruns are another issue addressed by observability. Many LLM services (such as OpenAIâ€™s API) charge based on token usage or compute time. Without\\\\n\\\\n---\\\\n\\\\nSource: 724c404f6a885694:cc35f490\\\\nsafety and compliance. This may involve scanning model outputs with profanity or hate speech detectors, bias evaluators, or using AI-driven content moderation APIs. A good observability platform might, for example, automatically tag any response that seems to contain personally identifiable information or that violates certain ethical guidelines. In regulated industries, observability also means keeping an audit trail â€” the tool should log all prompts and responses securely so that if thereâ€™s ever a question or incident, thereâ€™s a record to review. Features like PII redaction (masking sensitive user data in logs) and access control become ... LLM observability tools As the demand for LLM observability has grown, a number of tools and platforms have emerged to help developers monitor and debug their language model applications. Each tool has its own focus and strengths, but all aim to shed light on the â€œblack boxâ€ of LLMs in production. Some of the leading observability tools in this sp\\\\n\\\\n---\\\\n\\\\nSource: 06f031e91c9c0236:cc35f490\\\\nheir applications in just a few lines of code and immediately gain end-to-end visibility and confidence in their modelâ€™s behavior. The tutorial illustrated how easy it is to get started with Weave and how it can be leveraged for tasks ranging from real-time monitoring to quality assurance and ethical compliance. Looking ahead, LLM observability tools will likely ... know they can catch mistakes and iterate quickly. Tools like W&B Weave are at the forefront of this effort, providing the means to monitor, debug, and improve LLM applications in one unified environment. By adopting LLM observability practices now, developers and organizations set themselves up for success â€” ensuring that their AI systems remain performant, trustworthy, and aligned with user needs as both the models and usage grow. As these observability tools evolve, they will undoubtedly play a central role in the future of AI deployments, helping us harness the full potential of LLMs while keeping a watchful eye on\\\\n\\\\n---\\\\n\\\\nSource: 9758ab006e75cff3:cc35f490\\\\n Weave, instrumented our LLM call with one line ( @weave.op ), ran the application to log data, optionally added an evaluation, and then ... bottlenecks in your LLM application. Suppose your app has higher-than-desired latency; by looking at traces, you might discover that a database lookup or a particular model call is the slow part. With that insight, you could try caching results, using a smaller model for that step, or otherwise improving the architecture. Likewise, Weaveâ€™s token usage tracking can show if certain prompts are excessively long (costly) â€” prompting you to optimize prompt formats to reduce token counts. Over time, you can use Weave metrics to verify that optimizations are effective (e.g., see average latency drop after a change, or a reduction ... when you have a robust observability tool in place. Continuous evaluation and model improvement: You can use Weave not just for monitoring but for ongoing evaluation even after deployment. Letâ€™s say you regularly update your\\\\n\\\\n---\\\\n\\\\nSource: d7c061743dc2e1ee:cc35f490\\\\n on LLM deployments. On top of all that, W&Bâ€™s focus on enterprise readiness (with secure cloud hosting, or on-prem deployment if needed, and compliance features) gives confidence to companies that need to monitor LLMs in production while meeting security requirements. In summary, W&B Weave is a preferred observability tool because it combines comprehensive tracing, real-time performance monitoring, automated output evaluation, and an intuitive UI in one platform. It effectively bridges the gap between model development ... enter your API key Once installed and authenticated, initialize Weave in your script or notebook by calling weave.init() at the start of your program. This sets up a new Weave project where all your logs and traces will be stored. For example: import weave # Initialize a Weave project (give a name for this observability run) weave.init(project=\"llm-observability-demo\") After this initialization, Weave is ready to capture data. It will associate all logs with the pro\\\\n\\\\n---\\\\n\\\\nSource: 2d3c24ba61c5ba18:cc35f490\\\\nas its own focus and strengths, but all aim to shed light on the â€œblack boxâ€ of LLMs in production. Some of the leading observability tools in this space include Lunary, LangSmith, and W&B Weave, among others. Lunary Lunary is an open-source platform that provides analytics and tracing for LLM-powered apps. Itâ€™s designed to be model-agnostic, meaning it can work with any LLM provider or architecture. Lunary focuses on ... and intermediate actions (like calls to tools or external APIs). This provides end-to-end tracing for chain-of-thought style applications. LangSmith also emphasizes prompt evaluation and testing. It allows developers to systematically evaluate their prompts and model outputs by using datasets of queries and expected answers. In the LangSmith dashboard, you can see how often your modelâ€™s responses match the expected results or where it might be failing. It supports scoring of outputs for accuracy, completeness, or custom metrics you define. Additionally, LangSmith prov\\\\n\\\\n---\\\\n\\\\nSource: e5ee1e1a6d3f0816:cc35f490\\\\nthen to an LLM, and then into some post-processing, ... means W&B Weave not only tells you that an answer was given, but also gives an indication of whether that answer was good or potentially problematic â€” all in real time. Few other observability tools have this level of integrated quality checking. Seamless integration and agnostic support are also key features of W&B Weave. It is designed to be framework-agnostic and model-agnostic: whether youâ€™re using OpenAIâ€™s GPT-4 via API, an open-source model on Hugging Face, or a LangChain application, Weave can plug in. The tool provides simple SDK methods and decorators to instrument your code. For instance, by adding a ... in a W&B report format) to communicate your findings. The collaborative features (permissions, team projects, comments) make it a strong choice for enterprise teams working on LLM deployments. On top of all that, W&Bâ€™s focus on enterprise readiness (with secure cloud hosting, or on-prem deployment if needed, and complian\\'}',\n",
       "       'system_prompt': 'You are a specialized agent in a larger workflow.\\n\\nYour role:\\n- PURPOSE: Perform technical and editorial review of the draft.\\n- GOAL: Identify factual issues, missing explanations, and stylistic problems. Suggest actionable edits and mark any unclear sections.\\n\\n- EXPECTED_RESULT: A structured review with strengths, issues, and concrete suggestions.\\n\\nGeneral behaviour:\\n- Be precise and concise unless the task requires extended output.\\n- Use clear structure (headings/bullets) when helpful.\\n- If you are calling tools, interpret their outputs carefully and explain your reasoning.\\n',\n",
       "       'temperature': 0.7,\n",
       "       'max_new_tokens': 16000,\n",
       "       'stream': False},\n",
       "      'outputs': {'output': '# Review of the Blog Post: \"Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems\"\\n\\n---\\n\\n## âœ… Strengths\\n\\n- **Clear structure**: The blog post is well-organized with logical sections and a clear progression from introduction to conclusion.\\n- **Relevant content**: It effectively covers the core aspects of observability in LLM systems, including tracing, metrics, feedback loops, and content safety.\\n- **Practical tone**: The tone is practical and slightly opinionated, as requested, making it accessible for intermediate ML engineers.\\n- **Use of examples**: The post uses concrete examples (e.g., hallucinations, performance bottlenecks, cost overruns) to illustrate key points.\\n- **Actionable insights**: The post provides actionable insights for developers, such as using tools like LangSmith, W&B Weave, and Lunary.\\n- **Alignment with query**: The content thoroughly addresses the importance of observability in LLM systems and covers all requested components (tracing, metrics, feedback loops, failure patterns).\\n\\n---\\n\\n## ðŸš¨ Issues and Suggestions\\n\\n### 1. **Factual Inaccuracies and Missing Explanations**\\n\\n- **Tool Mentioned Without Context**: The post mentions tools like **LangSmith**, **W&B Weave**, and **Lunary**, but it doesnâ€™t explain what they are or how they are used in the context of LLM observability. This could confuse readers who are unfamiliar with these tools.  \\n  **Suggestion**: Add brief descriptions of each tool and its role in LLM observability, or provide links to their documentation for further reading.\\n\\n- **Overgeneralization of Observability**: The post uses the term \"observability\" broadly, but it doesnâ€™t clearly differentiate between **observability**, **monitoring**, and **debugging**. This can lead to confusion.  \\n  **Suggestion**: Clarify the distinction between these concepts, especially in the section on \"LLM Observability vs. LLM Monitoring.\"\\n\\n- **Limited Discussion on Feedback Loops**: While the post mentions feedback loops, it doesnâ€™t go into detail about how they are implemented or their impact on model behavior.  \\n  **Suggestion**: Expand on the concept of feedback loops, including examples like **reinforcement learning from human feedback (RLHF)** and **active learning**.\\n\\n- **Ambiguity Around \"Common Failure Patterns\"**: The post lists failure patterns (e.g., hallucinations, performance bottlenecks, cost overruns) but doesnâ€™t provide enough depth on how observability helps detect or mitigate these issues.  \\n  **Suggestion**: Include specific examples of how observability tools detect and address these failure patterns (e.g., how a tool identifies a hallucination and triggers a model retraining).\\n\\n---\\n\\n### 2. **Stylistic and Clarity Issues**\\n\\n- **Repetition of Terms**: The term \"observability\" is used repeatedly, which can make the post feel redundant.  \\n  **Suggestion**: Vary the language to avoid repetition. For example, use synonyms like \"visibility,\" \"diagnostic insights,\" or \"behavioral analysis.\"\\n\\n- **Unclear Transition Between Sections**: The transition from the section on \"Observability Stack for LLMs\" to \"The Role of Observability in AI Ethics and Trust\" is abrupt.  \\n  **Suggestion**: Add a brief paragraph or sentence that connects these sections, explaining how observability supports both technical and ethical aspects of AI.\\n\\n- **Lack of Visual Aids**: The post is text-heavy and lacks visual aids such as diagrams or charts that could help explain complex concepts like tracing or feedback loops.  \\n  **Suggestion**: Suggest including visual aids (e.g., flowcharts of a request lifecycle, graphs of performance metrics) in future versions of the post.\\n\\n- **Some Sentences Are Too Long**: Some sentences are long and complex, making them difficult to parse.  \\n  **Suggestion**: Break up long sentences into shorter ones for better readability. For example:\\n  > \"While monitoring might tell you that a model is returning incorrect answers, observability would tell you why the model is generating those answersâ€”whether it\\'s due to a faulty prompt, a data bias, or a misalignment in the training data.\"\\n  â†’\\n  > \"While monitoring might tell you that a model is returning incorrect answers, observability goes further. It tells you why the model is generating those answers. The cause could be a faulty prompt, data bias, or a misalignment in the training data.\"\\n\\n---\\n\\n### 3. **Unclarified Concepts**\\n\\n- **\"Feedback Loops\"**: The term is used without a clear definition. The post mentions that feedback loops involve collecting data on model performance and using it to improve the model, but it doesnâ€™t explain how this process works in practice.  \\n  **Suggestion**: Define \"feedback loops\" more clearly, perhaps with an example such as:\\n  > \"A feedback loop in the context of LLMs involves collecting data on model performance (e.g., incorrect outputs, bias) and using that data to retrain or fine-tune the model. This process is often referred to as reinforcement learning from human feedback (RLHF).\"\\n\\n- **\"Content Safety and Compliance\"**: The post touches on content safety and compliance but doesnâ€™t provide enough detail on how observability tools enforce these standards.  \\n  **Suggestion**: Expand on the role of observability in content safety, including examples of how tools detect and flag problematic content (e.g., profanity, bias, hallucinations).\\n\\n---\\n\\n## âœ… Summary of Actionable Edits\\n\\n| Issue | Suggestion |\\n|------|------------|\\n| Tool descriptions are too vague | Add brief descriptions of tools like LangSmith, W&B Weave, and Lunary |\\n| Overgeneralization of observability | Clarify the difference between observability, monitoring, and debugging |\\n| Limited discussion on feedback loops | Expand on how feedback loops are implemented and their impact |\\n| Ambiguity around failure patterns | Provide specific examples of how observability detects and mitigates failure patterns |\\n| Repetition of terms | Vary the language to avoid repetition |\\n| Unclear transitions between sections | Add connecting sentences between sections |\\n| Lack of visual aids | Suggest including diagrams or charts for better clarity |\\n| Long sentences | Break up long sentences for better readability |\\n| Unclarified concepts | Define \"feedback loops\" and \"content safety\" more clearly |\\n\\n---\\n\\n## âœ… Final Assessment\\n\\nThe blog post is well-written, informative, and aligned with the original query. It provides a clear argument for the importance of observability in LLM systems and covers the key components (tracing, metrics, feedback loops, content safety). However, there are opportunities for improvement in terms of clarity, structure, and depth of explanation. With the suggested edits, the post can be made even more effective for its target audience of intermediate ML engineers.'},\n",
       "      'meta': {},\n",
       "      'ok': True,\n",
       "      'error': None,\n",
       "      'logs': []}],\n",
       "    'meta': {'agent_type': 'generic_agent',\n",
       "     'agent_config': {'allow_input_pruning': True,\n",
       "      'repair_with_llm': True,\n",
       "      'strict_tool_call': False,\n",
       "      'temperature': 0.7,\n",
       "      'max_new_tokens': 16000,\n",
       "      'return_stream_by_default': False,\n",
       "      'retry': {'max_route_retries': 2,\n",
       "       'max_tool_retries': 1,\n",
       "       'backoff_sec': 0.7},\n",
       "      'strict_json': True,\n",
       "      'max_json_repair_attempts': 1},\n",
       "     'model': '/home/nomi/workspace/Model_Weights/Qwen3-8B-unsloth-bnb-4bit',\n",
       "     'toolkit': False,\n",
       "     'log_steps': True}}}},\n",
       " 'final': {'draft': \"# Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems\\n\\n## Introduction: The Rise of LLMs and the Black Box Problem\\n\\nLarge language models (LLMs) have rapidly transformed the landscape of artificial intelligence, becoming integral to applications across industries such as finance, healthcare, customer service, and content creation. Their ability to understand and generate human-like text has made them a cornerstone of modern AI systems. However, as these models scale in complexity and usage, a critical challenge has emerged: the **black box problem**.\\n\\nThe black box problem refers to the lack of transparency in how LLMs process inputs, generate outputs, and make decisions. Unlike traditional machine learning models, which often have more interpretable structures, LLMs are inherently complex and opaque. This opacity makes it difficult for developers and stakeholders to understand how the model behaves, especially in production environments where reliability, safety, and compliance are paramount.\\n\\nTraditional monitoring and debugging approaches, which are well-suited for conventional software systems, fall short when applied to LLMs. These approaches typically focus on system-level metrics such as CPU usage, memory consumption, and response times. However, they fail to capture the internal behavior of the model itselfâ€”what the model is thinking, how it processes data, and whether it is generating safe, accurate, or ethical outputs.\\n\\nThis is where **observability** comes in. Observability is not just about monitoring; it is about **understanding** the behavior of a system in real time. For LLMs, observability means having the ability to track requests end-to-end, measure performance across all layers, and create feedback loops to continuously improve the model's behavior.\\n\\nIn this blog post, we will explore the concept of observability in the context of LLM systems. We will break down its key componentsâ€”tracing, metrics, feedback loops, and content safetyâ€”and discuss how they contribute to building transparent, reliable, and ethical AI systems. We will also examine common failure patterns in LLM systems and how observability can help address them.\\n\\nBy the end of this post, you will understand why observability is not just a nice-to-haveâ€”it is essential for the future of AI.\\n\\n---\\n\\n## Understanding Observability in LLM Systems\\n\\n### What is Observability?\\n\\nObservability is the ability to understand the internal state of a system based on its external outputs. In the context of LLM systems, observability goes beyond traditional monitoring. It involves **tracking the flow of requests through the model**, **measuring performance at every layer**, and **analyzing the model's behavior in real time**.\\n\\nUnlike monitoring, which is reactive and focuses on predefined metrics, observability is **proactive and holistic**. It provides a **comprehensive view of the system**, enabling developers to not only detect issues but also understand why they are happening.\\n\\nFor example, while monitoring might tell you that a model is returning incorrect answers, observability would tell you **why** the model is generating those answersâ€”whether it's due to a faulty prompt, a data bias, or a misalignment in the training data.\\n\\n### The Observability Stack for LLMs\\n\\nAn effective observability strategy for LLM systems consists of several key components. These components work together to provide a complete picture of the model's behavior and performance. Let's explore each of them:\\n\\n#### 1. Tracing\\n\\n**Tracing** is the process of tracking a request through the entire system, from the initial user input to the final output. In LLM systems, tracing is especially important because it allows developers to understand how the model processes inputs, interacts with external tools, and generates outputs.\\n\\nFor example, consider an application that uses an LLM to answer customer questions. The request might start with a user query, then pass through a database lookup, an API call, and finally the LLM itself. Tracing would allow developers to see the entire journey of the request, including any delays or errors that occurred at each step.\\n\\nTools like **LangSmith** and **W&B Weave** provide end-to-end tracing capabilities, allowing developers to visualize the flow of requests and identify performance bottlenecks.\\n\\n#### 2. Metrics\\n\\n**Metrics** are quantitative measures of system performance and behavior. In the context of LLM systems, metrics can include:\\n\\n- **Response latency**: How long it takes the model to generate a response.\\n- **Token usage**: The number of tokens processed by the model.\\n- **Error rates**: The percentage of requests that result in errors.\\n- **Throughput**: The number of requests handled per second.\\n- **Resource usage**: CPU, memory, and GPU usage.\\n\\nThese metrics provide valuable insights into the performance of the model and help developers identify issues such as latency spikes, resource exhaustion, or inefficient token usage.\\n\\nFor instance, if a model starts to take longer than expected to generate responses, a developer might use metrics to determine whether the issue is due to a slow API call, a misconfigured model, or an overload of incoming requests.\\n\\n#### 3. Feedback Loops\\n\\n**Feedback loops** are mechanisms that allow the model to learn from its own behavior. In the context of LLM systems, feedback loops involve **collecting data on model performance** and using it to **improve the model over time**.\\n\\nFor example, if a model generates incorrect answers, developers can use feedback loops to **retrain the model on the incorrect outputs**, helping it to avoid making the same mistakes in the future. This process is often referred to as **model fine-tuning** or **reinforcement learning from human feedback (RLHF)**.\\n\\nTools like **W&B Weave** and **LangSmith** provide features that enable developers to collect and analyze feedback data, making it easier to iterate on model improvements.\\n\\n#### 4. Content Safety and Compliance\\n\\nAs LLMs become more integrated into production applications, **content safety and compliance** have become a major concern. These systems must ensure that the model's outputs are **accurate, safe, and aligned with ethical and legal standards**.\\n\\nObservability plays a crucial role in addressing these concerns. By **monitoring the model's outputs in real time**, developers can detect and flag potentially harmful or unethical content. This includes identifying **hallucinations**, **bias**, **profanity**, and **inappropriate content**.\\n\\nFor example, a healthcare application using an LLM to generate patient summaries must ensure that the model does not disclose sensitive information or make false claims. Observability tools can help detect such issues by **scanning model outputs for PII (Personally Identifiable Information)** or **violations of ethical guidelines**.\\n\\nIn regulated industries, **observability also means maintaining an audit trail**. This ensures that if there is ever a question or incident, there is a record to review. Features like **PII redaction** and **access control** are essential for maintaining compliance in such environments.\\n\\n---\\n\\n## The Role of Observability in AI Ethics and Trust\\n\\n### Building Trust Through Transparency\\n\\nTrust is a fundamental component of any AI system. For users, developers, and stakeholders, trust is built through **transparency, reliability, and accountability**. Observability plays a crucial role in building trust by providing **visibility into the model's behavior**.\\n\\nWhen users know that an AI system is **transparent and accountable**, they are more likely to trust its outputs. This is especially important in high-stakes applications such as **healthcare, finance, and legal services**, where the consequences of incorrect outputs can be severe.\\n\\nObservability also helps build **trust among developers and data scientists**. By providing **real-time insights into model behavior**, observability enables developers to **debug issues more effectively** and **improve the model's performance over time**.\\n\\n### Ensuring Ethical AI\\n\\nEthical AI is not just about avoiding harm; it's also about **ensuring that AI systems are fair, just, and aligned with societal values**. Observability supports ethical AI by providing **insights into model behavior**, **detecting biases**, and **ensuring compliance with ethical guidelines**.\\n\\nFor example, if an LLM is used in a hiring application, it must be **free from biases that could disadvantage certain groups**. Observability tools can help detect such biases by **analyzing the model's outputs for fairness** and **flagging any discriminatory patterns**.\\n\\nIn addition, observability helps ensure that **AI systems are aligned with ethical guidelines**. For instance, if an LLM is used to generate content for a news website, it must be **free from misinformation and bias**. Observability tools can help detect such issues by **scanning model outputs for factual accuracy** and **flagging any potential misinformation**.\\n\\n---\\n\\n## Common Failure Patterns in LLM Systems\\n\\n### Hallucinations\\n\\nOne of the most common failure patterns in LLM systems is **hallucination**, where the model generates **false or made-up information**. This can be a significant issue, especially in applications where **accuracy is critical**.\\n\\nFor example, an LLM used in a customer support application might generate **incorrect information about product features**, leading to **confusion and dissatisfaction** among users. Observability tools can help detect hallucinations by **monitoring model outputs** and **flagging any inconsistencies**.\\n\\n### Performance Bottlenecks\\n\\nAs LLMs scale in complexity and usage, **performance bottlenecks** can emerge. These bottlenecks can be due to a variety of factors, including **slow API calls**, **inefficient token usage**, or **resource exhaustion**.\\n\\nObservability tools can help identify these bottlenecks by **tracking performance metrics** and **analyzing the flow of requests**. For example, if a model starts to take longer than expected to generate responses, a developer might use observability tools to **identify the slow API call** or **optimize token usage**.\\n\\n### Cost Overruns\\n\\nMany LLM services charge based on **token usage or compute time**, making **cost overruns** a significant concern. Without observability, it is difficult to **track and optimize costs**.\\n\\nObservability tools can help prevent cost overruns by **tracking token usage in real time** and **identifying any excessive usage**. For example, if a model is generating **long or unnecessary prompts**, observability tools can **flag these prompts** and **suggest optimizations**.\\n\\n---\\n\\n## The Future of Observability in AI\\n\\nAs LLMs continue to evolve and become more integrated into production systems, **observability will play an increasingly important role** in building **transparent, reliable, and ethical AI**.\\n\\nTools like **W&B Weave**, **LangSmith**, and **Lunary** are at the forefront of this movement, providing **comprehensive observability solutions** for LLM systems. These tools not only help developers **monitor and debug their models** but also enable them to **improve model performance over time**.\\n\\nIn the future, we can expect to see **more advanced observability tools** that provide **even deeper insights into model behavior**, **automated feedback loops**, and **real-time ethical compliance checks**. These tools will be essential for **building AI systems that are not only powerful but also trustworthy**.\\n\\n---\\n\\n## Conclusion\\n\\nIn conclusion, **observability is not just a technical necessityâ€”it is a strategic imperative** for the future of AI. As LLMs become more complex and widely used, **the need for transparency and accountability has never been greater**.\\n\\nBy adopting an **observability-first approach**, organizations can **gain real-time insights into model behavior**, **detect and fix issues quickly**, and **ensure that their AI systems are safe, reliable, and ethical**.\\n\\nIn the next section, we will explore **practical examples of observability in action**, including **how to implement observability in your own LLM projects** and **best practices for building observability-first AI systems**. Stay tuned for more insights on this critical topic.\",\n",
       "  'review': '# Review of the Blog Post: \"Stop Treating LLMs as Black Boxes: The Case for Observability-First AI Systems\"\\n\\n---\\n\\n## âœ… Strengths\\n\\n- **Clear structure**: The blog post is well-organized with logical sections and a clear progression from introduction to conclusion.\\n- **Relevant content**: It effectively covers the core aspects of observability in LLM systems, including tracing, metrics, feedback loops, and content safety.\\n- **Practical tone**: The tone is practical and slightly opinionated, as requested, making it accessible for intermediate ML engineers.\\n- **Use of examples**: The post uses concrete examples (e.g., hallucinations, performance bottlenecks, cost overruns) to illustrate key points.\\n- **Actionable insights**: The post provides actionable insights for developers, such as using tools like LangSmith, W&B Weave, and Lunary.\\n- **Alignment with query**: The content thoroughly addresses the importance of observability in LLM systems and covers all requested components (tracing, metrics, feedback loops, failure patterns).\\n\\n---\\n\\n## ðŸš¨ Issues and Suggestions\\n\\n### 1. **Factual Inaccuracies and Missing Explanations**\\n\\n- **Tool Mentioned Without Context**: The post mentions tools like **LangSmith**, **W&B Weave**, and **Lunary**, but it doesnâ€™t explain what they are or how they are used in the context of LLM observability. This could confuse readers who are unfamiliar with these tools.  \\n  **Suggestion**: Add brief descriptions of each tool and its role in LLM observability, or provide links to their documentation for further reading.\\n\\n- **Overgeneralization of Observability**: The post uses the term \"observability\" broadly, but it doesnâ€™t clearly differentiate between **observability**, **monitoring**, and **debugging**. This can lead to confusion.  \\n  **Suggestion**: Clarify the distinction between these concepts, especially in the section on \"LLM Observability vs. LLM Monitoring.\"\\n\\n- **Limited Discussion on Feedback Loops**: While the post mentions feedback loops, it doesnâ€™t go into detail about how they are implemented or their impact on model behavior.  \\n  **Suggestion**: Expand on the concept of feedback loops, including examples like **reinforcement learning from human feedback (RLHF)** and **active learning**.\\n\\n- **Ambiguity Around \"Common Failure Patterns\"**: The post lists failure patterns (e.g., hallucinations, performance bottlenecks, cost overruns) but doesnâ€™t provide enough depth on how observability helps detect or mitigate these issues.  \\n  **Suggestion**: Include specific examples of how observability tools detect and address these failure patterns (e.g., how a tool identifies a hallucination and triggers a model retraining).\\n\\n---\\n\\n### 2. **Stylistic and Clarity Issues**\\n\\n- **Repetition of Terms**: The term \"observability\" is used repeatedly, which can make the post feel redundant.  \\n  **Suggestion**: Vary the language to avoid repetition. For example, use synonyms like \"visibility,\" \"diagnostic insights,\" or \"behavioral analysis.\"\\n\\n- **Unclear Transition Between Sections**: The transition from the section on \"Observability Stack for LLMs\" to \"The Role of Observability in AI Ethics and Trust\" is abrupt.  \\n  **Suggestion**: Add a brief paragraph or sentence that connects these sections, explaining how observability supports both technical and ethical aspects of AI.\\n\\n- **Lack of Visual Aids**: The post is text-heavy and lacks visual aids such as diagrams or charts that could help explain complex concepts like tracing or feedback loops.  \\n  **Suggestion**: Suggest including visual aids (e.g., flowcharts of a request lifecycle, graphs of performance metrics) in future versions of the post.\\n\\n- **Some Sentences Are Too Long**: Some sentences are long and complex, making them difficult to parse.  \\n  **Suggestion**: Break up long sentences into shorter ones for better readability. For example:\\n  > \"While monitoring might tell you that a model is returning incorrect answers, observability would tell you why the model is generating those answersâ€”whether it\\'s due to a faulty prompt, a data bias, or a misalignment in the training data.\"\\n  â†’\\n  > \"While monitoring might tell you that a model is returning incorrect answers, observability goes further. It tells you why the model is generating those answers. The cause could be a faulty prompt, data bias, or a misalignment in the training data.\"\\n\\n---\\n\\n### 3. **Unclarified Concepts**\\n\\n- **\"Feedback Loops\"**: The term is used without a clear definition. The post mentions that feedback loops involve collecting data on model performance and using it to improve the model, but it doesnâ€™t explain how this process works in practice.  \\n  **Suggestion**: Define \"feedback loops\" more clearly, perhaps with an example such as:\\n  > \"A feedback loop in the context of LLMs involves collecting data on model performance (e.g., incorrect outputs, bias) and using that data to retrain or fine-tune the model. This process is often referred to as reinforcement learning from human feedback (RLHF).\"\\n\\n- **\"Content Safety and Compliance\"**: The post touches on content safety and compliance but doesnâ€™t provide enough detail on how observability tools enforce these standards.  \\n  **Suggestion**: Expand on the role of observability in content safety, including examples of how tools detect and flag problematic content (e.g., profanity, bias, hallucinations).\\n\\n---\\n\\n## âœ… Summary of Actionable Edits\\n\\n| Issue | Suggestion |\\n|------|------------|\\n| Tool descriptions are too vague | Add brief descriptions of tools like LangSmith, W&B Weave, and Lunary |\\n| Overgeneralization of observability | Clarify the difference between observability, monitoring, and debugging |\\n| Limited discussion on feedback loops | Expand on how feedback loops are implemented and their impact |\\n| Ambiguity around failure patterns | Provide specific examples of how observability detects and mitigates failure patterns |\\n| Repetition of terms | Vary the language to avoid repetition |\\n| Unclear transitions between sections | Add connecting sentences between sections |\\n| Lack of visual aids | Suggest including diagrams or charts for better clarity |\\n| Long sentences | Break up long sentences for better readability |\\n| Unclarified concepts | Define \"feedback loops\" and \"content safety\" more clearly |\\n\\n---\\n\\n## âœ… Final Assessment\\n\\nThe blog post is well-written, informative, and aligned with the original query. It provides a clear argument for the importance of observability in LLM systems and covers the key components (tracing, metrics, feedback loops, content safety). However, there are opportunities for improvement in terms of clarity, structure, and depth of explanation. With the suggested edits, the post can be made even more effective for its target audience of intermediate ML engineers.'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324b84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b29e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616aa999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59defcd4",
   "metadata": {},
   "source": [
    "## Docs Gen Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6cb6796c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-26 14:42:37\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: directory_scan\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-26 14:42:37\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: code_symbol_index\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-26 14:42:37\u001b[0m | \u001b[96mdir_scanning.py:__call__\u001b[0m | DirectoryScanTool scanning project_root=/home/nomi/workspace/neurosurfer/neurosurfer docs_root=None max_depth=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_root': '/home/nomi/workspace/neurosurfer/neurosurfer',\n",
       " 'docs_root': None,\n",
       " 'python_files': [{'path': 'version.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/version.py',\n",
       "   'module': 'neurosurfer.version',\n",
       "   'package': 'neurosurfer'},\n",
       "  {'path': 'config.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/config.py',\n",
       "   'module': 'neurosurfer.config',\n",
       "   'package': 'neurosurfer'},\n",
       "  {'path': 'diagnostics.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/diagnostics.py',\n",
       "   'module': 'neurosurfer.diagnostics',\n",
       "   'package': 'neurosurfer'},\n",
       "  {'path': 'logger.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/logger.py',\n",
       "   'module': 'neurosurfer.logger',\n",
       "   'package': 'neurosurfer'},\n",
       "  {'path': '__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/__init__.py',\n",
       "   'module': 'neurosurfer.__init__',\n",
       "   'package': 'neurosurfer'},\n",
       "  {'path': 'db/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/db/__init__.py',\n",
       "   'module': 'neurosurfer.db.__init__',\n",
       "   'package': 'neurosurfer.db'},\n",
       "  {'path': 'db/sql_database.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/db/sql_database.py',\n",
       "   'module': 'neurosurfer.db.sql_database',\n",
       "   'package': 'neurosurfer.db'},\n",
       "  {'path': 'db/sql_schema_store.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/db/sql_schema_store.py',\n",
       "   'module': 'neurosurfer.db.sql_schema_store',\n",
       "   'package': 'neurosurfer.db'},\n",
       "  {'path': 'examples/quickstart_app.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/examples/quickstart_app.py',\n",
       "   'module': 'neurosurfer.examples.quickstart_app',\n",
       "   'package': 'neurosurfer.examples'},\n",
       "  {'path': 'vectorstores/in_memory_store.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/vectorstores/in_memory_store.py',\n",
       "   'module': 'neurosurfer.vectorstores.in_memory_store',\n",
       "   'package': 'neurosurfer.vectorstores'},\n",
       "  {'path': 'vectorstores/base.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/vectorstores/base.py',\n",
       "   'module': 'neurosurfer.vectorstores.base',\n",
       "   'package': 'neurosurfer.vectorstores'},\n",
       "  {'path': 'vectorstores/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/vectorstores/__init__.py',\n",
       "   'module': 'neurosurfer.vectorstores.__init__',\n",
       "   'package': 'neurosurfer.vectorstores'},\n",
       "  {'path': 'vectorstores/chroma.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/vectorstores/chroma.py',\n",
       "   'module': 'neurosurfer.vectorstores.chroma',\n",
       "   'package': 'neurosurfer.vectorstores'},\n",
       "  {'path': 'utils/helper.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/utils/helper.py',\n",
       "   'module': 'neurosurfer.utils.helper',\n",
       "   'package': 'neurosurfer.utils'},\n",
       "  {'path': 'utils/response_wrappers.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/utils/response_wrappers.py',\n",
       "   'module': 'neurosurfer.utils.response_wrappers',\n",
       "   'package': 'neurosurfer.utils'},\n",
       "  {'path': 'utils/prompts.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/utils/prompts.py',\n",
       "   'module': 'neurosurfer.utils.prompts',\n",
       "   'package': 'neurosurfer.utils'},\n",
       "  {'path': 'utils/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/utils/__init__.py',\n",
       "   'module': 'neurosurfer.utils.__init__',\n",
       "   'package': 'neurosurfer.utils'},\n",
       "  {'path': 'tracing/step_context.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tracing/step_context.py',\n",
       "   'module': 'neurosurfer.tracing.step_context',\n",
       "   'package': 'neurosurfer.tracing'},\n",
       "  {'path': 'tracing/models.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tracing/models.py',\n",
       "   'module': 'neurosurfer.tracing.models',\n",
       "   'package': 'neurosurfer.tracing'},\n",
       "  {'path': 'tracing/span.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tracing/span.py',\n",
       "   'module': 'neurosurfer.tracing.span',\n",
       "   'package': 'neurosurfer.tracing'},\n",
       "  {'path': 'tracing/tracer.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tracing/tracer.py',\n",
       "   'module': 'neurosurfer.tracing.tracer',\n",
       "   'package': 'neurosurfer.tracing'},\n",
       "  {'path': 'tracing/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tracing/__init__.py',\n",
       "   'module': 'neurosurfer.tracing.__init__',\n",
       "   'package': 'neurosurfer.tracing'},\n",
       "  {'path': 'models/embedders/sentence_transformer.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/embedders/sentence_transformer.py',\n",
       "   'module': 'neurosurfer.models.embedders.sentence_transformer',\n",
       "   'package': 'neurosurfer.models.embedders'},\n",
       "  {'path': 'models/embedders/base.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/embedders/base.py',\n",
       "   'module': 'neurosurfer.models.embedders.base',\n",
       "   'package': 'neurosurfer.models.embedders'},\n",
       "  {'path': 'models/embedders/llamacpp.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/embedders/llamacpp.py',\n",
       "   'module': 'neurosurfer.models.embedders.llamacpp',\n",
       "   'package': 'neurosurfer.models.embedders'},\n",
       "  {'path': 'models/embedders/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/embedders/__init__.py',\n",
       "   'module': 'neurosurfer.models.embedders.__init__',\n",
       "   'package': 'neurosurfer.models.embedders'},\n",
       "  {'path': 'models/chat_models/transformers.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/chat_models/transformers.py',\n",
       "   'module': 'neurosurfer.models.chat_models.transformers',\n",
       "   'package': 'neurosurfer.models.chat_models'},\n",
       "  {'path': 'models/chat_models/base.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/chat_models/base.py',\n",
       "   'module': 'neurosurfer.models.chat_models.base',\n",
       "   'package': 'neurosurfer.models.chat_models'},\n",
       "  {'path': 'models/chat_models/unsloth.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/chat_models/unsloth.py',\n",
       "   'module': 'neurosurfer.models.chat_models.unsloth',\n",
       "   'package': 'neurosurfer.models.chat_models'},\n",
       "  {'path': 'models/chat_models/llamacpp.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/chat_models/llamacpp.py',\n",
       "   'module': 'neurosurfer.models.chat_models.llamacpp',\n",
       "   'package': 'neurosurfer.models.chat_models'},\n",
       "  {'path': 'models/chat_models/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/chat_models/__init__.py',\n",
       "   'module': 'neurosurfer.models.chat_models.__init__',\n",
       "   'package': 'neurosurfer.models.chat_models'},\n",
       "  {'path': 'models/chat_models/openai.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/chat_models/openai.py',\n",
       "   'module': 'neurosurfer.models.chat_models.openai',\n",
       "   'package': 'neurosurfer.models.chat_models'},\n",
       "  {'path': 'runtime/checks.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/runtime/checks.py',\n",
       "   'module': 'neurosurfer.runtime.checks',\n",
       "   'package': 'neurosurfer.runtime'},\n",
       "  {'path': 'runtime/paths.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/runtime/paths.py',\n",
       "   'module': 'neurosurfer.runtime.paths',\n",
       "   'package': 'neurosurfer.runtime'},\n",
       "  {'path': 'server/reset.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/reset.py',\n",
       "   'module': 'neurosurfer.server.reset',\n",
       "   'package': 'neurosurfer.server'},\n",
       "  {'path': 'server/models_registry.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/models_registry.py',\n",
       "   'module': 'neurosurfer.server.models_registry',\n",
       "   'package': 'neurosurfer.server'},\n",
       "  {'path': 'server/utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/utils.py',\n",
       "   'module': 'neurosurfer.server.utils',\n",
       "   'package': 'neurosurfer.server'},\n",
       "  {'path': 'server/runtime.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/runtime.py',\n",
       "   'module': 'neurosurfer.server.runtime',\n",
       "   'package': 'neurosurfer.server'},\n",
       "  {'path': 'server/config.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/config.py',\n",
       "   'module': 'neurosurfer.server.config',\n",
       "   'package': 'neurosurfer.server'},\n",
       "  {'path': 'server/app.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/app.py',\n",
       "   'module': 'neurosurfer.server.app',\n",
       "   'package': 'neurosurfer.server'},\n",
       "  {'path': 'server/security.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/security.py',\n",
       "   'module': 'neurosurfer.server.security',\n",
       "   'package': 'neurosurfer.server'},\n",
       "  {'path': 'server/gunicorn.conf.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/gunicorn.conf.py',\n",
       "   'module': 'neurosurfer.server.gunicorn.conf',\n",
       "   'package': 'neurosurfer.server'},\n",
       "  {'path': 'server/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/__init__.py',\n",
       "   'module': 'neurosurfer.server.__init__',\n",
       "   'package': 'neurosurfer.server'},\n",
       "  {'path': 'server/db/models.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/db/models.py',\n",
       "   'module': 'neurosurfer.server.db.models',\n",
       "   'package': 'neurosurfer.server.db'},\n",
       "  {'path': 'server/db/db.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/db/db.py',\n",
       "   'module': 'neurosurfer.server.db.db',\n",
       "   'package': 'neurosurfer.server.db'},\n",
       "  {'path': 'server/db/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/db/__init__.py',\n",
       "   'module': 'neurosurfer.server.db.__init__',\n",
       "   'package': 'neurosurfer.server.db'},\n",
       "  {'path': 'server/services/follow_up_questions.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/follow_up_questions.py',\n",
       "   'module': 'neurosurfer.server.services.follow_up_questions',\n",
       "   'package': 'neurosurfer.server.services'},\n",
       "  {'path': 'server/services/rag/metadata_filter.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/metadata_filter.py',\n",
       "   'module': 'neurosurfer.server.services.rag.metadata_filter',\n",
       "   'package': 'neurosurfer.server.services.rag'},\n",
       "  {'path': 'server/services/rag/models.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/models.py',\n",
       "   'module': 'neurosurfer.server.services.rag.models',\n",
       "   'package': 'neurosurfer.server.services.rag'},\n",
       "  {'path': 'server/services/rag/ingestor.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/ingestor.py',\n",
       "   'module': 'neurosurfer.server.services.rag.ingestor',\n",
       "   'package': 'neurosurfer.server.services.rag'},\n",
       "  {'path': 'server/services/rag/summarizer.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/summarizer.py',\n",
       "   'module': 'neurosurfer.server.services.rag.summarizer',\n",
       "   'package': 'neurosurfer.server.services.rag'},\n",
       "  {'path': 'server/services/rag/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/__init__.py',\n",
       "   'module': 'neurosurfer.server.services.rag.__init__',\n",
       "   'package': 'neurosurfer.server.services.rag'},\n",
       "  {'path': 'server/services/rag/orchestrator.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/orchestrator.py',\n",
       "   'module': 'neurosurfer.server.services.rag.orchestrator',\n",
       "   'package': 'neurosurfer.server.services.rag'},\n",
       "  {'path': 'server/services/rag/gate.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/gate.py',\n",
       "   'module': 'neurosurfer.server.services.rag.gate',\n",
       "   'package': 'neurosurfer.server.services.rag'},\n",
       "  {'path': 'server/schemas/model_response.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/schemas/model_response.py',\n",
       "   'module': 'neurosurfer.server.schemas.model_response',\n",
       "   'package': 'neurosurfer.server.schemas'},\n",
       "  {'path': 'server/schemas/chats.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/schemas/chats.py',\n",
       "   'module': 'neurosurfer.server.schemas.chats',\n",
       "   'package': 'neurosurfer.server.schemas'},\n",
       "  {'path': 'server/schemas/completions.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/schemas/completions.py',\n",
       "   'module': 'neurosurfer.server.schemas.completions',\n",
       "   'package': 'neurosurfer.server.schemas'},\n",
       "  {'path': 'server/schemas/model_registry.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/schemas/model_registry.py',\n",
       "   'module': 'neurosurfer.server.schemas.model_registry',\n",
       "   'package': 'neurosurfer.server.schemas'},\n",
       "  {'path': 'server/schemas/auth.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/schemas/auth.py',\n",
       "   'module': 'neurosurfer.server.schemas.auth',\n",
       "   'package': 'neurosurfer.server.schemas'},\n",
       "  {'path': 'server/schemas/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/schemas/__init__.py',\n",
       "   'module': 'neurosurfer.server.schemas.__init__',\n",
       "   'package': 'neurosurfer.server.schemas'},\n",
       "  {'path': 'server/api/api_chat_completions.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/api/api_chat_completions.py',\n",
       "   'module': 'neurosurfer.server.api.api_chat_completions',\n",
       "   'package': 'neurosurfer.server.api'},\n",
       "  {'path': 'server/api/api_files.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/api/api_files.py',\n",
       "   'module': 'neurosurfer.server.api.api_files',\n",
       "   'package': 'neurosurfer.server.api'},\n",
       "  {'path': 'server/api/api_chats.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/api/api_chats.py',\n",
       "   'module': 'neurosurfer.server.api.api_chats',\n",
       "   'package': 'neurosurfer.server.api'},\n",
       "  {'path': 'server/api/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/api/__init__.py',\n",
       "   'module': 'neurosurfer.server.api.__init__',\n",
       "   'package': 'neurosurfer.server.api'},\n",
       "  {'path': 'server/api/api_auth.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/api/api_auth.py',\n",
       "   'module': 'neurosurfer.server.api.api_auth',\n",
       "   'package': 'neurosurfer.server.api'},\n",
       "  {'path': 'tools/toolkit.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/toolkit.py',\n",
       "   'module': 'neurosurfer.tools.toolkit',\n",
       "   'package': 'neurosurfer.tools'},\n",
       "  {'path': 'tools/base_tool.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/base_tool.py',\n",
       "   'module': 'neurosurfer.tools.base_tool',\n",
       "   'package': 'neurosurfer.tools'},\n",
       "  {'path': 'tools/tool_spec.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/tool_spec.py',\n",
       "   'module': 'neurosurfer.tools.tool_spec',\n",
       "   'package': 'neurosurfer.tools'},\n",
       "  {'path': 'tools/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/__init__.py',\n",
       "   'module': 'neurosurfer.tools.__init__',\n",
       "   'package': 'neurosurfer.tools'},\n",
       "  {'path': 'tools/mermaid/erd_diagram_generator.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/mermaid/erd_diagram_generator.py',\n",
       "   'module': 'neurosurfer.tools.mermaid.erd_diagram_generator',\n",
       "   'package': 'neurosurfer.tools.mermaid'},\n",
       "  {'path': 'tools/mermaid/mermaid_diagram_generator.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/mermaid/mermaid_diagram_generator.py',\n",
       "   'module': 'neurosurfer.tools.mermaid.mermaid_diagram_generator',\n",
       "   'package': 'neurosurfer.tools.mermaid'},\n",
       "  {'path': 'tools/common/general_query_assistant.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/common/general_query_assistant.py',\n",
       "   'module': 'neurosurfer.tools.common.general_query_assistant',\n",
       "   'package': 'neurosurfer.tools.common'},\n",
       "  {'path': 'tools/common/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/common/__init__.py',\n",
       "   'module': 'neurosurfer.tools.common.__init__',\n",
       "   'package': 'neurosurfer.tools.common'},\n",
       "  {'path': 'tools/sql/final_answer_formatter.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/sql/final_answer_formatter.py',\n",
       "   'module': 'neurosurfer.tools.sql.final_answer_formatter',\n",
       "   'package': 'neurosurfer.tools.sql'},\n",
       "  {'path': 'tools/sql/sql_executor.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/sql/sql_executor.py',\n",
       "   'module': 'neurosurfer.tools.sql.sql_executor',\n",
       "   'package': 'neurosurfer.tools.sql'},\n",
       "  {'path': 'tools/sql/db_insights_tool.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/sql/db_insights_tool.py',\n",
       "   'module': 'neurosurfer.tools.sql.db_insights_tool',\n",
       "   'package': 'neurosurfer.tools.sql'},\n",
       "  {'path': 'tools/sql/relevant_tables_schema_retriever.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/sql/relevant_tables_schema_retriever.py',\n",
       "   'module': 'neurosurfer.tools.sql.relevant_tables_schema_retriever',\n",
       "   'package': 'neurosurfer.tools.sql'},\n",
       "  {'path': 'tools/sql/sql_query_generator.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/sql/sql_query_generator.py',\n",
       "   'module': 'neurosurfer.tools.sql.sql_query_generator',\n",
       "   'package': 'neurosurfer.tools.sql'},\n",
       "  {'path': 'tools/sql/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/sql/__init__.py',\n",
       "   'module': 'neurosurfer.tools.sql.__init__',\n",
       "   'package': 'neurosurfer.tools.sql'},\n",
       "  {'path': 'tools/rag/docs_generator.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/rag/docs_generator.py',\n",
       "   'module': 'neurosurfer.tools.rag.docs_generator',\n",
       "   'package': 'neurosurfer.tools.rag'},\n",
       "  {'path': 'tools/rag/simple_query_assitant.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/rag/simple_query_assitant.py',\n",
       "   'module': 'neurosurfer.tools.rag.simple_query_assitant',\n",
       "   'package': 'neurosurfer.tools.rag'},\n",
       "  {'path': 'tools/websearch/extractor.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/extractor.py',\n",
       "   'module': 'neurosurfer.tools.websearch.extractor',\n",
       "   'package': 'neurosurfer.tools.websearch'},\n",
       "  {'path': 'tools/websearch/tool.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/tool.py',\n",
       "   'module': 'neurosurfer.tools.websearch.tool',\n",
       "   'package': 'neurosurfer.tools.websearch'},\n",
       "  {'path': 'tools/websearch/utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/utils.py',\n",
       "   'module': 'neurosurfer.tools.websearch.utils',\n",
       "   'package': 'neurosurfer.tools.websearch'},\n",
       "  {'path': 'tools/websearch/config.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/config.py',\n",
       "   'module': 'neurosurfer.tools.websearch.config',\n",
       "   'package': 'neurosurfer.tools.websearch'},\n",
       "  {'path': 'tools/websearch/templates.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/templates.py',\n",
       "   'module': 'neurosurfer.tools.websearch.templates',\n",
       "   'package': 'neurosurfer.tools.websearch'},\n",
       "  {'path': 'tools/websearch/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/__init__.py',\n",
       "   'module': 'neurosurfer.tools.websearch.__init__',\n",
       "   'package': 'neurosurfer.tools.websearch'},\n",
       "  {'path': 'tools/websearch/rag.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/rag.py',\n",
       "   'module': 'neurosurfer.tools.websearch.rag',\n",
       "   'package': 'neurosurfer.tools.websearch'},\n",
       "  {'path': 'tools/websearch/engines/serpapi.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/engines/serpapi.py',\n",
       "   'module': 'neurosurfer.tools.websearch.engines.serpapi',\n",
       "   'package': 'neurosurfer.tools.websearch.engines'},\n",
       "  {'path': 'tools/websearch/engines/base.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/engines/base.py',\n",
       "   'module': 'neurosurfer.tools.websearch.engines.base',\n",
       "   'package': 'neurosurfer.tools.websearch.engines'},\n",
       "  {'path': 'tools/websearch/engines/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/engines/__init__.py',\n",
       "   'module': 'neurosurfer.tools.websearch.engines.__init__',\n",
       "   'package': 'neurosurfer.tools.websearch.engines'},\n",
       "  {'path': 'agents/tools_router_agent.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/tools_router_agent.py',\n",
       "   'module': 'neurosurfer.agents.tools_router_agent',\n",
       "   'package': 'neurosurfer.agents'},\n",
       "  {'path': 'agents/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/__init__.py',\n",
       "   'module': 'neurosurfer.agents.__init__',\n",
       "   'package': 'neurosurfer.agents'},\n",
       "  {'path': 'agents/sql_agent.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/sql_agent.py',\n",
       "   'module': 'neurosurfer.agents.sql_agent',\n",
       "   'package': 'neurosurfer.agents'},\n",
       "  {'path': 'agents/common/utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/common/utils.py',\n",
       "   'module': 'neurosurfer.agents.common.utils',\n",
       "   'package': 'neurosurfer.agents.common'},\n",
       "  {'path': 'agents/rag/ingestor.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/ingestor.py',\n",
       "   'module': 'neurosurfer.agents.rag.ingestor',\n",
       "   'package': 'neurosurfer.agents.rag'},\n",
       "  {'path': 'agents/rag/picker.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/picker.py',\n",
       "   'module': 'neurosurfer.agents.rag.picker',\n",
       "   'package': 'neurosurfer.agents.rag'},\n",
       "  {'path': 'agents/rag/constants.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/constants.py',\n",
       "   'module': 'neurosurfer.agents.rag.constants',\n",
       "   'package': 'neurosurfer.agents.rag'},\n",
       "  {'path': 'agents/rag/config.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/config.py',\n",
       "   'module': 'neurosurfer.agents.rag.config',\n",
       "   'package': 'neurosurfer.agents.rag'},\n",
       "  {'path': 'agents/rag/chunker.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/chunker.py',\n",
       "   'module': 'neurosurfer.agents.rag.chunker',\n",
       "   'package': 'neurosurfer.agents.rag'},\n",
       "  {'path': 'agents/rag/context_builder.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/context_builder.py',\n",
       "   'module': 'neurosurfer.agents.rag.context_builder',\n",
       "   'package': 'neurosurfer.agents.rag'},\n",
       "  {'path': 'agents/rag/templates.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/templates.py',\n",
       "   'module': 'neurosurfer.agents.rag.templates',\n",
       "   'package': 'neurosurfer.agents.rag'},\n",
       "  {'path': 'agents/rag/agent.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/agent.py',\n",
       "   'module': 'neurosurfer.agents.rag.agent',\n",
       "   'package': 'neurosurfer.agents.rag'},\n",
       "  {'path': 'agents/rag/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/__init__.py',\n",
       "   'module': 'neurosurfer.agents.rag.__init__',\n",
       "   'package': 'neurosurfer.agents.rag'},\n",
       "  {'path': 'agents/rag/token_utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/token_utils.py',\n",
       "   'module': 'neurosurfer.agents.rag.token_utils',\n",
       "   'package': 'neurosurfer.agents.rag'},\n",
       "  {'path': 'agents/rag/filereader.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/filereader.py',\n",
       "   'module': 'neurosurfer.agents.rag.filereader',\n",
       "   'package': 'neurosurfer.agents.rag'},\n",
       "  {'path': 'agents/react/memory.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/memory.py',\n",
       "   'module': 'neurosurfer.agents.react.memory',\n",
       "   'package': 'neurosurfer.agents.react'},\n",
       "  {'path': 'agents/react/types.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/types.py',\n",
       "   'module': 'neurosurfer.agents.react.types',\n",
       "   'package': 'neurosurfer.agents.react'},\n",
       "  {'path': 'agents/react/history.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/history.py',\n",
       "   'module': 'neurosurfer.agents.react.history',\n",
       "   'package': 'neurosurfer.agents.react'},\n",
       "  {'path': 'agents/react/retry.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/retry.py',\n",
       "   'module': 'neurosurfer.agents.react.retry',\n",
       "   'package': 'neurosurfer.agents.react'},\n",
       "  {'path': 'agents/react/config.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/config.py',\n",
       "   'module': 'neurosurfer.agents.react.config',\n",
       "   'package': 'neurosurfer.agents.react'},\n",
       "  {'path': 'agents/react/parser.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/parser.py',\n",
       "   'module': 'neurosurfer.agents.react.parser',\n",
       "   'package': 'neurosurfer.agents.react'},\n",
       "  {'path': 'agents/react/base.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/base.py',\n",
       "   'module': 'neurosurfer.agents.react.base',\n",
       "   'package': 'neurosurfer.agents.react'},\n",
       "  {'path': 'agents/react/scratchpad.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/scratchpad.py',\n",
       "   'module': 'neurosurfer.agents.react.scratchpad',\n",
       "   'package': 'neurosurfer.agents.react'},\n",
       "  {'path': 'agents/react/agent.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/agent.py',\n",
       "   'module': 'neurosurfer.agents.react.agent',\n",
       "   'package': 'neurosurfer.agents.react'},\n",
       "  {'path': 'agents/react/exceptions.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/exceptions.py',\n",
       "   'module': 'neurosurfer.agents.react.exceptions',\n",
       "   'package': 'neurosurfer.agents.react'},\n",
       "  {'path': 'agents/react/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/__init__.py',\n",
       "   'module': 'neurosurfer.agents.react.__init__',\n",
       "   'package': 'neurosurfer.agents.react'},\n",
       "  {'path': 'agents/graph/loader.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/loader.py',\n",
       "   'module': 'neurosurfer.agents.graph.loader',\n",
       "   'package': 'neurosurfer.agents.graph'},\n",
       "  {'path': 'agents/graph/manager.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/manager.py',\n",
       "   'module': 'neurosurfer.agents.graph.manager',\n",
       "   'package': 'neurosurfer.agents.graph'},\n",
       "  {'path': 'agents/graph/utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/utils.py',\n",
       "   'module': 'neurosurfer.agents.graph.utils',\n",
       "   'package': 'neurosurfer.agents.graph'},\n",
       "  {'path': 'agents/graph/schema.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/schema.py',\n",
       "   'module': 'neurosurfer.agents.graph.schema',\n",
       "   'package': 'neurosurfer.agents.graph'},\n",
       "  {'path': 'agents/graph/export.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/export.py',\n",
       "   'module': 'neurosurfer.agents.graph.export',\n",
       "   'package': 'neurosurfer.agents.graph'},\n",
       "  {'path': 'agents/graph/errors.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/errors.py',\n",
       "   'module': 'neurosurfer.agents.graph.errors',\n",
       "   'package': 'neurosurfer.agents.graph'},\n",
       "  {'path': 'agents/graph/executor.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/executor.py',\n",
       "   'module': 'neurosurfer.agents.graph.executor',\n",
       "   'package': 'neurosurfer.agents.graph'},\n",
       "  {'path': 'agents/graph/templates.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/templates.py',\n",
       "   'module': 'neurosurfer.agents.graph.templates',\n",
       "   'package': 'neurosurfer.agents.graph'},\n",
       "  {'path': 'agents/graph/agent.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/agent.py',\n",
       "   'module': 'neurosurfer.agents.graph.agent',\n",
       "   'package': 'neurosurfer.agents.graph'},\n",
       "  {'path': 'agents/graph/model_pool.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/model_pool.py',\n",
       "   'module': 'neurosurfer.agents.graph.model_pool',\n",
       "   'package': 'neurosurfer.agents.graph'},\n",
       "  {'path': 'agents/graph/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/__init__.py',\n",
       "   'module': 'neurosurfer.agents.graph.__init__',\n",
       "   'package': 'neurosurfer.agents.graph'},\n",
       "  {'path': 'agents/graph/artifacts.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/artifacts.py',\n",
       "   'module': 'neurosurfer.agents.graph.artifacts',\n",
       "   'package': 'neurosurfer.agents.graph'},\n",
       "  {'path': 'agents/agent/responses.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/agent/responses.py',\n",
       "   'module': 'neurosurfer.agents.agent.responses',\n",
       "   'package': 'neurosurfer.agents.agent'},\n",
       "  {'path': 'agents/agent/config.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/agent/config.py',\n",
       "   'module': 'neurosurfer.agents.agent.config',\n",
       "   'package': 'neurosurfer.agents.agent'},\n",
       "  {'path': 'agents/agent/templates.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/agent/templates.py',\n",
       "   'module': 'neurosurfer.agents.agent.templates',\n",
       "   'package': 'neurosurfer.agents.agent'},\n",
       "  {'path': 'agents/agent/agent.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/agent/agent.py',\n",
       "   'module': 'neurosurfer.agents.agent.agent',\n",
       "   'package': 'neurosurfer.agents.agent'},\n",
       "  {'path': 'agents/agent/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/agent/__init__.py',\n",
       "   'module': 'neurosurfer.agents.agent.__init__',\n",
       "   'package': 'neurosurfer.agents.agent'},\n",
       "  {'path': 'agents/agent/schema_utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/agent/schema_utils.py',\n",
       "   'module': 'neurosurfer.agents.agent.schema_utils',\n",
       "   'package': 'neurosurfer.agents.agent'},\n",
       "  {'path': 'cli/processes.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/cli/processes.py',\n",
       "   'module': 'neurosurfer.cli.processes',\n",
       "   'package': 'neurosurfer.cli'},\n",
       "  {'path': 'cli/serve.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/cli/serve.py',\n",
       "   'module': 'neurosurfer.cli.serve',\n",
       "   'package': 'neurosurfer.cli'},\n",
       "  {'path': 'cli/utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/cli/utils.py',\n",
       "   'module': 'neurosurfer.cli.utils',\n",
       "   'package': 'neurosurfer.cli'},\n",
       "  {'path': 'cli/main.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/cli/main.py',\n",
       "   'module': 'neurosurfer.cli.main',\n",
       "   'package': 'neurosurfer.cli'},\n",
       "  {'path': 'cli/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/cli/__init__.py',\n",
       "   'module': 'neurosurfer.cli.__init__',\n",
       "   'package': 'neurosurfer.cli'}],\n",
       " 'doc_files': [],\n",
       " 'packages': ['neurosurfer',\n",
       "  'neurosurfer.agents',\n",
       "  'neurosurfer.agents.agent',\n",
       "  'neurosurfer.agents.common',\n",
       "  'neurosurfer.agents.graph',\n",
       "  'neurosurfer.agents.rag',\n",
       "  'neurosurfer.agents.react',\n",
       "  'neurosurfer.cli',\n",
       "  'neurosurfer.db',\n",
       "  'neurosurfer.examples',\n",
       "  'neurosurfer.models.chat_models',\n",
       "  'neurosurfer.models.embedders',\n",
       "  'neurosurfer.runtime',\n",
       "  'neurosurfer.server',\n",
       "  'neurosurfer.server.api',\n",
       "  'neurosurfer.server.db',\n",
       "  'neurosurfer.server.schemas',\n",
       "  'neurosurfer.server.services',\n",
       "  'neurosurfer.server.services.rag',\n",
       "  'neurosurfer.tools',\n",
       "  'neurosurfer.tools.common',\n",
       "  'neurosurfer.tools.mermaid',\n",
       "  'neurosurfer.tools.rag',\n",
       "  'neurosurfer.tools.sql',\n",
       "  'neurosurfer.tools.websearch',\n",
       "  'neurosurfer.tools.websearch.engines',\n",
       "  'neurosurfer.tracing',\n",
       "  'neurosurfer.utils',\n",
       "  'neurosurfer.vectorstores'],\n",
       " 'modules': ['neurosurfer.__init__',\n",
       "  'neurosurfer.agents.__init__',\n",
       "  'neurosurfer.agents.agent.__init__',\n",
       "  'neurosurfer.agents.agent.agent',\n",
       "  'neurosurfer.agents.agent.config',\n",
       "  'neurosurfer.agents.agent.responses',\n",
       "  'neurosurfer.agents.agent.schema_utils',\n",
       "  'neurosurfer.agents.agent.templates',\n",
       "  'neurosurfer.agents.common.utils',\n",
       "  'neurosurfer.agents.graph.__init__',\n",
       "  'neurosurfer.agents.graph.agent',\n",
       "  'neurosurfer.agents.graph.artifacts',\n",
       "  'neurosurfer.agents.graph.errors',\n",
       "  'neurosurfer.agents.graph.executor',\n",
       "  'neurosurfer.agents.graph.export',\n",
       "  'neurosurfer.agents.graph.loader',\n",
       "  'neurosurfer.agents.graph.manager',\n",
       "  'neurosurfer.agents.graph.model_pool',\n",
       "  'neurosurfer.agents.graph.schema',\n",
       "  'neurosurfer.agents.graph.templates',\n",
       "  'neurosurfer.agents.graph.utils',\n",
       "  'neurosurfer.agents.rag.__init__',\n",
       "  'neurosurfer.agents.rag.agent',\n",
       "  'neurosurfer.agents.rag.chunker',\n",
       "  'neurosurfer.agents.rag.config',\n",
       "  'neurosurfer.agents.rag.constants',\n",
       "  'neurosurfer.agents.rag.context_builder',\n",
       "  'neurosurfer.agents.rag.filereader',\n",
       "  'neurosurfer.agents.rag.ingestor',\n",
       "  'neurosurfer.agents.rag.picker',\n",
       "  'neurosurfer.agents.rag.templates',\n",
       "  'neurosurfer.agents.rag.token_utils',\n",
       "  'neurosurfer.agents.react.__init__',\n",
       "  'neurosurfer.agents.react.agent',\n",
       "  'neurosurfer.agents.react.base',\n",
       "  'neurosurfer.agents.react.config',\n",
       "  'neurosurfer.agents.react.exceptions',\n",
       "  'neurosurfer.agents.react.history',\n",
       "  'neurosurfer.agents.react.memory',\n",
       "  'neurosurfer.agents.react.parser',\n",
       "  'neurosurfer.agents.react.retry',\n",
       "  'neurosurfer.agents.react.scratchpad',\n",
       "  'neurosurfer.agents.react.types',\n",
       "  'neurosurfer.agents.sql_agent',\n",
       "  'neurosurfer.agents.tools_router_agent',\n",
       "  'neurosurfer.cli.__init__',\n",
       "  'neurosurfer.cli.main',\n",
       "  'neurosurfer.cli.processes',\n",
       "  'neurosurfer.cli.serve',\n",
       "  'neurosurfer.cli.utils',\n",
       "  'neurosurfer.config',\n",
       "  'neurosurfer.db.__init__',\n",
       "  'neurosurfer.db.sql_database',\n",
       "  'neurosurfer.db.sql_schema_store',\n",
       "  'neurosurfer.diagnostics',\n",
       "  'neurosurfer.examples.quickstart_app',\n",
       "  'neurosurfer.logger',\n",
       "  'neurosurfer.models.chat_models.__init__',\n",
       "  'neurosurfer.models.chat_models.base',\n",
       "  'neurosurfer.models.chat_models.llamacpp',\n",
       "  'neurosurfer.models.chat_models.openai',\n",
       "  'neurosurfer.models.chat_models.transformers',\n",
       "  'neurosurfer.models.chat_models.unsloth',\n",
       "  'neurosurfer.models.embedders.__init__',\n",
       "  'neurosurfer.models.embedders.base',\n",
       "  'neurosurfer.models.embedders.llamacpp',\n",
       "  'neurosurfer.models.embedders.sentence_transformer',\n",
       "  'neurosurfer.runtime.checks',\n",
       "  'neurosurfer.runtime.paths',\n",
       "  'neurosurfer.server.__init__',\n",
       "  'neurosurfer.server.api.__init__',\n",
       "  'neurosurfer.server.api.api_auth',\n",
       "  'neurosurfer.server.api.api_chat_completions',\n",
       "  'neurosurfer.server.api.api_chats',\n",
       "  'neurosurfer.server.api.api_files',\n",
       "  'neurosurfer.server.app',\n",
       "  'neurosurfer.server.config',\n",
       "  'neurosurfer.server.db.__init__',\n",
       "  'neurosurfer.server.db.db',\n",
       "  'neurosurfer.server.db.models',\n",
       "  'neurosurfer.server.gunicorn.conf',\n",
       "  'neurosurfer.server.models_registry',\n",
       "  'neurosurfer.server.reset',\n",
       "  'neurosurfer.server.runtime',\n",
       "  'neurosurfer.server.schemas.__init__',\n",
       "  'neurosurfer.server.schemas.auth',\n",
       "  'neurosurfer.server.schemas.chats',\n",
       "  'neurosurfer.server.schemas.completions',\n",
       "  'neurosurfer.server.schemas.model_registry',\n",
       "  'neurosurfer.server.schemas.model_response',\n",
       "  'neurosurfer.server.security',\n",
       "  'neurosurfer.server.services.follow_up_questions',\n",
       "  'neurosurfer.server.services.rag.__init__',\n",
       "  'neurosurfer.server.services.rag.gate',\n",
       "  'neurosurfer.server.services.rag.ingestor',\n",
       "  'neurosurfer.server.services.rag.metadata_filter',\n",
       "  'neurosurfer.server.services.rag.models',\n",
       "  'neurosurfer.server.services.rag.orchestrator',\n",
       "  'neurosurfer.server.services.rag.summarizer',\n",
       "  'neurosurfer.server.utils',\n",
       "  'neurosurfer.tools.__init__',\n",
       "  'neurosurfer.tools.base_tool',\n",
       "  'neurosurfer.tools.common.__init__',\n",
       "  'neurosurfer.tools.common.general_query_assistant',\n",
       "  'neurosurfer.tools.mermaid.erd_diagram_generator',\n",
       "  'neurosurfer.tools.mermaid.mermaid_diagram_generator',\n",
       "  'neurosurfer.tools.rag.docs_generator',\n",
       "  'neurosurfer.tools.rag.simple_query_assitant',\n",
       "  'neurosurfer.tools.sql.__init__',\n",
       "  'neurosurfer.tools.sql.db_insights_tool',\n",
       "  'neurosurfer.tools.sql.final_answer_formatter',\n",
       "  'neurosurfer.tools.sql.relevant_tables_schema_retriever',\n",
       "  'neurosurfer.tools.sql.sql_executor',\n",
       "  'neurosurfer.tools.sql.sql_query_generator',\n",
       "  'neurosurfer.tools.tool_spec',\n",
       "  'neurosurfer.tools.toolkit',\n",
       "  'neurosurfer.tools.websearch.__init__',\n",
       "  'neurosurfer.tools.websearch.config',\n",
       "  'neurosurfer.tools.websearch.engines.__init__',\n",
       "  'neurosurfer.tools.websearch.engines.base',\n",
       "  'neurosurfer.tools.websearch.engines.serpapi',\n",
       "  'neurosurfer.tools.websearch.extractor',\n",
       "  'neurosurfer.tools.websearch.rag',\n",
       "  'neurosurfer.tools.websearch.templates',\n",
       "  'neurosurfer.tools.websearch.tool',\n",
       "  'neurosurfer.tools.websearch.utils',\n",
       "  'neurosurfer.tracing.__init__',\n",
       "  'neurosurfer.tracing.models',\n",
       "  'neurosurfer.tracing.span',\n",
       "  'neurosurfer.tracing.step_context',\n",
       "  'neurosurfer.tracing.tracer',\n",
       "  'neurosurfer.utils.__init__',\n",
       "  'neurosurfer.utils.helper',\n",
       "  'neurosurfer.utils.prompts',\n",
       "  'neurosurfer.utils.response_wrappers',\n",
       "  'neurosurfer.vectorstores.__init__',\n",
       "  'neurosurfer.vectorstores.base',\n",
       "  'neurosurfer.vectorstores.chroma',\n",
       "  'neurosurfer.vectorstores.in_memory_store',\n",
       "  'neurosurfer.version'],\n",
       " 'summary': {'python_file_count': 140,\n",
       "  'doc_file_count': 0,\n",
       "  'package_count': 29,\n",
       "  'module_count': 140}}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neurosurfer_labs.tools.doc_gen.dir_scanning import DirectoryScanTool\n",
    "from neurosurfer_labs.tools.doc_gen.code_symbol_index import CodeSymbolIndexTool\n",
    "from neurosurfer.tools.toolkit import Toolkit\n",
    "\n",
    "root_dir = \"../neurosurfer\"\n",
    "dir_scan_tool = DirectoryScanTool()\n",
    "code_index_tool = CodeSymbolIndexTool()\n",
    "\n",
    "toolkit = Toolkit(tools=[dir_scan_tool, code_index_tool])\n",
    "\n",
    "params = {\n",
    "    \"project_root\": root_dir,\n",
    "    \"docs_root\": \"docs\",\n",
    "}\n",
    "\n",
    "scan_results = dir_scan_tool(project_root=root_dir)\n",
    "scan_results.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "303ddfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modules': {'neurosurfer.version': {'path': 'version.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/version.py',\n",
       "   'package': 'neurosurfer',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.config': {'path': 'config.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/config.py',\n",
       "   'package': 'neurosurfer',\n",
       "   'classes': [{'name': 'BaseModelConfig',\n",
       "     'docstring': 'Base configuration shared across all model types.\\n\\nAll model configs inherit these common parameters.\\nCan be extended with model-specific settings.',\n",
       "     'methods': [{'name': 'to_dict',\n",
       "       'docstring': 'Convert config to dictionary for unpacking into model __init__.\\n\\nArgs:\\n    exclude_none: Exclude fields with None values\\n    \\nReturns:\\n    Dictionary of configuration parameters',\n",
       "       'args': ['self', 'exclude_none']}]},\n",
       "    {'name': 'AppConfig',\n",
       "     'docstring': 'General application configuration.\\n\\nManages application-level settings like versioning, networking,\\nfile paths, and runtime behavior.',\n",
       "     'methods': [{'name': 'get_dynamic_host_ip',\n",
       "       'docstring': 'Get interface IP dynamically',\n",
       "       'args': ['self']},\n",
       "      {'name': 'host_url',\n",
       "       'docstring': 'Construct full host URL',\n",
       "       'args': ['self']},\n",
       "      {'name': 'vector_store_path',\n",
       "       'docstring': 'Vector store storage path',\n",
       "       'args': ['self']},\n",
       "      {'name': 'database_url',\n",
       "       'docstring': 'SQLite database URL',\n",
       "       'args': ['self']}]},\n",
       "    {'name': 'DatabaseConfig',\n",
       "     'docstring': 'External database connection configuration.\\n\\nFor SQL Server or other external databases used by the SQL Agent.',\n",
       "     'methods': []},\n",
       "    {'name': 'ChunkerConfig',\n",
       "     'docstring': 'Configuration for the GenericCodeChunker.\\nControls chunking sizes, overlaps, and fallback behavior\\nfor both line-based (code-friendly) and char-based (generic text) splitting.',\n",
       "     'methods': []},\n",
       "    {'name': 'Config',\n",
       "     'docstring': 'Main configuration class aggregating all sub-configurations.\\n\\nThis is the primary entry point for all application configuration.\\nProvides structured access to app, database, model, and processing configs.\\n\\nUsage:\\n    # Initialize config (loads from .env automatically)\\n    config = Config()\\n    \\n    # Access app settings\\n    print(config.app.host_url)\\n    \\n    # Configure and instantiate a model\\n    config.model.unsloth.model_name = \"custom/model\"\\n    config.model.unsloth.max_seq_length = 16000\\n    model = UnslothModel(**config.model.unsloth.to_dict())\\n\\nAttributes:\\n    app: Application-level configuration\\n    database: External database configuration\\n    chunker: Chunker configuration',\n",
       "     'methods': [{'name': 'get_logger',\n",
       "       'docstring': 'Get a configured logger instance.\\n\\nArgs:\\n    name: Logger name (typically __name__)\\n    \\nReturns:\\n    Configured logger instance',\n",
       "       'args': ['self', 'name']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.diagnostics': {'path': 'diagnostics.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/diagnostics.py',\n",
       "   'package': 'neurosurfer',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'main', 'docstring': None, 'args': ['argv']}]},\n",
       "  'neurosurfer.logger': {'path': 'logger.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/logger.py',\n",
       "   'package': 'neurosurfer',\n",
       "   'classes': [{'name': 'ColorFormatter',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'format',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'record']}]}],\n",
       "   'functions': [{'name': 'configure_logging',\n",
       "     'docstring': None,\n",
       "     'args': []}]},\n",
       "  'neurosurfer.__init__': {'path': '__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/__init__.py',\n",
       "   'package': 'neurosurfer',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.db.__init__': {'path': 'db/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/db/__init__.py',\n",
       "   'package': 'neurosurfer.db',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.db.sql_database': {'path': 'db/sql_database.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/db/sql_database.py',\n",
       "   'package': 'neurosurfer.db',\n",
       "   'classes': [{'name': 'SQLDatabase',\n",
       "     'docstring': 'Production-grade SQLAlchemy database wrapper.\\n\\nThis class provides a high-level interface for database operations,\\nincluding schema introspection, query execution, and metadata caching.\\nIt\\'s designed for use with SQL agents and tools.\\n\\nFeatures:\\n    - Automatic schema introspection\\n    - Table filtering (include/ignore)\\n    - Sample data for context\\n    - Metadata caching for performance\\n    - View support\\n    - Safe query execution\\n\\nAttributes:\\n    _engine (Engine): SQLAlchemy engine\\n    _schema (Optional[str]): Database schema name\\n    _inspector: SQLAlchemy inspector\\n    _all_tables (set): All available tables\\n    _usable_tables (set): Filtered tables based on include/ignore\\n\\nExample:\\n    >>> db = SQLDatabase(\\n    ...     database_uri=\"sqlite:///mydb.db\",\\n    ...     include_tables=[\"users\", \"products\"],\\n    ...     sample_rows_in_table_info=3\\n    ... )\\n    >>> \\n    >>> # Get schema\\n    >>> schema_info = db.get_table_info()\\n    >>> \\n    >>> # Execute query\\n    >>> results = db.run(\"SELECT COUNT(*) FROM users\")\\n    >>> \\n    >>> # Get table names\\n    >>> tables = db.get_usable_table_names()',\n",
       "     'methods': [{'name': 'dialect',\n",
       "       'docstring': 'Return string representation of dialect to use.',\n",
       "       'args': ['self']},\n",
       "      {'name': 'get_usable_table_names',\n",
       "       'docstring': 'Get names of tables available.',\n",
       "       'args': ['self']},\n",
       "      {'name': 'get_table_info',\n",
       "       'docstring': 'Get information about specified tables.\\nFollows best practices as specified in: Rajkumar et al, 2022\\n(https://arxiv.org/abs/2204.00498)\\nIf `sample_rows_in_table_info`, the specified number of sample rows will be\\nappended to each table description. This can increase performance as\\ndemonstrated in the paper.',\n",
       "       'args': ['self', 'table_names']},\n",
       "      {'name': 'build_connection_string',\n",
       "       'docstring': None,\n",
       "       'args': ['server',\n",
       "        'database',\n",
       "        'username',\n",
       "        'password',\n",
       "        'driver',\n",
       "        'port',\n",
       "        'odbc_driver']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.db.sql_schema_store': {'path': 'db/sql_schema_store.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/db/sql_schema_store.py',\n",
       "   'package': 'neurosurfer.db',\n",
       "   'classes': [{'name': 'SQLSchemaStore',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'train',\n",
       "       'docstring': 'Trains the schema summarizer by extracting schema data and optionally generating LLM summaries.\\nsummarize:\\n    - False: Store raw schema only.\\n    - True: Add LLM-generated short summary with schema.\\nforce:\\n    - If True, flushes existing storage and retrains all tables.',\n",
       "       'args': ['self', 'summarize', 'force']},\n",
       "      {'name': 'summarize_schema__',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'table_name', 'schema']},\n",
       "      {'name': 'get_all_table_schemas', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'save_to_file', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'load_from_file', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'get_table_data', 'docstring': None, 'args': ['self', 'table']},\n",
       "      {'name': 'get_db_name', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'get_tables_count', 'docstring': None, 'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.examples.quickstart_app': {'path': 'examples/quickstart_app.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/examples/quickstart_app.py',\n",
       "   'package': 'neurosurfer.examples',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'cleanup',\n",
       "     'docstring': 'Clean up temporary files and directories on application shutdown.',\n",
       "     'args': []},\n",
       "    {'name': 'handler',\n",
       "     'docstring': 'Process chat completion requests with RAG-enhanced context.\\n\\nThis is the main chat handler that processes incoming chat requests, optionally\\nenhances them with relevant context from uploaded documents using RAG, and\\ngenerates responses using the configured language model.\\n\\nArgs:\\n    args (ChatHandlerModel): The chat handler model contains:\\n        - user_id: User/session identifier\\n        - thread_id: Session/thread identifier for context management\\n        - message_id: Message identifier for context management\\n        - has_files_message: Whether the message contains files\\n        - model: The model to use for generation, selected from the UI\\n        - messages: ChatHandlerMessages\\n            - user_query: Last user message\\n            - user_msgs: List of user messages\\n            - assistant_msgs: List of assistant messages\\n            - system_msgs: List of system messages\\n            - converstaion: List of conversation messages\\n        - temperature: Sampling temperature for response generation\\n        - max_tokens: Maximum tokens to generate in response\\n        - stream: Whether to stream the response\\n        - system_prompt: Optional system prompt for generation\\n        - tools: Optional list of tools to use for generation\\n        - tool_choice: Optional tool choice for generation\\n        - metadata: Optional metadata for generation\\n        - files: Optional list of uploaded files for context\\n\\nReturns:\\n    The AppResponseModel, which can be either:\\n        - Complete response object (non-streaming)\\n        - Streaming response generator (streaming mode)\\n\\nProcessing Flow:\\n    1. Extract user messages, system messages, and conversation history\\n    2. Apply RAG enhancement if files/context available for the thread\\n    3. Configure generation parameters (temperature, max_tokens)\\n    4. Call LLM with enhanced query and chat history\\n\\nRAG Enhancement:\\n    - Checks if RAG system is available and thread_id is provided\\n    - Applies document retrieval and context injection\\n    - Logs RAG usage statistics (similarity scores, usage decisions)\\n    - Falls back to original query if no relevant context found\\n\\nConfiguration:\\n    - Uses DEFAULT_SYSTEM_PROMPT if no system message provided\\n    - Applies temperature/max_tokens limits with fallbacks to config defaults\\n    - Maintains recent chat history (last 10 messages by default)\\n\\nNote:\\n    - Thread-based context allows for persistent conversations\\n    - File uploads are processed and converted to document chunks\\n    - Streaming responses are supported for real-time interaction\\n    - RAG context injection happens before LLM generation',\n",
       "     'args': ['args']},\n",
       "    {'name': 'create_app', 'docstring': None, 'args': []},\n",
       "    {'name': 'main', 'docstring': None, 'args': []}]},\n",
       "  'neurosurfer.vectorstores.in_memory_store': {'path': 'vectorstores/in_memory_store.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/vectorstores/in_memory_store.py',\n",
       "   'package': 'neurosurfer.vectorstores',\n",
       "   'classes': [{'name': 'InMemoryVectorStore',\n",
       "     'docstring': 'Minimal, production-friendly baseline. Replace with FAISS, PGVecto, Chroma, Milvus, etc.',\n",
       "     'methods': [{'name': 'add_documents',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'docs']},\n",
       "      {'name': 'similarity_search',\n",
       "       'docstring': None,\n",
       "       'args': ['self',\n",
       "        'query_embedding',\n",
       "        'top_k',\n",
       "        'metadata_filter',\n",
       "        'similarity_threshold']},\n",
       "      {'name': 'count', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'list_all_documents',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'metadata_filter']},\n",
       "      {'name': 'delete_collection', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'clear_collection', 'docstring': None, 'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.vectorstores.base': {'path': 'vectorstores/base.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/vectorstores/base.py',\n",
       "   'package': 'neurosurfer.vectorstores',\n",
       "   'classes': [{'name': 'Doc',\n",
       "     'docstring': 'Document data structure for vector stores.\\n\\nRepresents a single document with its text content, embedding vector,\\nand associated metadata. Used throughout Neurosurfer\\'s RAG and retrieval systems.\\n\\nAttributes:\\n    id (str): Unique identifier for the document\\n    text (str): The actual text content of the document\\n    embedding (Optional[List[float]]): Vector embedding of the text. None if not yet embedded.\\n    metadata (Dict[str, Any]): Additional metadata (e.g., filename, source, chunk_idx, etc.)\\n\\nExample:\\n    >>> doc = Doc(\\n    ...     id=\"doc_123\",\\n    ...     text=\"Machine learning is a subset of AI.\",\\n    ...     embedding=[0.1, 0.2, 0.3, ...],\\n    ...     metadata={\"source\": \"textbook.pdf\", \"page\": 42}\\n    ... )',\n",
       "     'methods': []},\n",
       "    {'name': 'BaseVectorDB',\n",
       "     'docstring': 'Abstract base class for all vector database implementations in Neurosurfer.\\n\\nThis class defines a unified interface for vector stores, enabling seamless\\nswitching between different backends (Chroma, in-memory, etc.) without\\nchanging application code.\\n\\nCore Operations:\\n    - add_documents(): Store documents with embeddings\\n    - similarity_search(): Find similar documents using vector similarity\\n    - list_all_documents(): Retrieve all documents (with optional filtering)\\n    - delete_documents(): Remove specific documents\\n    - clear_collection(): Remove all documents from collection\\n    - delete_collection(): Delete the entire collection\\n\\nAbstract Methods:\\n    All methods must be implemented by concrete vector store classes.\\n\\nExample:\\n    >>> class MyVectorDB(BaseVectorDB):\\n    ...     def add_documents(self, docs):\\n    ...         # Implementation\\n    ...         pass\\n    ...     # ... implement other methods\\n    >>> \\n    >>> vectordb = MyVectorDB()\\n    >>> vectordb.add_documents([doc1, doc2])\\n    >>> results = vectordb.similarity_search(query_embedding, top_k=5)',\n",
       "     'methods': [{'name': 'add_documents',\n",
       "       'docstring': 'Add documents with embeddings to the vector store.\\n\\nThis method stores documents along with their vector embeddings and metadata.\\nDocuments can be retrieved later using similarity_search().\\n\\nArgs:\\n    docs (List[Doc]): List of Doc objects to add. Each Doc must have:\\n        - id: Unique identifier\\n        - text: Document text content\\n        - embedding: Vector embedding (List[float])\\n        - metadata: Optional metadata dict\\n\\nRaises:\\n    NotImplementedError: If not implemented by subclass\\n\\nExample:\\n    >>> docs = [\\n    ...     Doc(id=\"1\", text=\"Hello\", embedding=[0.1, 0.2], metadata={\"source\": \"file.txt\"}),\\n    ...     Doc(id=\"2\", text=\"World\", embedding=[0.3, 0.4], metadata={\"source\": \"file.txt\"})\\n    ... ]\\n    >>> vectordb.add_documents(docs)',\n",
       "       'args': ['self', 'docs']},\n",
       "      {'name': 'similarity_search',\n",
       "       'docstring': 'Perform similarity search to find documents most similar to the query.\\n\\nUses vector similarity (typically cosine similarity) to find and rank\\ndocuments by their relevance to the query embedding.\\n\\nArgs:\\n    query_embedding (List[float]): Query vector to search for\\n    top_k (int): Maximum number of results to return. Default: 5\\n    metadata_filter (Optional[Dict[str, Any]]): Filter results by metadata.\\n        Only documents matching all key-value pairs are returned. Default: None\\n    similarity_threshold (Optional[float]): Minimum similarity score (0.0-1.0).\\n        Documents below this threshold are excluded. Default: None\\n\\nReturns:\\n    List[Tuple[Doc, float]]: List of (document, similarity_score) tuples,\\n        sorted by similarity in descending order (most similar first)\\n\\nRaises:\\n    NotImplementedError: If not implemented by subclass\\n\\nExample:\\n    >>> query_emb = embedder.embed(\"machine learning\")\\n    >>> results = vectordb.similarity_search(\\n    ...     query_embedding=query_emb,\\n    ...     top_k=10,\\n    ...     metadata_filter={\"category\": \"AI\"},\\n    ...     similarity_threshold=0.7\\n    ... )\\n    >>> for doc, score in results:\\n    ...     print(f\"Score: {score:.3f} - {doc.text[:50]}\")',\n",
       "       'args': ['self',\n",
       "        'query_embedding',\n",
       "        'top_k',\n",
       "        'metadata_filter',\n",
       "        'similarity_threshold']},\n",
       "      {'name': 'count',\n",
       "       'docstring': 'Get the total number of documents in the collection.\\n\\nReturns:\\n    int: Number of documents currently stored\\n\\nRaises:\\n    NotImplementedError: If not implemented by subclass\\n\\nExample:\\n    >>> count = vectordb.count()\\n    >>> print(f\"Collection contains {count} documents\")',\n",
       "       'args': ['self']},\n",
       "      {'name': 'list_all_documents',\n",
       "       'docstring': 'Retrieve all documents from the collection.\\n\\nArgs:\\n    metadata_filter (Optional[Dict[str, Any]]): Filter by metadata.\\n        Only documents matching all key-value pairs are returned. Default: None\\n\\nReturns:\\n    List[Doc]: List of all documents (or filtered subset)\\n\\nRaises:\\n    NotImplementedError: If not implemented by subclass\\n\\nExample:\\n    >>> # Get all documents\\n    >>> all_docs = vectordb.list_all_documents()\\n    >>> \\n    >>> # Get documents from specific source\\n    >>> filtered_docs = vectordb.list_all_documents(\\n    ...     metadata_filter={\"source\": \"manual.pdf\"}\\n    ... )',\n",
       "       'args': ['self', 'metadata_filter']},\n",
       "      {'name': 'delete_documents',\n",
       "       'docstring': 'Delete specific documents from the collection.\\n\\nArgs:\\n    docs (List[Doc]): List of documents to delete (matched by ID)\\n\\nRaises:\\n    NotImplementedError: If not implemented by subclass\\n\\nExample:\\n    >>> docs_to_delete = vectordb.list_all_documents(\\n    ...     metadata_filter={\"source\": \"old_file.txt\"}\\n    ... )\\n    >>> vectordb.delete_documents(docs_to_delete)',\n",
       "       'args': ['self', 'docs']},\n",
       "      {'name': 'delete_collection',\n",
       "       'docstring': 'Delete the entire collection permanently.\\n\\nThis removes the collection and all its documents from the vector store.\\nThe collection cannot be recovered after deletion.\\n\\nRaises:\\n    NotImplementedError: If not implemented by subclass\\n\\nExample:\\n    >>> vectordb.delete_collection()',\n",
       "       'args': ['self']},\n",
       "      {'name': 'clear_collection',\n",
       "       'docstring': 'Remove all documents from the collection.\\n\\nThis clears the collection but keeps it available for new documents.\\nTypically implemented by dropping and recreating the collection.\\n\\nRaises:\\n    NotImplementedError: If not implemented by subclass\\n\\nExample:\\n    >>> vectordb.clear_collection()\\n    >>> vectordb.count()  # Returns 0',\n",
       "       'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.vectorstores.__init__': {'path': 'vectorstores/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/vectorstores/__init__.py',\n",
       "   'package': 'neurosurfer.vectorstores',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.vectorstores.chroma': {'path': 'vectorstores/chroma.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/vectorstores/chroma.py',\n",
       "   'package': 'neurosurfer.vectorstores',\n",
       "   'classes': [{'name': 'ChromaVectorStore',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'add_documents',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'docs']},\n",
       "      {'name': 'similarity_search',\n",
       "       'docstring': None,\n",
       "       'args': ['self',\n",
       "        'query_embedding',\n",
       "        'top_k',\n",
       "        'metadata_filter',\n",
       "        'similarity_threshold']},\n",
       "      {'name': 'clear_collection', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'delete_documents', 'docstring': None, 'args': ['self', 'ids']},\n",
       "      {'name': 'delete_collection', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'count', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'list_all_documents',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'metadata_filter']}]}],\n",
       "   'functions': [{'name': 'DefaultEmbeddingFunction',\n",
       "     'docstring': None,\n",
       "     'args': []}]},\n",
       "  'neurosurfer.utils.helper': {'path': 'utils/helper.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/utils/helper.py',\n",
       "   'package': 'neurosurfer.utils',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'get_uploaded_files',\n",
       "     'docstring': None,\n",
       "     'args': ['project_path', 'extensions', 'exclude_dirs']},\n",
       "    {'name': 'retrieve_filtered_chunks',\n",
       "     'docstring': None,\n",
       "     'args': ['keywords', 'query', 'vector_db', 'top_k']},\n",
       "    {'name': 'keywords_filter_chunks',\n",
       "     'docstring': None,\n",
       "     'args': ['keywords', 'docs']},\n",
       "    {'name': 'create_context', 'docstring': None, 'args': ['results']},\n",
       "    {'name': 'build_chat_context',\n",
       "     'docstring': 'Builds a formatted conversation context string from the last 10 chat history records.\\n\\nArgs:\\n    chat_history (list): List of dicts like {\"role\": \"user\" or \"assistant\", \"content\": \"...\"}\\n\\nReturns:\\n    str: Formatted context string.',\n",
       "     'args': ['chat_history', 'n_recent_chats']},\n",
       "    {'name': 'chat_completion_wrapper',\n",
       "     'docstring': None,\n",
       "     'args': ['call_id', 'model_name', 'content']},\n",
       "    {'name': 'get_text_only_history',\n",
       "     'docstring': None,\n",
       "     'args': ['chat_history', 'num_recent_chats']},\n",
       "    {'name': 'generate_folder_structure',\n",
       "     'docstring': 'Generate a tree-like folder structure.\\nSupports both directory paths and .zip files.\\nIf a zip is given, it is extracted to a temporary directory,\\nprocessed, and then deleted.',\n",
       "     'args': ['root_path', 'max_depth', 'exclude_dirs', 'supported_files']},\n",
       "    {'name': 'reconstruct_code_from_chunks',\n",
       "     'docstring': None,\n",
       "     'args': ['chunks', 'overlap_lines']},\n",
       "    {'name': 'is_prompt_like', 'docstring': None, 'args': ['text']},\n",
       "    {'name': 'mermaid_save_diagram',\n",
       "     'docstring': \"Save Mermaid diagram as an image using Mermaid CLI.\\nArgs:\\n    mermaid_code (str): Mermaid syntax code.\\n    output_path (str): File path to save the image.\\n    image_format (str): 'png' or 'svg'.\\n    background_color (str): Background color.\\n    scale (float): Scale multiplier.\",\n",
       "     'args': ['mermaid_code',\n",
       "      'output_path',\n",
       "      'image_format',\n",
       "      'background_color',\n",
       "      'scale',\n",
       "      'puppeteer_config_path',\n",
       "      'mmdc_path',\n",
       "      'logger']}]},\n",
       "  'neurosurfer.utils.response_wrappers': {'path': 'utils/response_wrappers.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/utils/response_wrappers.py',\n",
       "   'package': 'neurosurfer.utils',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'chat_completion_response_wrapper',\n",
       "     'docstring': None,\n",
       "     'args': ['id', 'model', 'content', 'stream', 'finish_reason']}]},\n",
       "  'neurosurfer.utils.prompts': {'path': 'utils/prompts.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/utils/prompts.py',\n",
       "   'package': 'neurosurfer.utils',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.utils.__init__': {'path': 'utils/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/utils/__init__.py',\n",
       "   'package': 'neurosurfer.utils',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tracing.step_context': {'path': 'tracing/step_context.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tracing/step_context.py',\n",
       "   'package': 'neurosurfer.tracing',\n",
       "   'classes': [{'name': 'TraceStepContext',\n",
       "     'docstring': 'Context manager returned by Tracer.step().\\n\\n- On __enter__:\\n    * records start time\\n    * optionally logs a span \"â–¶ step.<kind>\"\\n- On __exit__:\\n    * sets duration_ms\\n    * sets ok/error\\n    * records a TraceStep via Tracer._record_step()\\n    * does NOT suppress exceptions',\n",
       "     'methods': [{'name': 'set_error',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'error']},\n",
       "      {'name': 'add_meta', 'docstring': None, 'args': ['self', '**kwargs']},\n",
       "      {'name': 'outputs',\n",
       "       'docstring': 'Add arbitrary key/value pairs to `outputs`.\\n\\nTypical usage:\\n    t.outputs(output=normalized_result)\\n    t.outputs(system_prompt=sys, user_prompt=usr)',\n",
       "       'args': ['self', '**kwargs']},\n",
       "      {'name': 'inputs',\n",
       "       'docstring': 'Add arbitrary key/value pairs to `inputs`.\\n\\nTypical usage:\\n    t.inputs(output=normalized_result)\\n    t.inputs(system_prompt=sys, user_prompt=usr)',\n",
       "       'args': ['self', '**kwargs']},\n",
       "      {'name': 'stream',\n",
       "       'docstring': 'Add an internal log line to this step.\\n\\n- Stored in the structured trace result (step.logs)\\n- Printed at the same indentation level as the step spans\\n  when `log_steps=True`.',\n",
       "       'args': ['self', 'message', 'type', '**data']},\n",
       "      {'name': 'log',\n",
       "       'docstring': 'Add an internal log line to this step.\\n\\n- Stored in the structured trace result (step.logs)\\n- Printed at the same indentation level as the step spans\\n  when `log_steps=True`.',\n",
       "       'args': ['self', 'message', 'type', 'type_keyword', '**data']},\n",
       "      {'name': 'start', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'is_closed', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'close', 'docstring': None, 'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tracing.models': {'path': 'tracing/models.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tracing/models.py',\n",
       "   'package': 'neurosurfer.tracing',\n",
       "   'classes': [{'name': 'TraceLog',\n",
       "     'docstring': 'A single log entry inside a traced step.',\n",
       "     'methods': []},\n",
       "    {'name': 'TraceStep',\n",
       "     'docstring': 'One traced step in a workflow.\\n\\nFlexible enough to cover:\\n  - LLM calls\\n  - tool calls\\n  - any custom step\\n\\nYou decide how to populate `inputs`, `outputs`, and `meta`.',\n",
       "     'methods': []},\n",
       "    {'name': 'TraceResult',\n",
       "     'docstring': 'Structured tracing results for a single run.\\n\\nYou can attach this object directly to Agent / GraphExecutor outputs.',\n",
       "     'methods': [{'name': 'summary', 'docstring': None, 'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tracing.span': {'path': 'tracing/span.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tracing/span.py',\n",
       "   'package': 'neurosurfer.tracing',\n",
       "   'classes': [{'name': 'SpanTracer',\n",
       "     'docstring': 'Low-level span tracer interface.\\n\\nThis is deliberately minimal: it just exposes a `span(name, attrs)` method\\nthat returns a context manager. Span tracers are used by higher-level\\ntracing utilities (e.g., Tracer) to emit human-readable logs.',\n",
       "     'methods': [{'name': 'span',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'name', 'attrs']}]},\n",
       "    {'name': 'ConsoleTracer',\n",
       "     'docstring': \"Span tracer that writes to stdout using `print`.\\n\\nExample line:\\n    [trace] â–¶ step.llm step_id=1 label='agent.llm.ask'\",\n",
       "     'methods': [{'name': 'span',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'name', 'attrs']}]},\n",
       "    {'name': 'LoggerTracer',\n",
       "     'docstring': 'Span tracer that writes to a Python `logging.Logger`.\\n\\nEach span line is logged at INFO level.',\n",
       "     'methods': [{'name': 'span',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'name', 'attrs']}]},\n",
       "    {'name': 'MemorySpanTracer',\n",
       "     'docstring': 'Span tracer that captures span events in-memory.\\n\\nMostly useful for tests or debugging.',\n",
       "     'methods': [{'name': 'span',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'name', 'attrs']}]},\n",
       "    {'name': 'NullSpanTracer',\n",
       "     'docstring': 'No-op span tracer.\\n\\nAll spans are ignored; useful when you want structured tracing\\nbut no human-readable logs.',\n",
       "     'methods': [{'name': 'span',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'name', 'attrs']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tracing.tracer': {'path': 'tracing/tracer.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tracing/tracer.py',\n",
       "   'package': 'neurosurfer.tracing',\n",
       "   'classes': [{'name': 'TracerConfig',\n",
       "     'docstring': 'Configuration options for Tracer.\\n\\nAttributes:\\n    enabled:\\n        If False, `step(...)` becomes a no-op. Your code can always call it.\\n    log_steps:\\n        If True, each step prints human-readable spans via `span_tracer`.\\n    max_output_preview_chars:\\n        When you store large outputs in `t.add(...)`, you may choose to\\n        also store a shortened preview in `outputs[\"preview\"]`.\\n        This class itself doesn\\'t enforce truncation â€” it\\'s up to your\\n        usage â€” but the parameter is here for convenience / future use.',\n",
       "     'methods': []},\n",
       "    {'name': 'Tracer',\n",
       "     'docstring': 'Unified, simple tracing class for agents / graphs.\\n\\nUsage example inside an Agent:\\n\\n    class Agent:\\n        def __init__(..., enable_tracing: bool = False, log_tracing_steps: bool = False):\\n            self.tracer = Tracer(\\n                config=TracerConfig(\\n                    enabled=enable_tracing,\\n                    log_steps=log_tracing_steps,\\n                ),\\n                span_tracer=RichTracer() if log_tracing_steps else NullSpanTracer(),\\n                meta={\"agent_type\": \"generic_agent\"},\\n            )\\n\\n        def run(...):\\n            inputs = {...}\\n            with self.tracer.step(\\n                kind=\"llm\",\\n                label=\"agent.llm.ask\",\\n                inputs=inputs,\\n                node_id=context.get(\"node_id\"),\\n                agent_id=\"main_agent\",\\n            ) as t:\\n                res = self.llm.ask(**inputs)\\n                norm = normalize_tool_observation(res)\\n                t.add(output=norm, raw_response=res)\\n\\n            return AgentResult(\\n                ...,\\n                traces=self.tracer.results,\\n            )\\n\\nKey properties:\\n  - If `config.enabled=False`, `step(...)` returns a no-op context manager\\n    and no steps are recorded â€” your calling code doesn\\'t change.\\n  - Each step has:\\n      * auto-incremented step_id\\n      * timing (started_at, duration_ms)\\n      * ok/error (based on exceptions)\\n      * inputs/outputs/meta dicts\\n  - `results` returns a `TraceResult` Pydantic model.',\n",
       "     'methods': [{'name': 'meta',\n",
       "       'docstring': 'Metadata attached to the whole trace run (e.g. graph_name, run_id).',\n",
       "       'args': ['self']},\n",
       "      {'name': 'set_meta',\n",
       "       'docstring': 'Update global metadata for this tracer.',\n",
       "       'args': ['self', '**kwargs']},\n",
       "      {'name': 'results',\n",
       "       'docstring': 'Structured tracing results for this run.',\n",
       "       'args': ['self']},\n",
       "      {'name': 'reset', 'docstring': None, 'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tracing.__init__': {'path': 'tracing/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tracing/__init__.py',\n",
       "   'package': 'neurosurfer.tracing',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.models.embedders.sentence_transformer': {'path': 'models/embedders/sentence_transformer.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/embedders/sentence_transformer.py',\n",
       "   'package': 'neurosurfer.models.embedders',\n",
       "   'classes': [{'name': 'SentenceTransformerEmbedder',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'embed',\n",
       "       'docstring': None,\n",
       "       'args': ['self',\n",
       "        'query',\n",
       "        'convert_to_tensor',\n",
       "        'normalize_embeddings']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.models.embedders.base': {'path': 'models/embedders/base.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/embedders/base.py',\n",
       "   'package': 'neurosurfer.models.embedders',\n",
       "   'classes': [{'name': 'BaseEmbedder',\n",
       "     'docstring': 'Abstract base class for all embedding models in Neurosurfer.\\n\\nThis class provides a unified interface for generating text embeddings\\nfrom various backends. Embeddings are used for semantic search, RAG,\\nand similarity computations.\\n\\nAttributes:\\n    logger (logging.Logger): Logger instance for debugging\\n    model: The underlying embedding model instance (implementation-specific)\\n\\nAbstract Methods:\\n    embed(): Generate embeddings for single or multiple text inputs\\n\\nExample:\\n    >>> class MyEmbedder(BaseEmbedder):\\n    ...     def embed(self, query, **kwargs):\\n    ...         # Generate embeddings\\n    ...         return embeddings\\n    >>> \\n    >>> embedder = MyEmbedder()\\n    >>> embedding = embedder.embed(\"Hello world\")\\n    >>> embeddings = embedder.embed([\"Text 1\", \"Text 2\"])',\n",
       "     'methods': [{'name': 'embed',\n",
       "       'docstring': 'Generate embeddings for text input(s).\\n\\nThis method must be implemented by all concrete embedder classes.\\nIt should handle both single strings and lists of strings.\\n\\nArgs:\\n    query (Union[str, List[str]]): Single text string or list of text strings\\n    **kwargs: Additional embedder-specific parameters (e.g., normalize_embeddings)\\n\\nReturns:\\n    Union[List[float], List[List[float]]]:\\n        - If query is str: Returns List[float] (single embedding vector)\\n        - If query is List[str]: Returns List[List[float]] (list of embedding vectors)\\n\\nExample:\\n    >>> # Single text\\n    >>> embedding = embedder.embed(\"Hello world\")\\n    >>> len(embedding)  # e.g., 384 for all-MiniLM-L6-v2\\n    384\\n    \\n    >>> # Multiple texts\\n    >>> embeddings = embedder.embed([\"Text 1\", \"Text 2\"])\\n    >>> len(embeddings)\\n    2',\n",
       "       'args': ['self', 'query', '**kwargs']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.models.embedders.llamacpp': {'path': 'models/embedders/llamacpp.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/embedders/llamacpp.py',\n",
       "   'package': 'neurosurfer.models.embedders',\n",
       "   'classes': [{'name': 'LlamaCppEmbedder',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'embed',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'query', 'normalize_embeddings']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.models.embedders.__init__': {'path': 'models/embedders/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/embedders/__init__.py',\n",
       "   'package': 'neurosurfer.models.embedders',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.models.chat_models.transformers': {'path': 'models/chat_models/transformers.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/chat_models/transformers.py',\n",
       "   'package': 'neurosurfer.models.chat_models',\n",
       "   'classes': [{'name': 'StopSignalCriteria',\n",
       "     'docstring': None,\n",
       "     'methods': []},\n",
       "    {'name': 'TransformersModel',\n",
       "     'docstring': 'HF Transformers local model - now returns Pydantic models',\n",
       "     'methods': [{'name': 'stop_generation',\n",
       "       'docstring': None,\n",
       "       'args': ['self']},\n",
       "      {'name': 'set_stop_words', 'docstring': None, 'args': ['self', 'stops']},\n",
       "      {'name': 'init_model',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'load_in_4bit', '**kwargs']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.models.chat_models.base': {'path': 'models/chat_models/base.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/chat_models/base.py',\n",
       "   'package': 'neurosurfer.models.chat_models',\n",
       "   'classes': [{'name': 'BaseChatModel',\n",
       "     'docstring': 'Abstract base class for all chat models in Neurosurfer.\\n\\nThis class provides a unified interface for interacting with different LLM backends\\nwhile maintaining OpenAI-compatible response formats. All model implementations\\n(Transformers, Unsloth, vLLM, LlamaCpp, OpenAI) inherit from this class.\\n\\nAttributes:\\n    model_name (str): Identifier for the model (default: \"local-gpt\")\\n    verbose (bool): Enable verbose logging\\n    logger (logging.Logger): Logger instance for debugging\\n    call_id (str): Unique identifier for each generation call\\n    lock (Lock): Thread lock for concurrent access control\\n    model: The underlying model instance (implementation-specific)\\n    max_seq_length (int): Maximum context window size in tokens\\n\\nAbstract Methods:\\n    init_model(): Initialize the underlying model and tokenizer\\n    _call(): Perform non-streaming generation\\n    _stream(): Perform streaming generation\\n    stop_generation(): Stop ongoing generation\\n\\nExample:\\n    >>> class MyModel(BaseChatModel):\\n    ...     def init_model(self):\\n    ...         # Load model\\n    ...         pass\\n    ...     \\n    ...     def _call(self, user_prompt, system_prompt, **kwargs):\\n    ...         # Generate response\\n    ...         return self._final_nonstream_response(...)\\n    ...     \\n    ...     def _stream(self, user_prompt, system_prompt, **kwargs):\\n    ...         # Stream response\\n    ...         for token in tokens:\\n    ...             yield self._delta_chunk(...)\\n    ...         yield self._stop_chunk(...)',\n",
       "     'methods': [{'name': 'set_stop_signal',\n",
       "       'docstring': 'Set stop signal to halt generation',\n",
       "       'args': ['self']},\n",
       "      {'name': 'reset_stop_signal',\n",
       "       'docstring': 'Reset stop signal before new generation',\n",
       "       'args': ['self']},\n",
       "      {'name': 'init_model', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'ask',\n",
       "       'docstring': 'Main entry point for generating model responses.\\n\\nThis method provides a unified interface for both streaming and non-streaming\\ngeneration. It automatically routes to _call() or _stream() based on the\\nstream parameter.\\n\\nArgs:\\n    user_prompt (str): The user\\'s input message/question\\n    system_prompt (str): System-level instructions for the model.\\n        Default: \"You are a helpful assistant. Answer questions to the best of your ability.\"\\n    chat_history (List[dict]): Conversation history as list of message dicts.\\n        Each dict should have \\'role\\' and \\'content\\' keys. Default: []\\n    temperature (float): Sampling temperature (0.0-2.0). Lower = more deterministic,\\n        higher = more creative. Default: 0.7\\n    max_new_tokens (int): Maximum number of tokens to generate. Default: 2000\\n    stream (bool): Enable streaming response. Default: False\\n    **kwargs: Additional model-specific generation parameters\\n\\nReturns:\\n    LLM_RESPONSE_TYPE:\\n        - If stream=False: Returns ChatCompletionResponse (Pydantic model)\\n        - If stream=True: Returns Generator yielding ChatCompletionChunk objects\\n\\nExample:\\n    >>> # Non-streaming\\n    >>> response = model.ask(\"What is AI?\", temperature=0.5)\\n    >>> print(response.choices[0].message.content)\\n    \\n    >>> # Streaming\\n    >>> for chunk in model.ask(\"Explain quantum computing\", stream=True):\\n    ...     print(chunk.choices[0].delta.content, end=\"\")            ',\n",
       "       'args': ['self',\n",
       "        'user_prompt',\n",
       "        'system_prompt',\n",
       "        'chat_history',\n",
       "        'temperature',\n",
       "        'max_new_tokens',\n",
       "        'stream',\n",
       "        '**kwargs']},\n",
       "      {'name': 'stop_generation', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'silent', 'docstring': None, 'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.models.chat_models.unsloth': {'path': 'models/chat_models/unsloth.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/chat_models/unsloth.py',\n",
       "   'package': 'neurosurfer.models.chat_models',\n",
       "   'classes': [{'name': 'StopSignalCriteria',\n",
       "     'docstring': 'Custom stopping criteria that checks a stop signal function',\n",
       "     'methods': []},\n",
       "    {'name': 'UnslothModel',\n",
       "     'docstring': 'Unsloth FastLanguageModel wrapper with Pydantic response models.\\n\\nFeatures:\\n- Returns ChatCompletionResponse for non-streaming\\n- Yields ChatCompletionChunk for streaming\\n- Thread-safe stop signal\\n- Stop words support (truncates before stop sequence)\\n- Optional thinking mode with <think> tag suppression\\n- Token usage tracking',\n",
       "     'methods': [{'name': 'init_model',\n",
       "       'docstring': 'Initialize Unsloth model with specified configuration',\n",
       "       'args': ['self', '**kwargs']},\n",
       "      {'name': 'set_stop_words',\n",
       "       'docstring': 'Update stop words list',\n",
       "       'args': ['self', 'stops']},\n",
       "      {'name': 'stop_generation',\n",
       "       'docstring': 'Signal the model to stop generation immediately.\\nThread-safe operation.',\n",
       "       'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.models.chat_models.llamacpp': {'path': 'models/chat_models/llamacpp.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/chat_models/llamacpp.py',\n",
       "   'package': 'neurosurfer.models.chat_models',\n",
       "   'classes': [{'name': 'LlamaCppModel',\n",
       "     'docstring': 'llama.cpp model wrapper with Pydantic responses.\\n\\nFeatures:\\n- Supports both local GGUF files and HuggingFace repos\\n- Streaming and non-streaming completions\\n- Thread-safe generation\\n- Token usage tracking\\n- Stop signal support',\n",
       "     'methods': [{'name': 'init_model',\n",
       "       'docstring': 'Initialize llama.cpp model from local file or HuggingFace repo',\n",
       "       'args': ['self', '**kwargs']},\n",
       "      {'name': 'stop_generation',\n",
       "       'docstring': 'Signal to stop the current generation',\n",
       "       'args': ['self']},\n",
       "      {'name': 'set_stop_words',\n",
       "       'docstring': 'Update stop words list',\n",
       "       'args': ['self', 'stops']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.models.chat_models.__init__': {'path': 'models/chat_models/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/chat_models/__init__.py',\n",
       "   'package': 'neurosurfer.models.chat_models',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.models.chat_models.openai': {'path': 'models/chat_models/openai.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/models/chat_models/openai.py',\n",
       "   'package': 'neurosurfer.models.chat_models',\n",
       "   'classes': [{'name': 'OpenAIModel',\n",
       "     'docstring': 'An OpenAI/compatible chat client implementing BaseChatModel with Pydantic responses.\\n\\nWorks with:\\n  â€¢ OpenAI Cloud (leave base_url=None, set a real api_key)\\n  â€¢ LM Studio local server (e.g., base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\\n  â€¢ vLLM OpenAI server (e.g., base_url=\"http://localhost:8000/v1\")\\n  â€¢ Ollama OpenAI compat (e.g., base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")',\n",
       "     'methods': [{'name': 'init_model', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'strip_reasoning_text',\n",
       "       'docstring': 'Remove reasoning blocks from text',\n",
       "       'args': ['self', 'text']},\n",
       "      {'name': 'stop_generation',\n",
       "       'docstring': 'Set stop signal to halt generation',\n",
       "       'args': ['self']},\n",
       "      {'name': 'set_stop_words',\n",
       "       'docstring': 'Update stop words list',\n",
       "       'args': ['self', 'stops']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.runtime.checks': {'path': 'runtime/checks.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/runtime/checks.py',\n",
       "   'package': 'neurosurfer.runtime',\n",
       "   'classes': [{'name': 'DeviceInfo', 'docstring': None, 'methods': []}],\n",
       "   'functions': [{'name': 'detect_devices', 'docstring': None, 'args': []},\n",
       "    {'name': 'banner', 'docstring': None, 'args': ['info']},\n",
       "    {'name': 'print_banner_once', 'docstring': None, 'args': []},\n",
       "    {'name': 'warn_optional_llm_stack',\n",
       "     'docstring': 'Soft-check: warn if the LLM stack is missing. Called on import.',\n",
       "     'args': []},\n",
       "    {'name': 'require',\n",
       "     'docstring': 'Import a module or raise a RuntimeError with a helpful install hint.\\nUse in your code paths that need optional frameworks.',\n",
       "     'args': ['module', 'feature', 'install_hint']},\n",
       "    {'name': 'assert_minimum_runtime',\n",
       "     'docstring': 'Ensure core LLM runtime deps are present; call early in LLM/model startup paths.\\nKeep this OUT of global import to avoid hard-failing lightweight installs.',\n",
       "     'args': []},\n",
       "    {'name': 'diagnostic_report', 'docstring': None, 'args': []}]},\n",
       "  'neurosurfer.runtime.paths': {'path': 'runtime/paths.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/runtime/paths.py',\n",
       "   'package': 'neurosurfer.runtime',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'get_cache_dir',\n",
       "     'docstring': 'Return the per-user cache directory for Neurosurfer, creating it if needed.\\nRespects NEUROSURF_CACHE_DIR if set; otherwise uses platform-specific defaults.\\nLinux:  ~/.cache/Neurosurfer\\nmacOS:  ~/Library/Caches/Neurosurfer\\nWindows: %LOCALAPPDATA%\\\\Neurosurfer\\\\Cache (per platformdirs)',\n",
       "     'args': ['create', 'suffix']}]},\n",
       "  'neurosurfer.server.reset': {'path': 'server/reset.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/reset.py',\n",
       "   'package': 'neurosurfer.server',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'reset_db',\n",
       "     'docstring': 'Clear all chats and threads from the database. Only leave Users Information.',\n",
       "     'args': []}]},\n",
       "  'neurosurfer.server.models_registry': {'path': 'server/models_registry.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/models_registry.py',\n",
       "   'package': 'neurosurfer.server',\n",
       "   'classes': [{'name': 'ModelInfo', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ModelRegistry',\n",
       "     'docstring': 'Registry for managing AI models and their metadata.\\n\\nThis class provides a centralized registry for tracking available models,\\ntheir capabilities, and metadata. Used by the server to validate model\\nrequests and provide model information to clients.\\n\\nAttributes:\\n    _models (Dict[str, ModelCard]): Internal registry mapping model IDs to model cards\\n\\nExample:\\n    >>> registry = ModelRegistry()\\n    >>> \\n    >>> # Add models\\n    >>> registry.add(\\n    ...     id=\"llama-3-8b\",\\n    ...     family=\"Llama\",\\n    ...     provider=\"Meta\",\\n    ...     context_length=8192,\\n    ...     description=\"Llama 3 8B parameter model\"\\n    ... )\\n    >>> \\n    >>> # Retrieve model info\\n    >>> model = registry.get(\"llama-3-8b\")\\n    >>> print(f\"{model.family} - {model.context_length} tokens\")\\n    Llama - 8192 tokens\\n    >>> \\n    >>> # Check availability\\n    >>> registry.exists(\"gpt-4\")\\n    False',\n",
       "     'methods': [{'name': 'add',\n",
       "       'docstring': 'Register a new model in the registry.\\n\\nArgs:\\n    id (str): Unique model identifier\\n    family (str): Model family/architecture. Default: \"Unknown\"\\n    provider (Optional[str]): Model provider (e.g., \"OpenAI\", \"Meta\"). Default: None\\n    context_length (int): Maximum context window size in tokens. Default: None\\n    description (Optional[str]): Human-readable model description. Default: None\\n\\nRaises:\\n    ValueError: If model ID is already registered\\n\\nExample:\\n    >>> registry.add(\\n    ...     id=\"gpt-3.5-turbo\",\\n    ...     family=\"GPT\",\\n    ...     provider=\"OpenAI\",\\n    ...     context_length=4096,\\n    ...     description=\"Fast and efficient GPT-3.5 model\"\\n    ... )',\n",
       "       'args': ['self', 'llm', 'family', 'provider', 'description']},\n",
       "      {'name': 'get_first_available',\n",
       "       'docstring': 'Retrieve the first available model from the registry.\\n\\nReturns:\\n    ModelInfo: First available model in the registry\\n\\nRaises:\\n    ValueError: If no models are registered\\n\\nExample:\\n    >>> model = registry.get_first_available()\\n    >>> print(model.model_card.id)\\n    gpt-4',\n",
       "       'args': ['self']},\n",
       "      {'name': 'get',\n",
       "       'docstring': 'Retrieve model card by ID.\\n\\nArgs:\\n    model_id (str): Model identifier to look up\\n\\nReturns:\\n    ModelCard: Model card containing model metadata\\n\\nRaises:\\n    KeyError: If model ID is not found in registry\\n\\nExample:\\n    >>> model = registry.get(\"gpt-4\")\\n    >>> print(model.context_length)\\n    8192',\n",
       "       'args': ['self', 'model_id']},\n",
       "      {'name': 'exists',\n",
       "       'docstring': 'Check if a model is registered.\\n\\nArgs:\\n    model_id (str): Model identifier to check\\n\\nReturns:\\n    bool: True if model exists in registry, False otherwise\\n\\nExample:\\n    >>> registry.exists(\"gpt-4\")\\n    True\\n    >>> registry.exists(\"unknown-model\")\\n    False',\n",
       "       'args': ['self', 'model_id']},\n",
       "      {'name': 'all',\n",
       "       'docstring': 'Get all registered models.\\n\\nReturns:\\n    Dict[str, ModelCard]: Dictionary mapping model IDs to model cards\\n\\nExample:\\n    >>> all_models = registry.all()\\n    >>> for model_id, model_card in all_models.items():\\n    ...     print(f\"{model_id}: {model_card.family}\")\\n    gpt-4: GPT\\n    llama-3-8b: Llama',\n",
       "       'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.utils': {'path': 'server/utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/utils.py',\n",
       "   'package': 'neurosurfer.server',\n",
       "   'classes': [{'name': 'ApplicationPaths',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'rag_storage_path',\n",
       "       'docstring': None,\n",
       "       'args': ['user_id', 'thread_id']},\n",
       "      {'name': 'user_storage_path', 'docstring': None, 'args': ['user_id']},\n",
       "      {'name': 'thread_storage_path',\n",
       "       'docstring': None,\n",
       "       'args': ['user_id', 'thread_id']},\n",
       "      {'name': 'thread_files_storage_path',\n",
       "       'docstring': None,\n",
       "       'args': ['user_id', 'thread_id']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.runtime': {'path': 'server/runtime.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/runtime.py',\n",
       "   'package': 'neurosurfer.server',\n",
       "   'classes': [{'name': 'RequestContext',\n",
       "     'docstring': 'Container for request-specific runtime context.\\n\\nHolds metadata and control signals for a single request/operation,\\nenabling cancellation and context passing throughout the request lifecycle.\\n\\nAttributes:\\n    op_id (str): Unique operation identifier (e.g., \"op_a1b2c3d4e5f6\")\\n    stop_event (threading.Event): Event for signaling operation cancellation\\n    headers (dict): Request headers or additional metadata\\n\\nExample:\\n    >>> ctx = RequestContext(\\n    ...     op_id=\"op_123\",\\n    ...     stop_event=threading.Event(),\\n    ...     headers={\"user-agent\": \"client/1.0\"}\\n    ... )\\n    >>> \\n    >>> # Check if operation should stop\\n    >>> if ctx.stop_event.is_set():\\n    ...     return \"Operation cancelled\"',\n",
       "     'methods': []},\n",
       "    {'name': 'OperationManager',\n",
       "     'docstring': 'Manages operation lifecycle and cancellation signals.\\n\\nThis class provides centralized management of active operations,\\nallowing creation, cancellation, and cleanup of request contexts.\\nThread-safe for concurrent access.\\n\\nAttributes:\\n    _ops (Dict[str, threading.Event]): Mapping of operation IDs to stop events\\n\\nExample:\\n    >>> manager = OperationManager()\\n    >>> \\n    >>> # Create new operation\\n    >>> ctx = manager.create()\\n    >>> \\n    >>> # Stop from another thread\\n    >>> manager.stop(ctx.op_id)\\n    >>> \\n    >>> # Clean up\\n    >>> manager.done(ctx.op_id)',\n",
       "     'methods': [{'name': 'create',\n",
       "       'docstring': \"Create a new operation context with unique ID.\\n\\nGenerates a unique operation ID, creates a stop event, and\\nregisters the operation for lifecycle management.\\n\\nReturns:\\n    RequestContext: New request context with unique op_id and stop_event\\n\\nExample:\\n    >>> ctx = op_manager.create()\\n    >>> print(ctx.op_id)\\n    'op_a1b2c3d4e5f6'\\n    >>> ctx.stop_event.is_set()\\n    False\",\n",
       "       'args': ['self']},\n",
       "      {'name': 'stop',\n",
       "       'docstring': 'Signal an operation to stop.\\n\\nSets the stop event for the specified operation, allowing\\nthe operation to gracefully terminate.\\n\\nArgs:\\n    op_id (str): Operation ID to stop\\n\\nReturns:\\n    bool: True if operation was found and stopped, False if not found\\n\\nExample:\\n    >>> ctx = op_manager.create()\\n    >>> # ... operation running ...\\n    >>> success = op_manager.stop(ctx.op_id)\\n    >>> print(success)\\n    True\\n    >>> ctx.stop_event.is_set()\\n    True',\n",
       "       'args': ['self', 'op_id']},\n",
       "      {'name': 'done',\n",
       "       'docstring': \"Clean up a completed operation.\\n\\nRemoves the operation from the registry after completion.\\nSafe to call even if operation doesn't exist.\\n\\nArgs:\\n    op_id (str): Operation ID to clean up\\n\\nExample:\\n    >>> ctx = op_manager.create()\\n    >>> # ... operation completes ...\\n    >>> op_manager.done(ctx.op_id)\",\n",
       "       'args': ['self', 'op_id']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.config': {'path': 'server/config.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/config.py',\n",
       "   'package': 'neurosurfer.server',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.app': {'path': 'server/app.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/app.py',\n",
       "   'package': 'neurosurfer.server',\n",
       "   'classes': [{'name': 'NeurosurferApp',\n",
       "     'docstring': 'A comprehensive FastAPI-based application builder for AI-powered chat and API services.\\n\\nNeurosurferApp provides a simplicity-first approach to building AI applications with:\\n- Single chat handler registration via decorator or method\\n- Custom endpoint creation with one-line decorators\\n- Built-in authentication (API key or JWT-based)\\n- Database integration\\n- CORS support for cross-origin requests\\n- Automatic API documentation (Swagger/OpenAPI)\\n- Model registry for managing AI models\\n- Lifecycle management with startup/shutdown hooks\\n\\nThe app supports both simple API key authentication and full user authentication\\nwith JWT tokens. It includes built-in routes for health checks, model listings,\\nchat completions, and user management.\\n\\nKey Features:\\n- FastAPI-based with automatic OpenAPI documentation\\n- SQLAlchemy ORM for database operations\\n- Session middleware for CSRF protection\\n- Model registry for AI model management\\n- Flexible authentication (API key or JWT)\\n- CORS support for web applications\\n- Startup and shutdown lifecycle hooks\\n- Built-in routes for common operations\\n\\nExample:\\n    ```python\\n    from neurosurfer.server.app import NeurosurferApp\\n\\n    app = NeurosurferApp(api_key=\"your-api-key\")\\n\\n    @app.chat()\\n    def handle_chat(request, context):\\n        # Handle chat request and return response\\n        return {\"response\": \"Hello!\"}\\n\\n    if __name__ == \"__main__\":\\n        app.run()\\n    ```',\n",
       "     'methods': [{'name': 'register_model',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'model', 'provider', 'family', 'description']},\n",
       "      {'name': 'get_model', 'docstring': None, 'args': ['self', 'model_name']},\n",
       "      {'name': 'apply_rag',\n",
       "       'docstring': None,\n",
       "       'args': ['self',\n",
       "        'user_id',\n",
       "        'thread_id',\n",
       "        'message_id',\n",
       "        'user_query',\n",
       "        'has_files_message']},\n",
       "      {'name': 'require_api_key',\n",
       "       'docstring': 'Validate API key authentication for incoming requests.\\n\\nThis method checks if the request contains a valid API key in the Authorization header.\\nIf no API key is configured for the app, authentication is skipped.\\n\\nArgs:\\n    req (Request): The incoming FastAPI request object containing headers.\\n\\nRaises:\\n    HTTPException: If the API key is configured but missing or invalid, raises a 401\\n        Unauthorized exception with details about the authentication failure.\\n\\nNote:\\n    The expected format is: \"Bearer <api_key>\" in the Authorization header.\\n    If the API key is configured but the request doesn\\'t have a valid key,\\n    the request is rejected with a 401 status code.',\n",
       "       'args': ['self', 'req']},\n",
       "      {'name': 'require_auth',\n",
       "       'docstring': 'Authenticate requests using either API key or JWT-based user authentication.\\n\\nThis method implements a flexible authentication strategy that supports both:\\n1. API key authentication (for service-to-service communication)\\n2. JWT-based user authentication (for user sessions)\\n\\nThe method first checks for a valid API key if one is configured. If an API key\\nis provided and valid, the request is authenticated as a service/system caller.\\nOtherwise, it falls back to JWT-based user authentication.\\n\\nArgs:\\n    req (Request): The incoming FastAPI request object containing headers and state.\\n    response (Response): The FastAPI response object for potential cookie refresh.\\n    db (Session): SQLAlchemy database session for user authentication queries.\\n\\nRaises:\\n    HTTPException: If neither API key nor valid user authentication is provided,\\n        raises a 401 Unauthorized exception.\\n\\nNote:\\n    - API key format: \"Bearer <api_key>\" in Authorization header\\n    - For API key auth: sets req.state.user = None (indicates service caller)\\n    - For JWT auth: sets req.state.user to the authenticated user object\\n    - If API key is configured, it takes precedence over user authentication',\n",
       "       'args': ['self', 'req', 'response', 'db']},\n",
       "      {'name': 'on_startup',\n",
       "       'docstring': 'Register a function to be executed during application startup.\\n\\nThis decorator allows you to register callback functions that will be executed\\nwhen the FastAPI application starts up. The functions can be either synchronous\\nor asynchronous (coroutines).\\n\\nArgs:\\n    fn (Callable[[], Any]): A callable that takes no arguments and returns Any.\\n        Can be either a regular function or an async function (coroutine).\\n\\nReturns:\\n    Callable: The original function, allowing this to be used as a decorator.\\n\\nExample:\\n    ```python\\n    app = NeurosurferApp()\\n\\n    @app.on_startup\\n    def initialize_database():\\n        print(\"Setting up database connections...\")\\n\\n    @app.on_startup\\n    async def load_models():\\n        print(\"Loading AI models...\")\\n        await some_async_operation()\\n    ```\\n\\nNote:\\n    - Startup functions are executed in the order they are registered\\n    - Async functions are properly awaited during the startup process\\n    - These functions run before the server starts accepting requests',\n",
       "       'args': ['self', 'fn']},\n",
       "      {'name': 'on_shutdown',\n",
       "       'docstring': 'Register a function to be executed during application shutdown.\\n\\nThis decorator allows you to register callback functions that will be executed\\nwhen the FastAPI application shuts down gracefully. The functions can be either\\nsynchronous or asynchronous (coroutines).\\n\\nArgs:\\n    fn (Callable[[], Any]): A callable that takes no arguments and returns Any.\\n        Can be either a regular function or an async function (coroutine).\\n\\nReturns:\\n    Callable: The original function, allowing this to be used as a decorator.\\n\\nExample:\\n    ```python\\n    app = NeurosurferApp()\\n\\n    @app.on_shutdown\\n    def cleanup_resources():\\n        print(\"Cleaning up resources...\")\\n\\n    @app.on_shutdown\\n    async def save_state():\\n        print(\"Saving application state...\")\\n        await some_async_cleanup()\\n    ```\\n\\nNote:\\n    - Shutdown functions are executed in the order they are registered\\n    - Async functions are properly awaited during the shutdown process\\n    - These functions run after the server stops accepting requests\\n    - Useful for cleanup operations, saving state, or closing connections',\n",
       "       'args': ['self', 'fn']},\n",
       "      {'name': 'chat',\n",
       "       'docstring': 'Decorator to register the primary chat handler for the application.\\n\\nThis method serves as a decorator factory that allows you to register a single\\nchat handler function that will process all chat requests. The decorated function\\nwill be automatically integrated with the FastAPI routing system and will be\\naccessible via the chat completions API endpoints.\\n\\nThe decorator triggers the mounting of built-in routes including health checks,\\nmodel listings, and chat completion endpoints.\\n\\nReturns:\\n    Callable: A decorator function that registers the chat handler.\\n\\nExample:\\n    ```python\\n    app = NeurosurferApp()\\n    @app.chat()\\n    def handle_chat(request_data):\\n        # Process the chat request\\n        user_message = request_data.get(\"messages\", [{}])[-1].get(\"content\", \"\")\\n        # Generate response\\n        return {\\n            \"choices\": [{\\n                \"message\": {\\n                    \"role\": \"assistant\",\\n                    \"content\": f\"You said: {user_message}\"\\n                }\\n            }]\\n        }\\n\\n    # Alternative: async handler\\n    @app.chat()\\n    async def handle_chat_async(request_data):\\n        # Async processing\\n        response = await some_async_ai_call(request_data)\\n        return response\\n    ```\\n\\nNote:\\n    - Only one chat handler can be registered per application instance\\n    - The handler function should accept request data and return response data\\n    - Async handlers are supported and properly awaited\\n    - The decorator automatically mounts all necessary routes',\n",
       "       'args': ['self']},\n",
       "      {'name': 'endpoint',\n",
       "       'docstring': 'Create a decorator for registering custom API endpoints.\\n\\nThis method serves as a decorator factory that allows you to easily create custom\\nAPI endpoints with automatic request/response model validation, dependency injection,\\nand integration with FastAPI\\'s routing system.\\n\\nArgs:\\n    path (str): The URL path for the endpoint (e.g., \"/api/summarize\").\\n    method (str, optional): HTTP method for the endpoint. Must be one of:\\n        \"get\", \"post\", \"put\", \"patch\", \"delete\". Defaults to \"post\".\\n    request (Type[BaseModel], optional): Pydantic model class for request body validation.\\n        If provided, FastAPI will automatically parse and validate the request body.\\n        Defaults to None.\\n    response (Type[BaseModel], optional): Pydantic model class for response serialization.\\n        If provided, FastAPI will automatically serialize the response to match this model.\\n        Defaults to None.\\n    dependencies (list[Callable[..., Any]], optional): List of FastAPI dependency functions\\n        that will be executed before the endpoint handler. Useful for authentication,\\n        database sessions, etc. Defaults to None.\\n\\nReturns:\\n    Callable: A decorator function that registers the endpoint.\\n\\nExample:\\n    ```python\\n    from pydantic import BaseModel\\n\\n    class SummarizeRequest(BaseModel):\\n        text: str\\n        max_length: int = 100\\n\\n    class SummarizeResponse(BaseModel):\\n        summary: str\\n        original_length: int\\n        summary_length: int\\n\\n    app = NeurosurferApp()\\n\\n    @app.endpoint(\\n        \"/summarize\",\\n        method=\"post\",\\n        request=SummarizeRequest,\\n        response=SummarizeResponse\\n    )\\n    def summarize_text(data: SummarizeRequest):\\n        # data is automatically parsed and validated\\n        summary = summarize_function(data.text, data.max_length)\\n        return SummarizeResponse(\\n            summary=summary,\\n            original_length=len(data.text),\\n            summary_length=len(summary)\\n        )\\n\\n    # GET endpoint without request/response models\\n    @app.endpoint(\"/health\", method=\"get\")\\n    def custom_health():\\n        return {\"status\": \"custom health check\"}\\n    ```\\n\\nRaises:\\n    ValueError: If the provided HTTP method is not supported.\\n\\nNote:\\n    - The decorated function can access the request body via its first parameter\\n    - Request and response models provide automatic validation and serialization\\n    - Dependencies are executed before the endpoint handler runs\\n    - The endpoint is automatically registered with the main router',\n",
       "       'args': ['self',\n",
       "        'path',\n",
       "        'method',\n",
       "        'request',\n",
       "        'response',\n",
       "        'dependencies']},\n",
       "      {'name': 'run',\n",
       "       'docstring': 'Start the FastAPI application server using Uvicorn.\\n\\nThis method launches the configured FastAPI application using the Uvicorn ASGI server.\\nThe server will start with the configuration specified during initialization including\\nhost, port, reload settings, logging level, and worker processes.\\n\\nThe method handles server startup and graceful error handling. If the server fails\\nto start, it logs an error message with details about the failure.\\n\\nConfiguration used:\\n- host: Server bind address (from __init__)\\n- port: Server port number (from __init__)\\n- reload: Auto-reload during development (from __init__)\\n- log_level: Logging verbosity (from __init__)\\n- workers: Number of worker processes (from __init__)\\n\\nExample:\\n    ```python\\n    app = NeurosurferApp(port=9000, reload=True)\\n\\n    # Start the server\\n    app.run()\\n    ```\\n\\nNote:\\n    - This method blocks the current thread while the server is running\\n    - Use Ctrl+C or send SIGTERM to stop the server gracefully\\n    - In production, consider using a process manager like gunicorn\\n    - The method will not return until the server is stopped',\n",
       "       'args': ['self', 'host', 'port', 'reload', 'log_level', 'workers']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.security': {'path': 'server/security.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/security.py',\n",
       "   'package': 'neurosurfer.server',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'get_db',\n",
       "     'docstring': 'Database session dependency for FastAPI.\\n\\nCreates a new database session for each request and ensures\\nit\\'s properly closed after the request completes.\\n\\nYields:\\n    Session: SQLAlchemy database session\\n\\nExample:\\n    >>> from fastapi import Depends\\n    >>> from neurosurfer.server.security import get_db\\n    >>> \\n    >>> @app.get(\"/users\")\\n    >>> def list_users(db: Session = Depends(get_db)):\\n    ...     return db.query(User).all()',\n",
       "     'args': []},\n",
       "    {'name': 'hash_password',\n",
       "     'docstring': 'Hash a password using bcrypt.\\n\\nArgs:\\n    password (str): Plain text password to hash\\n\\nReturns:\\n    str: Bcrypt hashed password\\n\\nExample:\\n    >>> hashed = hash_password(\"my-secure-password\")\\n    >>> print(hashed[:7])\\n    \\'$2b$12$\\'',\n",
       "     'args': ['password']},\n",
       "    {'name': 'verify_password',\n",
       "     'docstring': 'Verify a password against its hash.\\n\\nArgs:\\n    plain (str): Plain text password to verify\\n    hashed (str): Bcrypt hashed password to compare against\\n\\nReturns:\\n    bool: True if password matches, False otherwise\\n\\nExample:\\n    >>> hashed = hash_password(\"password123\")\\n    >>> verify_password(\"password123\", hashed)\\n    True\\n    >>> verify_password(\"wrong-password\", hashed)\\n    False',\n",
       "     'args': ['plain', 'hashed']},\n",
       "    {'name': 'create_access_token',\n",
       "     'docstring': 'Create a JWT access token.\\n\\nGenerates a signed JWT token with expiration time. The \\'sub\\' (subject)\\nfield is automatically converted to string if present.\\n\\nArgs:\\n    data (dict): Token payload data (typically {\"sub\": user_id})\\n    expires_minutes (Optional[int]): Token expiration in minutes.\\n        Default: ACCESS_TOKEN_EXPIRE_MINUTES from config\\n\\nReturns:\\n    str: Encoded JWT token\\n\\nExample:\\n    >>> token = create_access_token({\"sub\": \"user123\"})\\n    >>> print(token[:20])\\n    \\'eyJhbGciOiJIUzI1NiIs\\'\\n    >>> \\n    >>> # Custom expiration\\n    >>> short_token = create_access_token({\"sub\": \"user123\"}, expires_minutes=15)',\n",
       "     'args': ['data', 'expires_minutes']},\n",
       "    {'name': 'decode_token',\n",
       "     'docstring': 'Decode and validate a JWT token.\\n\\nArgs:\\n    token (str): JWT token to decode\\n\\nReturns:\\n    dict: Decoded token payload\\n\\nRaises:\\n    JWTError: If token is invalid or expired\\n\\nExample:\\n    >>> token = create_access_token({\"sub\": \"user123\"})\\n    >>> payload = decode_token(token)\\n    >>> print(payload[\"sub\"])\\n    \\'user123\\'',\n",
       "     'args': ['token']},\n",
       "    {'name': 'set_login_cookie',\n",
       "     'docstring': 'Set authentication cookie in response.\\n\\nConfigures a secure HttpOnly cookie with the JWT token,\\nusing settings from config (secure, samesite, max_age).\\n\\nArgs:\\n    response (Response): FastAPI response object\\n    token (str): JWT token to store in cookie\\n\\nExample:\\n    >>> from fastapi import Response\\n    >>> response = Response()\\n    >>> token = create_access_token({\"sub\": \"user123\"})\\n    >>> set_login_cookie(response, token)',\n",
       "     'args': ['response', 'token']},\n",
       "    {'name': 'clear_login_cookie',\n",
       "     'docstring': 'Clear authentication cookie from response.\\n\\nRemoves the authentication cookie, effectively logging out the user.\\n\\nArgs:\\n    response (Response): FastAPI response object\\n\\nExample:\\n    >>> from fastapi import Response\\n    >>> response = Response()\\n    >>> clear_login_cookie(response)',\n",
       "     'args': ['response']},\n",
       "    {'name': 'get_current_user',\n",
       "     'docstring': 'FastAPI dependency for getting the current authenticated user.\\n\\nExtracts and validates the JWT token from request headers or cookies,\\nretrieves the user from database, and implements sliding session refresh\\n(automatically extends token if expiring soon).\\n\\nArgs:\\n    request (Request): FastAPI request object\\n    response (Response): FastAPI response object (for token refresh)\\n    db (Session): Database session (injected via Depends)\\n\\nReturns:\\n    User: Authenticated user object from database\\n\\nRaises:\\n    HTTPException: 401 if not authenticated or token invalid\\n    HTTPException: 404 if user not found in database\\n\\nExample:\\n    >>> from fastapi import Depends\\n    >>> from neurosurfer.server.security import get_current_user\\n    >>> \\n    >>> @app.get(\"/me\")\\n    >>> def get_profile(user = Depends(get_current_user)):\\n    ...     return {\"username\": user.username, \"email\": user.email}',\n",
       "     'args': ['request', 'response', 'db']},\n",
       "    {'name': 'resolve_actor_id', 'docstring': None, 'args': ['req', 'user']}]},\n",
       "  'neurosurfer.server.gunicorn.conf': {'path': 'server/gunicorn.conf.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/gunicorn.conf.py',\n",
       "   'package': 'neurosurfer.server',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.__init__': {'path': 'server/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/__init__.py',\n",
       "   'package': 'neurosurfer.server',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.db.models': {'path': 'server/db/models.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/db/models.py',\n",
       "   'package': 'neurosurfer.server.db',\n",
       "   'classes': [{'name': 'User',\n",
       "     'docstring': 'User account model.\\n\\nRepresents a registered user with authentication credentials.\\nUsers can have multiple chat threads and files.\\n\\nAttributes:\\n    id (int): Primary key\\n    email (str): Unique email address (indexed)\\n    full_name (str | None): Optional full name\\n    hashed_password (str): Bcrypt hashed password\\n    created_at (datetime): Account creation timestamp\\n    threads (list[ChatThread]): User\\'s chat threads (cascade delete)\\n\\nIndexes:\\n    - email (unique)\\n    - id (primary key)\\n\\nExample:\\n    >>> user = User(\\n    ...     email=\"user@example.com\",\\n    ...     full_name=\"John Doe\",\\n    ...     hashed_password=hash_password(\"password\")\\n    ... )',\n",
       "     'methods': []},\n",
       "    {'name': 'ChatThread',\n",
       "     'docstring': 'Chat thread (conversation) model.\\n\\nRepresents a conversation thread containing multiple messages.\\nEach thread belongs to a user and can have associated files.\\n\\nAttributes:\\n    id (int): Primary key\\n    user_id (int): Foreign key to User (indexed)\\n    title (str | None): Thread title (auto-generated from first message)\\n    created_at (datetime): Thread creation timestamp\\n    updated_at (datetime): Last update timestamp (auto-updated)\\n    user (User): Thread owner\\n    messages (list[Message]): Messages in this thread (cascade delete, ordered by created_at)\\n    files (list[NSFile]): Files attached to this thread (cascade delete)\\n\\nIndexes:\\n    - id (primary key)\\n    - user_id\\n    - (user_id, created_at) composite index\\n\\nExample:\\n    >>> thread = ChatThread(\\n    ...     user_id=1,\\n    ...     title=\"Discussion about AI\"\\n    ... )',\n",
       "     'methods': []},\n",
       "    {'name': 'Message',\n",
       "     'docstring': 'Chat message model.\\n\\nRepresents a single message within a chat thread.\\nMessages have a role (user, assistant, system, tool) and content.\\n\\nAttributes:\\n    id (int): Primary key\\n    thread_id (int): Foreign key to ChatThread (indexed)\\n    role (str): Message role (user | assistant | system | tool)\\n    content (str): Message text content\\n    model_name (str | None): Model used to generate this message (if assistant)\\n    created_at (datetime): Message creation timestamp\\n    thread (ChatThread): Parent thread\\n\\nIndexes:\\n    - id (primary key)\\n    - thread_id\\n    - (thread_id, created_at) composite index\\n\\nExample:\\n    >>> message = Message(\\n    ...     thread_id=1,\\n    ...     role=\"user\",\\n    ...     content=\"Hello, how are you?\"\\n    ... )',\n",
       "     'methods': []},\n",
       "    {'name': 'NSFile',\n",
       "     'docstring': 'File attachment model.\\n\\nRepresents a file uploaded by a user and associated with a chat thread.\\nFiles are ingested into vector stores for RAG functionality.\\n\\nAttributes:\\n    id (str): Primary key (file ID)\\n    user_id (int): Foreign key to User (indexed)\\n    thread_id (int): Foreign key to ChatThread (indexed)\\n    filename (str): Original filename\\n    stored_path (str): Path where file is stored (may be removed after ingest)\\n    mime (str | None): MIME type (e.g., \"application/pdf\")\\n    size (int | None): File size in bytes\\n    collection (str): Vector store collection name for this file\\n    created_at (datetime): Upload timestamp\\n    thread (ChatThread): Parent thread\\n\\nIndexes:\\n    - id (primary key)\\n    - user_id\\n    - thread_id\\n\\nExample:\\n    >>> file = NSFile(\\n    ...     id=\"file_abc123\",\\n    ...     user_id=1,\\n    ...     thread_id=1,\\n    ...     filename=\"document.pdf\",\\n    ...     stored_path=\"/uploads/document.pdf\",\\n    ...     mime=\"application/pdf\",\\n    ...     size=1024000,\\n    ...     collection=\"nm_u1_t1\"\\n    ... )',\n",
       "     'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.db.db': {'path': 'server/db/db.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/db/db.py',\n",
       "   'package': 'neurosurfer.server.db',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'init_db',\n",
       "     'docstring': \"Initialize database tables.\\n\\nCreates all tables defined in models if they don't exist.\\nThis function should be called once during application startup.\\n\\nExample:\\n    >>> from neurosurfer.server.db.db import init_db\\n    >>> init_db()\\n    Database initialized successfully...\",\n",
       "     'args': []}]},\n",
       "  'neurosurfer.server.db.__init__': {'path': 'server/db/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/db/__init__.py',\n",
       "   'package': 'neurosurfer.server.db',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.services.follow_up_questions': {'path': 'server/services/follow_up_questions.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/follow_up_questions.py',\n",
       "   'package': 'neurosurfer.server.services',\n",
       "   'classes': [{'name': 'FollowUpQuestions',\n",
       "     'docstring': \"Service to generate and parse follow-up questions from an existing LLM.\\nThe LLM must support a 'ask(user_prompt, system_prompt, chat_history, stream=False, **kwargs)' call.\",\n",
       "     'methods': [{'name': 'set_llm',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'llm']},\n",
       "      {'name': 'generate',\n",
       "       'docstring': \"- messages: list of {'role': 'system'|'user'|'assistant', 'content': str}\\nReturns a list of up to 3 questions.\",\n",
       "       'args': ['self', 'messages']}]}],\n",
       "   'functions': [{'name': 'robust_parse_followups',\n",
       "     'docstring': None,\n",
       "     'args': ['raw']}]},\n",
       "  'neurosurfer.server.services.rag.metadata_filter': {'path': 'server/services/rag/metadata_filter.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/metadata_filter.py',\n",
       "   'package': 'neurosurfer.server.services.rag',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'build_metadata_filter_from_related_files',\n",
       "     'docstring': \"Map gate LLM's related_files (filenames) to NSFile IDs,\\nthen build a metadata_filter suitable for the vectorstore.\",\n",
       "     'args': ['db', 'user_id', 'thread_id', 'collection', 'related_files']}]},\n",
       "  'neurosurfer.server.services.rag.models': {'path': 'server/services/rag/models.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/models.py',\n",
       "   'package': 'neurosurfer.server.services.rag',\n",
       "   'classes': [{'name': 'RAGResult',\n",
       "     'docstring': 'Outcome of a RAG orchestration step.',\n",
       "     'methods': []},\n",
       "    {'name': 'GateDecision',\n",
       "     'docstring': 'Result from the RAG routing (gate) LLM.',\n",
       "     'methods': [{'name': 'from_dict',\n",
       "       'docstring': None,\n",
       "       'args': ['cls', 'data']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.services.rag.ingestor': {'path': 'server/services/rag/ingestor.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/ingestor.py',\n",
       "   'package': 'neurosurfer.server.services.rag',\n",
       "   'classes': [{'name': 'FileIngestor',\n",
       "     'docstring': 'RAG File Ingestor\\n\\nTakes already-persisted NSFile records (one per physical file on disk)\\nand ingests them into the vectorstore.\\n\\nIt assumes:\\n  - Files are already stored on disk at nsfile.stored_path\\n  - Zips have already been expanded at the API layer (one NSFile per\\n    extracted inner file)',\n",
       "     'methods': [{'name': 'ingest_files',\n",
       "       'docstring': 'Ingest existing NSFile rows into the vectorstore.\\n\\nArgs:\\n    db: SQLAlchemy session\\n    user_id, thread_id: scope sanity-checks\\n    collection_name: vectorstore collection name\\n    files: NSFile records to ingest (e.g. new files for the last message)\\n    reset_state: passed only to the first ingest call\\nReturns:\\n    List of ingestion summaries.',\n",
       "       'args': ['self',\n",
       "        'db',\n",
       "        'user_id',\n",
       "        'thread_id',\n",
       "        'collection_name',\n",
       "        'files',\n",
       "        'reset_state']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.services.rag.summarizer': {'path': 'server/services/rag/summarizer.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/summarizer.py',\n",
       "   'package': 'neurosurfer.server.services.rag',\n",
       "   'classes': [{'name': 'FileSummarizer',\n",
       "     'docstring': 'Responsible for turning file content into a short textual summary,\\nused by the routing LLM (gate) and stored in NSFile.summary.',\n",
       "     'methods': [{'name': 'summarize_path',\n",
       "       'docstring': \"Build a summary for the file at `path` using actual content if possible.\\n\\n- Reads via RAGAgent's FileReader.\\n- Samples text (start/middle/end) with a budget.\\n- Uses LLM if available:\\n    - zip member -> ~2 sentences\\n    - single file -> ~1 short paragraph\",\n",
       "       'args': ['self', 'path', 'is_zip_member']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.services.rag.__init__': {'path': 'server/services/rag/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/__init__.py',\n",
       "   'package': 'neurosurfer.server.services.rag',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.services.rag.orchestrator': {'path': 'server/services/rag/orchestrator.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/orchestrator.py',\n",
       "   'package': 'neurosurfer.server.services.rag',\n",
       "   'classes': [{'name': 'RAGOrchestrator',\n",
       "     'docstring': 'Thin coordinator that wires:\\n  - File ingestion (SQL + vectorstore)\\n  - File summaries\\n  - RAG routing (gate LLM)\\n  - Retrieval via RAGAgent',\n",
       "     'methods': [{'name': 'apply',\n",
       "       'docstring': 'Main entry point.\\n- user_id, thread_id: scope for both SQL and vectorstore collection\\n- user_query: latest user message text\\n- files: base64-encoded upload metadata from request (if any)',\n",
       "       'args': ['self',\n",
       "        'user_id',\n",
       "        'thread_id',\n",
       "        'user_query',\n",
       "        'message_id',\n",
       "        'has_files_message']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.services.rag.gate': {'path': 'server/services/rag/gate.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/services/rag/gate.py',\n",
       "   'package': 'neurosurfer.server.services.rag',\n",
       "   'classes': [{'name': 'RAGGate',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'decide',\n",
       "       'docstring': 'Decide whether to use RAG, which files are relevant, and the retrieval scope.\\n\\nBehavior:\\n- If there are files attached to the current message:\\n    - They are ALWAYS included in related_files (pinned).\\n    - If there are other files in the thread and a gate LLM is configured,\\n      we ask the LLM to select any additional relevant files from the rest.\\n- If there are no message_files:\\n    - We fall back to normal gate behavior over all thread files.',\n",
       "       'args': ['self',\n",
       "        'db',\n",
       "        'user_id',\n",
       "        'thread_id',\n",
       "        'collection',\n",
       "        'user_query',\n",
       "        'message_files']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.schemas.model_response': {'path': 'server/schemas/model_response.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/schemas/model_response.py',\n",
       "   'package': 'neurosurfer.server.schemas',\n",
       "   'classes': [{'name': 'ChoiceMessage',\n",
       "     'docstring': 'Message in a non-streaming completion response',\n",
       "     'methods': []},\n",
       "    {'name': 'Choice',\n",
       "     'docstring': 'Choice in a non-streaming completion response',\n",
       "     'methods': []},\n",
       "    {'name': 'Usage', 'docstring': 'Token usage information', 'methods': []},\n",
       "    {'name': 'ChatCompletionResponse',\n",
       "     'docstring': 'Complete non-streaming chat completion response',\n",
       "     'methods': []},\n",
       "    {'name': 'DeltaContent',\n",
       "     'docstring': 'Delta content for streaming chunks',\n",
       "     'methods': []},\n",
       "    {'name': 'StreamChoice',\n",
       "     'docstring': 'Choice in a streaming chunk',\n",
       "     'methods': []},\n",
       "    {'name': 'ChatCompletionChunk',\n",
       "     'docstring': 'Streaming chunk response',\n",
       "     'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.schemas.chats': {'path': 'server/schemas/chats.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/schemas/chats.py',\n",
       "   'package': 'neurosurfer.server.schemas',\n",
       "   'classes': [{'name': 'Chat', 'docstring': None, 'methods': []},\n",
       "    {'name': 'UploadedFileIn', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ChatMessageIn', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ChatFileOut', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ChatMessageOut', 'docstring': None, 'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.schemas.completions': {'path': 'server/schemas/completions.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/schemas/completions.py',\n",
       "   'package': 'neurosurfer.server.schemas',\n",
       "   'classes': [{'name': 'ToolDefFunction', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ToolDef', 'docstring': None, 'methods': []},\n",
       "    {'name': 'FileContent', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ChatCompletionRequest', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ChatHandlerMessages', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ChatHandlerModel', 'docstring': None, 'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.schemas.model_registry': {'path': 'server/schemas/model_registry.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/schemas/model_registry.py',\n",
       "   'package': 'neurosurfer.server.schemas',\n",
       "   'classes': [{'name': 'ModelCard',\n",
       "     'docstring': 'Represents one registered model.',\n",
       "     'methods': []},\n",
       "    {'name': 'ModelList', 'docstring': None, 'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.schemas.auth': {'path': 'server/schemas/auth.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/schemas/auth.py',\n",
       "   'package': 'neurosurfer.server.schemas',\n",
       "   'classes': [{'name': 'User', 'docstring': None, 'methods': []},\n",
       "    {'name': 'LoginRequest', 'docstring': None, 'methods': []},\n",
       "    {'name': 'LoginResponse', 'docstring': None, 'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.schemas.__init__': {'path': 'server/schemas/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/schemas/__init__.py',\n",
       "   'package': 'neurosurfer.server.schemas',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.api.api_chat_completions': {'path': 'server/api/api_chat_completions.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/api/api_chat_completions.py',\n",
       "   'package': 'neurosurfer.server.api',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'make_chat_response',\n",
       "     'docstring': 'Create OpenAI-compatible chat completion response from plain text.\\n\\nHelper function to wrap simple text responses in the proper Pydantic model format.\\n\\nArgs:\\n    op_id (str): Operation/request ID\\n    model (str): Model identifier\\n    text (str): Response text content\\n\\nReturns:\\n    ChatCompletionResponse: Formatted response with usage stats\\n\\nExample:\\n    >>> response = make_chat_response(\"op_123\", \"gpt-4\", \"Hello!\")\\n    >>> print(response.choices[0].message.content)\\n    \\'Hello!\\'',\n",
       "     'args': ['op_id', 'model', 'text']},\n",
       "    {'name': 'chat_completion_router',\n",
       "     'docstring': 'Create FastAPI router for chat completion endpoints.\\n\\nFactory function that creates a router with the /chat/completions endpoint,\\nconfigured with the provided chat handler and model registry.\\n\\nArgs:\\n    _chat_handler: Custom chat handler function (sync or async)\\n    model_registry (ModelRegistry): Registry of available models\\n\\nReturns:\\n    APIRouter: Configured FastAPI router\\n\\nExample:\\n    >>> def my_handler(request, context):\\n    ...     return \"Response\"\\n    >>> router = chat_completion_router(my_handler, registry)\\n    >>> app.include_router(router)',\n",
       "     'args': ['_chat_handler', 'model_registry']}]},\n",
       "  'neurosurfer.server.api.api_files': {'path': 'server/api/api_files.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/api/api_files.py',\n",
       "   'package': 'neurosurfer.server.api',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'download_file',\n",
       "     'docstring': 'Download a file that belongs to the current user.\\n\\n- Ensures the file exists\\n- Ensures it belongs to this user\\n- Streams the file with correct filename + mime type',\n",
       "     'args': ['file_id', 'db', 'user']}]},\n",
       "  'neurosurfer.server.api.api_chats': {'path': 'server/api/api_chats.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/api/api_chats.py',\n",
       "   'package': 'neurosurfer.server.api',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'thread_to_chat',\n",
       "     'docstring': 'Convert ChatThread model to Chat schema.\\n\\nArgs:\\n    th (ChatThread): Database chat thread model\\n\\nReturns:\\n    Chat: API chat schema',\n",
       "     'args': ['th']},\n",
       "    {'name': 'list_threads',\n",
       "     'docstring': 'List all chat threads for the current user.\\n\\nReturns threads ordered by most recent activity (last message timestamp).\\nIncludes message count and timestamps for each thread.\\n\\nArgs:\\n    db (Session): Database session\\n    user (User): Current authenticated user\\n\\nReturns:\\n    List[Chat]: List of chat threads with metadata\\n\\nExample:\\n    >>> GET /chats\\n    >>> # Returns: [{\"id\": \"1\", \"title\": \"Chat 1\", \"messagesCount\": 5, ...}]',\n",
       "     'args': ['db', 'user']},\n",
       "    {'name': 'create_thread',\n",
       "     'docstring': 'Create a new chat thread.\\n\\nArgs:\\n    data (dict): Thread data (optional title)\\n    db (Session): Database session\\n    user (User): Current authenticated user\\n\\nReturns:\\n    Chat: Created chat thread\\n\\nExample:\\n    >>> POST /chats\\n    >>> {\"title\": \"My New Chat\"}\\n    >>> # Returns: {\"id\": \"1\", \"title\": \"My New Chat\", ...}',\n",
       "     'args': ['data', 'db', 'user']},\n",
       "    {'name': 'get_thread',\n",
       "     'docstring': 'Get a specific chat thread.\\n\\nArgs:\\n    chat_id (int): Chat thread ID\\n    db (Session): Database session\\n    user (User): Current authenticated user\\n\\nReturns:\\n    Chat: Chat thread details\\n\\nRaises:\\n    HTTPException: 404 if chat not found or doesn\\'t belong to user\\n\\nExample:\\n    >>> GET /chats/1\\n    >>> # Returns: {\"id\": \"1\", \"title\": \"My Chat\", ...}',\n",
       "     'args': ['chat_id', 'db', 'user']},\n",
       "    {'name': 'list_messages',\n",
       "     'docstring': 'List all messages in a chat thread.\\n\\nReturns messages ordered chronologically (oldest first).\\n\\nArgs:\\n    chat_id (int): Chat thread ID\\n    db (Session): Database session\\n    user (User): Current authenticated user\\n\\nReturns:\\n    List[ChatMessageOut]: List of messages in the thread\\n\\nRaises:\\n    HTTPException: 404 if chat not found or doesn\\'t belong to user\\n\\nExample:\\n    >>> GET /chats/1/messages\\n    >>> # Returns: [{\"id\": 1, \"role\": \"user\", \"content\": \"Hi\", ...}]',\n",
       "     'args': ['chat_id', 'db', 'user']},\n",
       "    {'name': 'append_message',\n",
       "     'docstring': None,\n",
       "     'args': ['chat_id', 'body', 'db', 'user']},\n",
       "    {'name': 'delete_thread',\n",
       "     'docstring': None,\n",
       "     'args': ['chat_id', 'db', 'user']},\n",
       "    {'name': 'update_thread',\n",
       "     'docstring': None,\n",
       "     'args': ['chat_id', 'data', 'db', 'user']}]},\n",
       "  'neurosurfer.server.api.__init__': {'path': 'server/api/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/api/__init__.py',\n",
       "   'package': 'neurosurfer.server.api',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.server.api.api_auth': {'path': 'server/api/api_auth.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/server/api/api_auth.py',\n",
       "   'package': 'neurosurfer.server.api',\n",
       "   'classes': [{'name': 'RegisterBody',\n",
       "     'docstring': 'Request body for user registration.',\n",
       "     'methods': []},\n",
       "    {'name': 'LoginBody',\n",
       "     'docstring': 'Request body for user login.',\n",
       "     'methods': []},\n",
       "    {'name': 'DeleteBody',\n",
       "     'docstring': 'Request body for account deletion (requires password confirmation).',\n",
       "     'methods': []}],\n",
       "   'functions': [{'name': 'user_to_schema',\n",
       "     'docstring': 'Convert database User model to API schema.\\n\\nArgs:\\n    u (User): Database user model\\n\\nReturns:\\n    UserSchema: API user schema with id, name, and email',\n",
       "     'args': ['u']},\n",
       "    {'name': 'register',\n",
       "     'docstring': 'Register a new user account.\\n\\nCreates a new user with hashed password and generates an authentication token.\\nEmail must be unique.\\n\\nArgs:\\n    body (RegisterBody): Registration data (email, password, optional full_name)\\n    response (Response): FastAPI response object\\n    db (Session): Database session\\n\\nReturns:\\n    UserSchema: Created user information\\n\\nRaises:\\n    HTTPException: 400 if email already registered\\n\\nExample:\\n    >>> POST /auth/register\\n    >>> {\"email\": \"user@example.com\", \"password\": \"pass123\", \"full_name\": \"John\"}\\n    >>> # Returns: {\"id\": \"1\", \"name\": \"John\", \"email\": \"user@example.com\"}',\n",
       "     'args': ['body', 'response', 'db']},\n",
       "    {'name': 'login',\n",
       "     'docstring': 'Authenticate user and create session.\\n\\nValidates credentials and creates JWT token stored in secure cookie.\\n\\nArgs:\\n    body (LoginBody): Login credentials (email, password)\\n    response (Response): FastAPI response object (for setting cookie)\\n    db (Session): Database session\\n\\nReturns:\\n    LoginResponse: Authentication token and user information\\n\\nRaises:\\n    HTTPException: 401 if user not found or password invalid\\n\\nExample:\\n    >>> POST /auth/login\\n    >>> {\"email\": \"user@example.com\", \"password\": \"pass123\"}\\n    >>> # Returns: {\"token\": \"eyJ...\", \"user\": {...}}',\n",
       "     'args': ['body', 'response', 'db']},\n",
       "    {'name': 'logout',\n",
       "     'docstring': 'Logout user by clearing authentication cookie.\\n\\nArgs:\\n    response (Response): FastAPI response object\\n\\nReturns:\\n    dict: Success confirmation\\n\\nExample:\\n    >>> POST /auth/logout\\n    >>> # Returns: {\"ok\": true}',\n",
       "     'args': ['response']},\n",
       "    {'name': 'me',\n",
       "     'docstring': 'Get current authenticated user profile.\\n\\nArgs:\\n    user (User): Current user (injected via dependency)\\n\\nReturns:\\n    UserSchema: Current user information\\n\\nRaises:\\n    HTTPException: 401 if not authenticated\\n\\nExample:\\n    >>> GET /auth/me\\n    >>> # Headers: Authorization: Bearer <token>\\n    >>> # Returns: {\"id\": \"1\", \"name\": \"John\", \"email\": \"user@example.com\"}',\n",
       "     'args': ['user']},\n",
       "    {'name': 'delete_account',\n",
       "     'docstring': 'Delete user account and all associated data.\\n\\nRequires password confirmation. Deletes all chat threads, messages,\\nand the user account. This operation is irreversible.\\n\\nArgs:\\n    body (DeleteBody): Password confirmation\\n    user (User): Current user (injected via dependency)\\n    db (Session): Database session\\n\\nReturns:\\n    UserSchema: Deleted user information\\n\\nRaises:\\n    HTTPException: 401 if password is incorrect\\n\\nExample:\\n    >>> POST /auth/delete_account\\n    >>> {\"password\": \"pass123\"}\\n    >>> # Returns: {\"id\": \"1\", \"name\": \"John\", \"email\": \"user@example.com\"}',\n",
       "     'args': ['body', 'user', 'db']}]},\n",
       "  'neurosurfer.tools.toolkit': {'path': 'tools/toolkit.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/toolkit.py',\n",
       "   'package': 'neurosurfer.tools',\n",
       "   'classes': [{'name': 'Toolkit',\n",
       "     'docstring': 'Tool registry and manager for agents.\\n\\nThis class manages a collection of tools, providing registration,\\nvalidation, and description generation capabilities. Agents use\\nthe Toolkit to discover and invoke available tools.\\n\\nAttributes:\\n    logger (logging.Logger): Logger instance\\n    registry (Dict[str, BaseTool]): Mapping of tool names to tool instances\\n    specs (Dict[str, ToolSpec]): Mapping of tool names to tool specifications\\n\\nExample:\\n    >>> toolkit = Toolkit()\\n    >>> \\n    >>> # Register tools\\n    >>> toolkit.register_tool(MyTool())\\n    >>> toolkit.register_tool(AnotherTool())\\n    >>> \\n    >>> # Get formatted descriptions\\n    >>> desc = toolkit.get_tools_description()\\n    >>> \\n    >>> # Access tools\\n    >>> tool = toolkit.registry[\"my_tool\"]\\n    >>> response = tool(param=\"value\")',\n",
       "     'methods': [{'name': 'register_tool',\n",
       "       'docstring': 'Register a tool in the toolkit.\\n\\nValidates the tool type and ensures no duplicate registrations.\\nOnce registered, the tool becomes available to agents.\\n\\nArgs:\\n    tool (BaseTool): Tool instance to register\\n\\nRaises:\\n    TypeError: If tool is not a BaseTool subclass\\n    ValueError: If tool name is already registered\\n\\nExample:\\n    >>> from neurosurfer.tools.sql import SQLQueryTool\\n    >>> toolkit = Toolkit()\\n    >>> toolkit.register_tool(SQLQueryTool())\\n    Registered tool: sql_query',\n",
       "       'args': ['self', 'tool']},\n",
       "      {'name': 'get_tools_description',\n",
       "       'docstring': \"Generate formatted descriptions of all registered tools.\\n\\nCreates a markdown-formatted string describing each tool's:\\n- Name and description\\n- When to use it\\n- Input parameters (with types and requirements)\\n- Return type and description\\n\\nThis description is used by agents to understand available tools.\\nReturns:\\n    str: Formatted tool descriptions in markdown\\n\\nExample:\\n    >>> toolkit = Toolkit()\\n    >>> toolkit.register_tool(MyTool())\\n    >>> desc = toolkit.get_tools_description()\\n    >>> print(desc)\\n    Available tools:\\n    Tool Name: <tool_name>\\n    Description: <tool_description>\\n    When to use: <when_to_use>\\n    Tool Inputs:\\n    - <input_name>: <input_type> (<required/optional>) â€” <input_description>\\n      ...\\n    Tool Return: <return_type> â€” <return_description>\\n    \",\n",
       "       'args': ['self']},\n",
       "      {'name': 'build_tool_args',\n",
       "       'docstring': 'Merge LLM-provided args with system/graph-provided args\\nbased on ToolParam.llm flag and optional bindings.',\n",
       "       'args': ['tool_spec', 'llm_args', 'context', 'bindings']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.base_tool': {'path': 'tools/base_tool.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/base_tool.py',\n",
       "   'package': 'neurosurfer.tools',\n",
       "   'classes': [{'name': 'ToolResponse',\n",
       "     'docstring': 'Structured response from tool execution.\\n\\nThis dataclass encapsulates the output of a tool, including whether it\\nrepresents a final answer, the results, and any extra data\\nto pass to subsequent tool calls.\\n\\nAttributes:\\n    final_answer (bool): If True, the results should be treated as the\\n        final answer to the user\\'s query (no further tool calls needed)\\n    results (Union[str, Generator]): The tool\\'s output. Can be a string\\n        or a generator for streaming responses (ChatCompletionChunk style)\\n    extras (dict): Additional data to store in agent memory for subsequent\\n        tool calls. Default: {}\\n\\nExample:\\n    >>> # Simple tool response\\n    >>> response = ToolResponse(\\n    ...     final_answer=False,\\n    ...     results=\"Found 3 matching records\",\\n    ...     extras={\"record_ids\": [1, 2, 3]}\\n    ... )\\n    >>> \\n    >>> # Final answer response\\n    >>> response = ToolResponse(\\n    ...     final_answer=True,\\n    ...     results=\"The answer is 42\"\\n    ... )',\n",
       "     'methods': []},\n",
       "    {'name': 'BaseTool',\n",
       "     'docstring': 'Abstract base class for all tools in Neurosurfer.\\n\\nTools extend agent capabilities by providing specific functionalities.\\nEach tool must define a ToolSpec that describes its name, description,\\nand input parameters. The spec is used by agents to understand how to\\ninvoke the tool.\\n\\nAttributes:\\n    spec (ToolSpec): Tool specification defining name, description, and inputs\\n\\nAbstract Methods:\\n    __call__(): Execute the tool with provided inputs\\n\\nExample:\\n    >>> from neurosurfer.tools import BaseTool, ToolResponse\\n    >>> from neurosurfer.tools.tool_spec import ToolSpec, InputParam\\n    >>> \\n    >>> class MyTool(BaseTool):\\n    ...     spec = ToolSpec(\\n    ...         name=\"my_tool\",\\n    ...         description=\"Does something useful\",\\n    ...         inputs=[\\n    ...             InputParam(name=\"query\", type=\"string\", description=\"Input query\", required=True)\\n    ...         ]\\n    ...     )\\n    ...     \\n    ...     def __call__(self, query: str, **kwargs):\\n    ...         result = f\"Processed: {query}\"\\n    ...         return ToolResponse(final_answer=False, results=result)\\n    >>> \\n    >>> tool = MyTool()\\n    >>> response = tool(query=\"test\")',\n",
       "     'methods': [{'name': 'get_tool_description',\n",
       "       'docstring': \"Generate a formatted description of the tool. This description is used by the agent to understand the tool.\\nThis description doesn't include the params which are not generated by the LLM, so the LLM knows only the params\\nit should generate.\\n\\nReturns:\\n    str: Formatted tool description in markdown\",\n",
       "       'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.tool_spec': {'path': 'tools/tool_spec.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/tool_spec.py',\n",
       "   'package': 'neurosurfer.tools',\n",
       "   'classes': [{'name': 'ToolParam',\n",
       "     'docstring': 'Specification for a tool input parameter.\\n\\nDefines the name, type, description, and requirement status of a\\nsingle tool parameter. Used for validation and documentation.\\n\\nAttributes:\\n    name (str): Parameter name\\n    type (str): Parameter type (string, integer, number, boolean, array, object)\\n    description (str): Human-readable description of the parameter\\n    required (bool): Whether the parameter is required. Default: True\\n\\nExample:\\n    >>> param = ToolParam(\\n    ...     name=\"query\",\\n    ...     type=\"string\",\\n    ...     description=\"SQL query to execute\",\\n    ...     required=True\\n    ... )',\n",
       "     'methods': []},\n",
       "    {'name': 'ToolReturn',\n",
       "     'docstring': 'Specification for a tool\\'s return value.\\n\\nDefines the type and description of what a tool returns.\\n\\nAttributes:\\n    type (str): Return type (string, integer, number, boolean, array, object)\\n    description (str): Human-readable description of the return value\\n\\nExample:\\n    >>> ret = ToolReturn(\\n    ...     type=\"object\",\\n    ...     description=\"Query results as a dictionary\"\\n    ... )',\n",
       "     'methods': []},\n",
       "    {'name': 'ToolSpec',\n",
       "     'docstring': 'Complete specification for a tool.\\n\\nDefines all metadata needed for a tool: name, description, usage guidelines,\\ninput parameters, and return type. Used for validation, documentation, and\\nagent understanding.\\n\\nAttributes:\\n    name (str): Unique tool identifier\\n    description (str): Brief description of what the tool does\\n    when_to_use (str): Guidelines for when to use this tool\\n    inputs (List[ToolParam]): List of input parameter specifications\\n    returns (ToolReturn): Return value specification\\n\\nMethods:\\n    validate(): Validate the specification\\n    to_json(): Convert to JSON-serializable dict\\n    check_inputs(): Validate and sanitize runtime inputs\\n\\nExample:\\n    >>> spec = ToolSpec(\\n    ...     name=\"web_search\",\\n    ...     description=\"Search the web for information\",\\n    ...     when_to_use=\"When you need current information from the internet\",\\n    ...     inputs=[\\n    ...         ToolParam(name=\"query\", type=\"string\", description=\"Search query\", required=True),\\n    ...         ToolParam(name=\"max_results\", type=\"integer\", description=\"Max results\", required=False)\\n    ...     ],\\n    ...     returns=ToolReturn(type=\"array\", description=\"List of search results\")\\n    ... )\\n    >>> spec.validate()\\n    >>> inputs = spec.check_inputs({\"query\": \"AI news\"})',\n",
       "     'methods': [{'name': 'validate',\n",
       "       'docstring': 'Validate the tool specification.\\n\\nChecks that:\\n- Name, description, and when_to_use are non-empty\\n- At least one input parameter is defined\\n- All parameter types are supported\\n- Parameter names are unique\\n- Return type is supported\\n\\nRaises:\\n    ValueError: If validation fails\\n\\nExample:\\n    >>> spec = ToolSpec(...)\\n    >>> spec.validate()  # Raises ValueError if invalid',\n",
       "       'args': ['self']},\n",
       "      {'name': 'to_json',\n",
       "       'docstring': 'Convert specification to JSON-serializable dictionary.\\n\\nReturns:\\n    Dict[str, Any]: Dictionary representation of the spec\\n\\nExample:\\n    >>> spec = ToolSpec(...)\\n    >>> json_data = spec.to_json()\\n    >>> print(json_data[\"name\"])\\n    \\'my_tool\\'',\n",
       "       'args': ['self']},\n",
       "      {'name': 'parse_inputs', 'docstring': None, 'args': ['self', 'raw']},\n",
       "      {'name': 'check_inputs',\n",
       "       'docstring': 'Validate and sanitize runtime inputs against the specification.\\n\\nPerforms strict validation:\\n- Ensures all required parameters are present\\n- Rejects extra/unknown parameters\\n- Validates parameter types\\n\\nArgs:\\n    raw (Dict[str, Any]): Raw input dictionary from LLM or user\\n    relax (bool): If True, relaxes validation (e.g. only check passed parameters)\\n\\nReturns:\\n    Dict[str, Any]: Validated input dictionary (same as input if valid)\\n\\nRaises:\\n    ValueError: If validation fails (missing required, wrong type, extra params)\\n\\nExample:\\n    >>> spec = ToolSpec(\\n    ...     inputs=[ToolParam(name=\"x\", type=\"number\", description=\"X\", required=True)],\\n    ...     ...\\n    ... )\\n    >>> validated = spec.check_inputs({\"x\": 42})\\n    >>> # Raises ValueError:\\n    >>> spec.check_inputs({\"x\": \"not a number\"})\\n    >>> spec.check_inputs({\"y\": 42})  # Missing required \\'x\\'\\n    >>> spec.check_inputs({\"x\": 42, \"z\": 10})  # Extra param \\'z\\'',\n",
       "       'args': ['self', 'raw', 'relax']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.__init__': {'path': 'tools/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/__init__.py',\n",
       "   'package': 'neurosurfer.tools',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.mermaid.erd_diagram_generator': {'path': 'tools/mermaid/erd_diagram_generator.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/mermaid/erd_diagram_generator.py',\n",
       "   'package': 'neurosurfer.tools.mermaid',\n",
       "   'classes': [{'name': 'ERDDiagramGenerator',\n",
       "     'docstring': None,\n",
       "     'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.mermaid.mermaid_diagram_generator': {'path': 'tools/mermaid/mermaid_diagram_generator.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/mermaid/mermaid_diagram_generator.py',\n",
       "   'package': 'neurosurfer.tools.mermaid',\n",
       "   'classes': [{'name': 'MermaidDiagramGenerator',\n",
       "     'docstring': None,\n",
       "     'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.common.general_query_assistant': {'path': 'tools/common/general_query_assistant.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/common/general_query_assistant.py',\n",
       "   'package': 'neurosurfer.tools.common',\n",
       "   'classes': [{'name': 'GeneralQueryAssistantTool',\n",
       "     'docstring': None,\n",
       "     'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.common.__init__': {'path': 'tools/common/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/common/__init__.py',\n",
       "   'package': 'neurosurfer.tools.common',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.sql.final_answer_formatter': {'path': 'tools/sql/final_answer_formatter.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/sql/final_answer_formatter.py',\n",
       "   'package': 'neurosurfer.tools.sql',\n",
       "   'classes': [{'name': 'FinalAnswerFormatter',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'preprocess_sql_results_for_llm',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'user_query', 'db_results']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.sql.sql_executor': {'path': 'tools/sql/sql_executor.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/sql/sql_executor.py',\n",
       "   'package': 'neurosurfer.tools.sql',\n",
       "   'classes': [{'name': 'SQLExecutor',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'get_results_with_columns',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'query']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.sql.db_insights_tool': {'path': 'tools/sql/db_insights_tool.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/sql/db_insights_tool.py',\n",
       "   'package': 'neurosurfer.tools.sql',\n",
       "   'classes': [{'name': 'DBInsightsTool',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'get_tables_summaries__',\n",
       "       'docstring': None,\n",
       "       'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.sql.relevant_tables_schema_retriever': {'path': 'tools/sql/relevant_tables_schema_retriever.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/sql/relevant_tables_schema_retriever.py',\n",
       "   'package': 'neurosurfer.tools.sql',\n",
       "   'classes': [{'name': 'RelevantTableSchemaFinderLLM',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'get_tables_summaries__',\n",
       "       'docstring': None,\n",
       "       'args': ['self']},\n",
       "      {'name': 'get_table_schema',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'tables']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.sql.sql_query_generator': {'path': 'tools/sql/sql_query_generator.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/sql/sql_query_generator.py',\n",
       "   'package': 'neurosurfer.tools.sql',\n",
       "   'classes': [{'name': 'SQLQueryGenerator',\n",
       "     'docstring': None,\n",
       "     'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.sql.__init__': {'path': 'tools/sql/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/sql/__init__.py',\n",
       "   'package': 'neurosurfer.tools.sql',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.rag.docs_generator': {'path': 'tools/rag/docs_generator.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/rag/docs_generator.py',\n",
       "   'package': 'neurosurfer.tools.rag',\n",
       "   'classes': [{'name': 'DocsGenerator',\n",
       "     'docstring': 'Generates a full project documentation by iterating fixed sections and synthesizing each one with RAG context.\\nThe tool streams a single merged Markdown document (grand doc).\\n\\nPurpose:\\n  Create a high-level documentation template using a fixed section list and the project\\'s folder structure,\\n  avoiding large single-prompt context windows by generating section-wise.\\n\\nInputs:\\n  - query (str): Optional short description/instruction for the doc generation run (e.g., style hints).\\n    All other runtime options come via **kwargs.\\n\\nCommon kwargs:\\n  - folder_structure (str): The textual tree of the project directories/files. Recommended.\\n  - sections (List[str]): Ordered list of section titles to generate. Defaults to the hybrid template.\\n  - section_instructions (Dict[str, str]): Extra hints per section title.\\n  - chat_history (List[Dict]): Prior conversation turns if you track them.\\n  - top_k (int): Retrieval depth (default 10).\\n  - temperature (float): LLM sampling temperature (default 0.3).\\n  - max_new_tokens (int): Cap per-section generation (default 700).\\n  - prepend_title (bool): Whether to start doc with \"# Project Documentation\" (default True).\\n  - doc_title (str): Custom H1 title if prepend_title is True (default \"Project Documentation\").',\n",
       "     'methods': [{'name': 'plan_docs',\n",
       "       'docstring': 'Returns a mapping: {section_title: instructions}',\n",
       "       'args': ['self',\n",
       "        'user_query',\n",
       "        'folder_structure',\n",
       "        'temperature',\n",
       "        'max_new_tokens',\n",
       "        'verbose',\n",
       "        'extra_hl_query',\n",
       "        'candidate_pool_size',\n",
       "        'n_files',\n",
       "        'max_total_chunks']},\n",
       "      {'name': 'build_high_level_queries',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'extra_query']},\n",
       "      {'name': 'diverse_pick',\n",
       "       'docstring': 'Two-pass selector:\\n1) satisfy per_file_min for as many files as possible\\n2) fill remaining by score order, respecting per_file_cap',\n",
       "       'args': ['self',\n",
       "        'hits',\n",
       "        'k',\n",
       "        'file_key',\n",
       "        'per_file_min',\n",
       "        'per_file_cap']},\n",
       "      {'name': 'select_high_level_context',\n",
       "       'docstring': None,\n",
       "       'args': ['self',\n",
       "        'extra_query',\n",
       "        'candidate_pool_size',\n",
       "        'n_files',\n",
       "        'max_total_chunks',\n",
       "        'per_file_min',\n",
       "        'per_file_cap',\n",
       "        'overfetch_factor',\n",
       "        'file_key',\n",
       "        'max_chars_per_chunk']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.rag.simple_query_assitant': {'path': 'tools/rag/simple_query_assitant.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/rag/simple_query_assitant.py',\n",
       "   'package': 'neurosurfer.tools.rag',\n",
       "   'classes': [{'name': 'RagSimpleAnswerTool',\n",
       "     'docstring': None,\n",
       "     'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.websearch.extractor': {'path': 'tools/websearch/extractor.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/extractor.py',\n",
       "   'package': 'neurosurfer.tools.websearch',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'get_domain',\n",
       "     'docstring': \"Extract normalized domain from URL (netloc without leading 'www.').\",\n",
       "     'args': ['url']},\n",
       "    {'name': 'html_to_text',\n",
       "     'docstring': 'Convert HTML to cleaned plain text using BeautifulSoup.\\n\\nUses domain-specific selectors from `domain_content_config` when possible,\\nfalling back to generic extraction otherwise.',\n",
       "     'args': ['html', 'url', 'domain_content_config']}]},\n",
       "  'neurosurfer.tools.websearch.tool': {'path': 'tools/websearch/tool.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/tool.py',\n",
       "   'package': 'neurosurfer.tools.websearch',\n",
       "   'classes': [{'name': 'WebSearchTool',\n",
       "     'docstring': 'Unified web search + optional crawl + optional LLM summarization.\\n\\nThis tool:\\n  1. Uses a pluggable engine (SerpAPI now; Tavily, etc. later) to get SERP results.\\n  2. Optionally crawls the top results, with smart domain-specific extraction.\\n  3. Optionally asks an LLM to create a long, refined summary of the results.\\n\\nParameters common across engines:\\n  - engine: which backend to use (\"serpapi\", ...)\\n  - engine_kwargs: dict with engine-specific config (api_key, endpoint, ...)\\n  - max_results, location, gl: SERP-level knobs\\n  - enable_crawl, max_crawl_results, max_concurrent_crawls, etc.\\n  - domain_content_config, preferred_domains: smarter content extraction\\n  - llm: optional LLM instance for summarization',\n",
       "     'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.websearch.utils': {'path': 'tools/websearch/utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/utils.py',\n",
       "   'package': 'neurosurfer.tools.websearch',\n",
       "   'classes': [{'name': 'WebSearchResult',\n",
       "     'docstring': 'Final normalized result, ready for LLM use.\\n\\nCombines:\\n  - engine-level info (title, url, snippet, score)\\n  - optional crawled content and length\\n  - crawl error, if any',\n",
       "     'methods': []}],\n",
       "   'functions': [{'name': 'build_results_with_crawl',\n",
       "     'docstring': 'Crawl top N results in parallel and return WebSearchResult list\\nin the original SERP order.\\n\\nIf preferred_domains are configured, we first try to crawl those. If none\\nare found, we fall back to the usual top-N.',\n",
       "     'args': ['config', 'engine_results']},\n",
       "    {'name': 'build_result_from_engine_result',\n",
       "     'docstring': None,\n",
       "     'args': ['config', 'item', 'crawl']},\n",
       "    {'name': 'crawl_url',\n",
       "     'docstring': 'Fetch page content and extract clean text.\\n\\nReturns a dict:\\n    {\\n        \"content\": str | None,\\n        \"error\": str | None,\\n    }',\n",
       "     'args': ['url', 'config']},\n",
       "    {'name': 'is_preferred_domain',\n",
       "     'docstring': None,\n",
       "     'args': ['url', 'preferred_domains']},\n",
       "    {'name': 'limit_content_length',\n",
       "     'docstring': 'Truncate `content` to at most `max_words` words using a given strategy.\\n\\nStrategies\\n----------\\n- \"first\":       Keep the first `max_words` words.\\n- \"last\":        Keep the last `max_words` words.\\n- \"middle\":      Keep a centered window of `max_words` words.\\n- \"head_tail\":   Split budget between start and end, join with \"...\".\\n- \"distributive\":Pick several short chunks distributed across the text,\\n                 joined with \"...\".',\n",
       "     'args': ['content',\n",
       "      'max_words',\n",
       "      'strategy',\n",
       "      'distributive_segment_size']},\n",
       "    {'name': 'summarize_with_llm',\n",
       "     'docstring': 'Use the configured LLM to produce a long, detailed summary of the search results.',\n",
       "     'args': ['llm', 'results_dict']}]},\n",
       "  'neurosurfer.tools.websearch.config': {'path': 'tools/websearch/config.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/config.py',\n",
       "   'package': 'neurosurfer.tools.websearch',\n",
       "   'classes': [{'name': 'WebSearchConfig', 'docstring': None, 'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.websearch.templates': {'path': 'tools/websearch/templates.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/templates.py',\n",
       "   'package': 'neurosurfer.tools.websearch',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.websearch.__init__': {'path': 'tools/websearch/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/__init__.py',\n",
       "   'package': 'neurosurfer.tools.websearch',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.websearch.rag': {'path': 'tools/websearch/rag.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/rag.py',\n",
       "   'package': 'neurosurfer.tools.websearch',\n",
       "   'classes': [{'name': 'WebSearchRAGResult',\n",
       "     'docstring': 'Return payload from RAG refinement of web search results.',\n",
       "     'methods': []}],\n",
       "   'functions': [{'name': 'run_web_rag',\n",
       "     'docstring': 'RAG-style refinement over normalized web search results.\\n\\n- Re-scores each result embedding vs. the query embedding.\\n- Selects top_k results.\\n- Builds a single concatenated context string and trims it to\\n  `max_context_tokens`.\\n- Optionally runs an LLM summarization over that context.\\n\\nThis is *ephemeral RAG* â€” no persistent vectorstore, just in-memory\\nembeddings for the current tool call.',\n",
       "     'args': ['query', 'results', 'llm', 'embedder', 'config']},\n",
       "    {'name': 'run_web_rag_on_results_dict',\n",
       "     'docstring': 'Convenience wrapper when you already have the full `results_dict`\\nproduced by WebSearchTool.',\n",
       "     'args': ['results_dict', 'llm', 'embedder', 'config']}]},\n",
       "  'neurosurfer.tools.websearch.engines.serpapi': {'path': 'tools/websearch/engines/serpapi.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/engines/serpapi.py',\n",
       "   'package': 'neurosurfer.tools.websearch.engines',\n",
       "   'classes': [{'name': 'SerpApiEngine',\n",
       "     'docstring': 'SerpAPI-backed implementation of SearchEngine (Google engine).',\n",
       "     'methods': [{'name': 'search',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'query', 'hl', 'max_results', 'location', 'gl']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.websearch.engines.base': {'path': 'tools/websearch/engines/base.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/engines/base.py',\n",
       "   'package': 'neurosurfer.tools.websearch.engines',\n",
       "   'classes': [{'name': 'EngineResult',\n",
       "     'docstring': 'One normalized result item from a search engine.\\n\\nEngines should populate at least:\\n  - title\\n  - url\\n  - snippet\\n\\nOptional:\\n  - score (relevance, rank, etc.)',\n",
       "     'methods': []},\n",
       "    {'name': 'EngineSearchMeta',\n",
       "     'docstring': 'Metadata about a search engine call.\\n\\ntotal_results:\\n    Estimated total results (if available).\\nprovider:\\n    Name/identifier of the provider (e.g. \"serpapi\", \"tavily\").\\nextra:\\n    Engine-specific metadata.',\n",
       "     'methods': []},\n",
       "    {'name': 'SearchEngine',\n",
       "     'docstring': 'Abstract base class for web search engines.\\n\\nConcrete engines (SerpApiEngine, TavilyEngine, etc.) must implement\\nthe `search()` method and expose a `name` attribute.',\n",
       "     'methods': [{'name': 'search',\n",
       "       'docstring': 'Execute a search and return:\\n\\n    (results, meta, raw)\\n\\nwhere:\\n  - results: list[EngineResult]\\n  - meta: EngineSearchMeta\\n  - raw: engine-specific raw payload (dict or any JSON-like object)',\n",
       "       'args': ['self', 'query', 'hl', 'max_results', 'location', 'gl']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.tools.websearch.engines.__init__': {'path': 'tools/websearch/engines/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/tools/websearch/engines/__init__.py',\n",
       "   'package': 'neurosurfer.tools.websearch.engines',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.tools_router_agent': {'path': 'agents/tools_router_agent.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/tools_router_agent.py',\n",
       "   'package': 'neurosurfer.agents',\n",
       "   'classes': [{'name': 'RouterRetryPolicy', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ToolsRouterConfig', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ToolAndInputsModel', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ToolsRouterAgent',\n",
       "     'docstring': \"Minimal, production-ready tools router:\\n  1) Uses LLM to select exactly one tool + inputs (strict JSON).\\n  2) Validates inputs against the tool's ToolSpec (prunes unknowns if enabled).\\n  3) Executes the tool, proxying streaming or returning a string.\\n  4) Retries routing and tool execution with backoff (bounded).\",\n",
       "     'methods': [{'name': 'run',\n",
       "       'docstring': 'Decide a tool with the LLM (JSON: {\"tool\": \"...\", \"inputs\": {...}}),\\nvalidate/repair inputs, and execute that tool.\\n- If stream=True (default from config), returns a Generator[str].\\n- If stream=False, returns a str.\\nExtra **kwargs are forwarded into the tool call (merged with inputs).',\n",
       "       'args': ['self',\n",
       "        'user_query',\n",
       "        'chat_history',\n",
       "        'execute_tool',\n",
       "        'strict_tool_call',\n",
       "        'stream',\n",
       "        'temperature',\n",
       "        'max_new_tokens',\n",
       "        '**kwargs']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.__init__': {'path': 'agents/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/__init__.py',\n",
       "   'package': 'neurosurfer.agents',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.sql_agent': {'path': 'agents/sql_agent.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/sql_agent.py',\n",
       "   'package': 'neurosurfer.agents',\n",
       "   'classes': [{'name': 'SQLAgent',\n",
       "     'docstring': 'SQL-aware ReActAgent with DB connection, schema cache, and SQL tools pre-wired.\\n\\nExample:\\n    >>> llm = TransformersModel(model_name=\"meta-llama/Llama-3.2-3B-Instruct\")\\n    >>> agent = SQLAgent(llm=llm, db_uri=\"sqlite:///my.db\", sample_rows_in_table_info=3)\\n    >>> for chunk in agent.run(\"How many users registered last month?\"):\\n    ...     print(chunk, end=\"\")',\n",
       "     'methods': [{'name': 'train',\n",
       "       'docstring': 'Warm up schema cache (optionally summarize). This yields progress strings.',\n",
       "       'args': ['self', 'summarize', 'force']},\n",
       "      {'name': 'is_trained',\n",
       "       'docstring': 'True if we have at least one cached schema summary.',\n",
       "       'args': ['self']},\n",
       "      {'name': 'register_tool',\n",
       "       'docstring': 'Register an extra tool and refresh the parent toolkit reference.',\n",
       "       'args': ['self', 'tool']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.common.utils': {'path': 'agents/common/utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/common/utils.py',\n",
       "   'package': 'neurosurfer.agents.common',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'stream_text_from_response',\n",
       "     'docstring': None,\n",
       "     'args': ['resp']},\n",
       "    {'name': 'nonstream_text_from_response',\n",
       "     'docstring': None,\n",
       "     'args': ['resp']},\n",
       "    {'name': 'normalize_response',\n",
       "     'docstring': 'For tool responses, normalize into either a string or a generator[str].',\n",
       "     'args': ['results']},\n",
       "    {'name': 'extract_and_repair_json',\n",
       "     'docstring': 'Extract and parse the first valid JSON object from arbitrary text (e.g., LLM output).\\n\\nHandles:\\n- Markdown code fences (```json ... ```)\\n- Extraneous text before/after JSON\\n- Nested braces {...}\\n- Incomplete or malformed chunks (best-effort parsing)\\n\\nReturns:\\n    dict if successful, otherwise None.',\n",
       "     'args': ['text', 'return_dict']},\n",
       "    {'name': 'rprint', 'docstring': None, 'args': ['msg', 'color', 'rich']}]},\n",
       "  'neurosurfer.agents.rag.ingestor': {'path': 'agents/rag/ingestor.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/ingestor.py',\n",
       "   'package': 'neurosurfer.agents.rag',\n",
       "   'classes': [{'name': 'RAGIngestor',\n",
       "     'docstring': 'Production-grade RAG document ingestor.\\n\\nThis class orchestrates the complete document ingestion pipeline:\\nreading, chunking, embedding, deduplication, and vector storage.\\n\\nIt supports multiple input sources and provides progress tracking,\\ncancellation, and parallel processing capabilities.\\n\\nAttributes:\\n    embedder (BaseEmbedder): Embedding model for generating vectors\\n    vs (BaseVectorDB): Vector store for document storage\\n    reader (FileReader): File reader for various formats\\n    chunker (Chunker): Document chunker\\n    log (logging.Logger): Logger instance\\n    progress_cb (Optional[ProgressCallback]): Progress callback function\\n    cancel_event (threading.Event): Event for cancellation\\n    batch_size (int): Batch size for embedding generation\\n    max_workers (int): Max parallel workers for processing\\n    deduplicate (bool): Enable content-based deduplication\\n    normalize_embeddings (bool): Normalize embedding vectors\\n\\nExample:\\n    >>> ingestor = RAGIngestor(\\n    ...     embedder=embedder,\\n    ...     vectorstore=vectorstore,\\n    ...     batch_size=64,\\n    ...     progress_cb=lambda p: print(f\"Progress: {p[\\'percent\\']:.1f}%\")\\n    ... )\\n    >>> \\n    >>> # Add multiple sources\\n    >>> ingestor.add_files([\"./docs\", \"./code\"])\\n    >>> ingestor.add_text(\"Custom content\", metadata={\"source\": \"manual\"})\\n    >>> \\n    >>> # Ingest all\\n    >>> stats = ingestor.ingest()\\n    >>> print(f\"Ingested {stats[\\'chunks_added\\']} chunks\")',\n",
       "     'methods': [{'name': 'set_vectorstore',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'vectorstore']},\n",
       "      {'name': 'add_files',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'paths', 'include_exts', 'root', 'extra_metadata']},\n",
       "      {'name': 'add_directory',\n",
       "       'docstring': None,\n",
       "       'args': ['self',\n",
       "        'directory',\n",
       "        'include_exts',\n",
       "        'exclude_dirs',\n",
       "        'extra_metadata']},\n",
       "      {'name': 'add_texts',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'texts', 'base_id', 'metadatas']},\n",
       "      {'name': 'add_urls',\n",
       "       'docstring': 'Pass a custom fetcher to keep this offline-friendly. Example fetcher can use requests/bs4.',\n",
       "       'args': ['self', 'urls', 'fetcher', 'extra_metadata']},\n",
       "      {'name': 'add_git_folder',\n",
       "       'docstring': 'Index an already-cloned repo folder (avoids adding .git, node_modules, etc.).',\n",
       "       'args': ['self', 'repo_root', 'include_exts', 'extra_metadata']},\n",
       "      {'name': 'add_zipfile',\n",
       "       'docstring': \"Extract a single .zip into a temporary folder, index its contents using add_directory,\\nthen delete the temp folder. Adds {'source_zip': <zip filename>} to each doc's metadata.\",\n",
       "       'args': ['self',\n",
       "        'zip_path',\n",
       "        'include_exts',\n",
       "        'exclude_dirs',\n",
       "        'extra_metadata']},\n",
       "      {'name': 'ingest',\n",
       "       'docstring': 'Unified high-level ingestion wrapper.\\n\\nThis method accepts a mix of paths, URLs, and raw text, routes them to the\\nappropriate add_* methods, and then runs the full pipeline via `build()`.\\n\\nSupported source forms:\\n    - File path (string or Path)\\n    - Directory path (string or Path)\\n    - .zip archive (string or Path)\\n    - Git repo folder (directory containing a .git subfolder)\\n    - URL string (starting with http:// or https://)\\n    - Raw text string (everything else)\\n\\nParameters\\n----------\\nsources:\\n    A single source or iterable of sources. Each element can be:\\n    - str\\n    - pathlib.Path\\nurl_fetcher:\\n    Optional callable used by `add_urls`. Signature: (url: str) -> Optional[str].\\ninclude_exts:\\n    Allowed file extensions for file/directory/zip ingestion. Defaults to\\n    `supported_file_types`.\\nextra_metadata:\\n    Extra metadata merged into each queued document\\'s metadata.\\nreset_state:\\n    If True, clears previous queue and seen-hash set before processing.\\n\\nReturns\\n-------\\nDict[str, Any]\\n    A summary dictionary, typically the result of `build()`. On error, returns:\\n        {\\n            \"status\": \"error\",\\n            \"error\": \"<message>\",\\n            \"unsupported\": [...optional list of unsupported items...]\\n        }',\n",
       "       'args': ['self',\n",
       "        'sources',\n",
       "        'url_fetcher',\n",
       "        'include_exts',\n",
       "        'extra_metadata',\n",
       "        'reset_state']},\n",
       "      {'name': 'build',\n",
       "       'docstring': 'Execute the ingestion:\\n  - chunk queued sources\\n  - dedupe by chunk hash\\n  - embed in batches\\n  - write to vector store\\nReturns a summary dict.',\n",
       "       'args': ['self']},\n",
       "      {'name': 'embed_query', 'docstring': None, 'args': ['self', 'text']},\n",
       "      {'name': 'search',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'text', 'top_k']}]}],\n",
       "   'functions': [{'name': 'sha256_text',\n",
       "     'docstring': 'Generate SHA256 hash of text string.',\n",
       "     'args': ['s']},\n",
       "    {'name': 'now_ts', 'docstring': 'Get current timestamp.', 'args': []}]},\n",
       "  'neurosurfer.agents.rag.picker': {'path': 'agents/rag/picker.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/picker.py',\n",
       "   'package': 'neurosurfer.agents.rag',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'pick_files_by_grouped_chunk_hits',\n",
       "     'docstring': 'Broad similarity search -> aggregate by file -> top-N.\\nUseful for large codebases to decide focus files.',\n",
       "     'args': ['embedder',\n",
       "      'vector_db',\n",
       "      'section_query',\n",
       "      'candidate_pool_size',\n",
       "      'n_files',\n",
       "      'file_key']}]},\n",
       "  'neurosurfer.agents.rag.constants': {'path': 'agents/rag/constants.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/constants.py',\n",
       "   'package': 'neurosurfer.agents.rag',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.rag.config': {'path': 'agents/rag/config.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/config.py',\n",
       "   'package': 'neurosurfer.agents.rag',\n",
       "   'classes': [{'name': 'RetrieveResult', 'docstring': None, 'methods': []},\n",
       "    {'name': 'RetrievalPlan',\n",
       "     'docstring': 'Plan for how much to retrieve and how to shape the context.\\n\\nThis is intentionally simple and can be created by:\\n  - RAGGate (UI layer) or\\n  - RAGAgent itself via an internal LLM call.',\n",
       "     'methods': []},\n",
       "    {'name': 'RAGIngestorConfig', 'docstring': None, 'methods': []},\n",
       "    {'name': 'RAGAgentConfig', 'docstring': None, 'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.rag.chunker': {'path': 'agents/rag/chunker.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/chunker.py',\n",
       "   'package': 'neurosurfer.agents.rag',\n",
       "   'classes': [{'name': 'CustomChunkHandler',\n",
       "     'docstring': None,\n",
       "     'methods': []},\n",
       "    {'name': 'Chunker',\n",
       "     'docstring': 'Extensible document chunker with pluggable strategy registry for different file types.\\n\\nThis class provides intelligent document chunking for RAG (Retrieval-Augmented Generation)\\nsystems with support for multiple file formats and customizable chunking strategies.\\n\\nKey Features:\\n    - Pluggable strategy system for different file types (Python, JavaScript, JSON, Markdown, etc.)\\n    - Line-based chunking for code (preserves structure and context)\\n    - Character-based chunking for prose and generic text\\n    - Configurable overlap between chunks to maintain context\\n    - Comment-aware filtering to exclude comment-only blocks\\n    - Blacklist patterns for excluding unwanted file types\\n    - AST-based chunking for Python code with structural awareness\\n    - Custom handler system for specialized chunking logic\\n    - Router system for dynamic strategy selection\\n    - Comprehensive error handling and fallback mechanisms\\n\\nThe chunker follows a priority-based strategy selection:\\n1. Explicit custom handlers (callable or registered name)\\n2. Router function results\\n3. File extension mappings to custom handlers\\n4. Built-in strategies by file extension\\n5. Heuristic fallback (line-based for code, character-based for prose)\\n\\nArgs:\\n    config: ChunkerConfig instance controlling chunking behavior.\\n           Uses default configuration if not provided.\\n\\nExample:\\n    >>> config = ChunkerConfig(fallback_chunk_size=30, overlap_lines=5)\\n    >>> chunker = Chunker(config)\\n    >>>\\n    >>> # Chunk Python code with structural awareness\\n    >>> chunks = chunker.chunk(python_code, file_path=\"script.py\")\\n    >>>\\n    >>> # Register custom strategy for specific file types\\n    >>> def my_strategy(text, file_path):\\n    ...     return text.split(\"\\\\n\\\\n\")\\n    >>> chunker.register([\".custom\"], my_strategy)',\n",
       "     'methods': [{'name': 'register',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'exts', 'fn']},\n",
       "      {'name': 'set_logger', 'docstring': None, 'args': ['self', 'logger_fn']},\n",
       "      {'name': 'register_custom',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'name', 'handler']},\n",
       "      {'name': 'unregister_custom',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'name']},\n",
       "      {'name': 'list_custom_handlers', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'use_custom_for_ext',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'exts', 'handler_name']},\n",
       "      {'name': 'clear_custom_for_ext',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'exts']},\n",
       "      {'name': 'set_router',\n",
       "       'docstring': 'router(file_path, text) -> handler_name or None\\nCalled before strategies and fallbacks. Return a registered name or None.',\n",
       "       'args': ['self', 'router']},\n",
       "      {'name': 'list_ext_mappings', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'chunk',\n",
       "       'docstring': 'Chunk text into smaller pieces using a priority-based strategy selection.\\n\\nThe method uses the following priority order:\\n1. Explicit custom handler (callable or registered name)\\n2. Router function result pointing to registered custom handler\\n3. File extension mapping to custom handler\\n4. Built-in strategy registry by file extension\\n5. Heuristic fallback (line-based for code-like content, char-based for prose)\\n\\nArgs:\\n    text: The text content to be chunked\\n    source_id: Optional identifier for the source document (currently unused)\\n    file_path: Optional file path to determine file type and apply blacklist filtering\\n    k: Minimum word count threshold - if text has fewer words, returns as single chunk\\n    custom: Optional custom chunking handler (either registered name or callable)\\n\\nReturns:\\n    List of text chunks. Returns empty list if text is empty, file should be skipped,\\n    or no valid chunks can be generated.\\n\\nRaises:\\n    No exceptions raised - all errors are logged and handled gracefully with fallbacks.',\n",
       "       'args': ['self', 'text', 'source_id', 'file_path', 'k', 'custom']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.rag.context_builder': {'path': 'agents/rag/context_builder.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/context_builder.py',\n",
       "   'package': 'neurosurfer.agents.rag',\n",
       "   'classes': [{'name': 'ContextBuilder',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'build',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'docs']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.rag.templates': {'path': 'agents/rag/templates.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/templates.py',\n",
       "   'package': 'neurosurfer.agents.rag',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.rag.agent': {'path': 'agents/rag/agent.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/agent.py',\n",
       "   'package': 'neurosurfer.agents.rag',\n",
       "   'classes': [{'name': 'RAGAgent',\n",
       "     'docstring': 'Retrieval core for RAG pipelines. VectorDB- and embedder-agnostic.\\nAdds a convenient `run(...)` that makes a full LLM call with the retrieved context.',\n",
       "     'methods': [{'name': 'set_collection',\n",
       "       'docstring': 'Call this method to set a new collection for ingesting and retrieving documents.',\n",
       "       'args': ['self', 'collection_name', 'clear_collection_on_init']},\n",
       "      {'name': 'ingest',\n",
       "       'docstring': 'Unified high-level ingestion wrapper.\\n\\nThis method accepts a mix of paths, URLs, and raw text, routes them to the\\nappropriate add_* methods, and then runs the full pipeline via `build()`.\\n\\nSupported source forms:\\n    - File path (string or Path)\\n    - Directory path (string or Path)\\n    - .zip archive (string or Path)\\n    - Git repo folder (directory containing a .git subfolder)\\n    - URL string (starting with http:// or https://)\\n    - Raw text string (everything else)\\n\\nParameters\\n----------\\nsources:\\n    A single source or iterable of sources. Each element can be:\\n    - str\\n    - pathlib.Path\\nurl_fetcher:\\n    Optional callable used by `add_urls`. Signature: (url: str) -> Optional[str].\\ninclude_exts:\\n    Allowed file extensions for file/directory/zip ingestion. Defaults to\\n    `supported_file_types`.\\nextra_metadata:\\n    Extra metadata merged into each queued document\\'s metadata.\\nreset_state:\\n    If True, clears previous queue and seen-hash set before processing.\\n\\nReturns\\n-------\\nDict[str, Any]\\n    A summary dictionary, typically the result of `build()`. On error, returns:\\n        {\\n            \"status\": \"error\",\\n            \"error\": \"<message>\",\\n            \"unsupported\": [...optional list of unsupported items...]\\n        }',\n",
       "       'args': ['self',\n",
       "        'sources',\n",
       "        'url_fetcher',\n",
       "        'include_exts',\n",
       "        'extra_metadata',\n",
       "        'reset_state']},\n",
       "      {'name': 'retrieve',\n",
       "       'docstring': \"Public interface for retrieving documents from the vectorstore.\\n\\nArgs:\\n    user_query: The user's query string.\\n    top_k: Optional number of documents to retrieve.\\n    metadata_filter: Optional metadata filter for the search.\\n    similarity_threshold: Optional similarity threshold for the search.\\n\\nReturns:\\n    RetrieveResult: The result of the retrieval process.\",\n",
       "       'args': ['self',\n",
       "        'user_query',\n",
       "        'top_k',\n",
       "        'metadata_filter',\n",
       "        'similarity_threshold',\n",
       "        'retrieval_mode',\n",
       "        'retrieval_scope',\n",
       "        'retrieval_plan']},\n",
       "      {'name': 'run',\n",
       "       'docstring': \"Public interface for running the RAG pipeline.\\n\\nArgs:\\n    user_query: The user's query string.\\n    system_prompt: Optional system prompt for the LLM.\\n    chat_history: Optional chat history for the LLM.\\n    stream: Whether to stream the response.\\n    top_k: Optional number of documents to retrieve.\\n    metadata_filter: Optional metadata filter for the search.\\n    similarity_threshold: Optional similarity threshold for the search.\\n    temperature: Optional temperature for the LLM.\\n    **llm_kwargs: Additional keyword arguments for the LLM.\\nReturns:\\n    LLM_RESPONSE_TYPE:\\n        - If stream=False: Returns ChatCompletionResponse (Pydantic model)\\n        - If stream=True: Returns Generator yielding ChatCompletionChunk objects\\nRaises:\\n    ValueError: If vectorstore or embedder is not provided.\\n    ValueError: If LLM is not provided.\",\n",
       "       'args': ['self',\n",
       "        'user_query',\n",
       "        'system_prompt',\n",
       "        'chat_history',\n",
       "        'top_k',\n",
       "        'metadata_filter',\n",
       "        'similarity_threshold',\n",
       "        'temperature',\n",
       "        'stream',\n",
       "        '**llm_kwargs']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.rag.__init__': {'path': 'agents/rag/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/__init__.py',\n",
       "   'package': 'neurosurfer.agents.rag',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.rag.token_utils': {'path': 'agents/rag/token_utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/token_utils.py',\n",
       "   'package': 'neurosurfer.agents.rag',\n",
       "   'classes': [{'name': 'TokenCounter',\n",
       "     'docstring': 'Vendor-agnostic token utilities.\\n- If llm.tokenizer is present (HF or similar), use it.\\n- Else, try tiktoken if available (OpenAI-like approximation).\\n- Else, fall back to char-based heuristic (chars_per_token).',\n",
       "     'methods': [{'name': 'count',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'text']},\n",
       "      {'name': 'trim_to_tokens',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'text', 'max_tokens']},\n",
       "      {'name': 'apply_chat_template',\n",
       "       'docstring': 'Try llm.tokenizer.apply_chat_template; else naive concatenation.',\n",
       "       'args': ['self', 'messages']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.rag.filereader': {'path': 'agents/rag/filereader.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/rag/filereader.py',\n",
       "   'package': 'neurosurfer.agents.rag',\n",
       "   'classes': [{'name': 'FileReader',\n",
       "     'docstring': 'Unified file reader for multiple formats.\\n\\nThis class provides a single interface for reading various file types\\ninto plain text. It automatically detects the file type by extension\\nand applies the appropriate reader method.\\n\\nAttributes:\\n    supported_types (dict): Mapping of file extensions to reader functions\\n\\nExample:\\n    >>> reader = FileReader()\\n    >>> \\n    >>> # Read different file types\\n    >>> pdf_text = reader.read(\"report.pdf\")\\n    >>> excel_text = reader.read(\"data.xlsx\")\\n    >>> code_text = reader.read(\"script.py\")\\n    >>> \\n    >>> # Check supported types\\n    >>> print(reader.supported_types.keys())',\n",
       "     'methods': [{'name': 'read',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'file_path']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.react.memory': {'path': 'agents/react/memory.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/memory.py',\n",
       "   'package': 'neurosurfer.agents.react',\n",
       "   'classes': [{'name': 'EphemeralMemory',\n",
       "     'docstring': 'Short-lived scratch memory for passing small items between steps.\\nCleared after each tool execution (by agent).',\n",
       "     'methods': [{'name': 'set',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'key', 'val']},\n",
       "      {'name': 'items', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'clear', 'docstring': None, 'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.react.types': {'path': 'agents/react/types.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/types.py',\n",
       "   'package': 'neurosurfer.agents.react',\n",
       "   'classes': [{'name': 'ToolCall', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ReactAgentResponse', 'docstring': None, 'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.react.history': {'path': 'agents/react/history.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/history.py',\n",
       "   'package': 'neurosurfer.agents.react',\n",
       "   'classes': [{'name': 'History',\n",
       "     'docstring': 'A simple list of strings appended in the loop.\\nEach entry is typically a block like:\\n  - Thought: ...\\n  - Action: {...}\\n  - results: ...',\n",
       "     'methods': [{'name': 'append',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'line']},\n",
       "      {'name': 'as_text', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'to_prompt',\n",
       "       'docstring': 'Render the previous reasoning steps as a readable history.\\n\\nWe emphasize that these are *past* steps so the model is\\nless tempted to repeat them verbatim.',\n",
       "       'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.react.retry': {'path': 'agents/react/retry.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/retry.py',\n",
       "   'package': 'neurosurfer.agents.react',\n",
       "   'classes': [{'name': 'RetryPolicy',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'sleep',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'attempt']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.react.config': {'path': 'agents/react/config.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/config.py',\n",
       "   'package': 'neurosurfer.agents.react',\n",
       "   'classes': [{'name': 'ReActConfig', 'docstring': None, 'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.react.parser': {'path': 'agents/react/parser.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/parser.py',\n",
       "   'package': 'neurosurfer.agents.react',\n",
       "   'classes': [{'name': 'ToolCallParser',\n",
       "     'docstring': 'Extracts and normalizes a tool call from an LLM message.\\n- tolerant to fenced blocks\\n- tolerant to trailing commas and missing braces\\n- returns ToolCall',\n",
       "     'methods': [{'name': 'extract',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'text']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.react.base': {'path': 'agents/react/base.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/base.py',\n",
       "   'package': 'neurosurfer.agents.react',\n",
       "   'classes': [{'name': 'AgentDelimiters', 'docstring': None, 'methods': []},\n",
       "    {'name': 'BaseAgent',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'stop_generation',\n",
       "       'docstring': None,\n",
       "       'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.react.scratchpad': {'path': 'agents/react/scratchpad.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/scratchpad.py',\n",
       "   'package': 'neurosurfer.agents.react',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.react.agent': {'path': 'agents/react/agent.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/agent.py',\n",
       "   'package': 'neurosurfer.agents.react',\n",
       "   'classes': [{'name': 'ReActAgent',\n",
       "     'docstring': 'Production-ready ReAct Agent with:\\n- tolerant Action parsing\\n- input sanitization vs ToolSpec (drop extras or repair)\\n- bounded retries on parse & tool errors\\n- reusable base class & utilities',\n",
       "     'methods': [{'name': 'run',\n",
       "       'docstring': 'Run the ReAct agent.\\n\\n- If stream=True: returns ReactAgentResponse with a Generator[str]\\n  that yields tokens as they are produced.\\n- If stream=False: returns ReactAgentResponse with the full string\\n  (we still use streaming under the hood, but we buffer it).',\n",
       "       'args': ['self',\n",
       "        'query',\n",
       "        'stream',\n",
       "        'temperature',\n",
       "        'max_new_tokens',\n",
       "        'specific_instructions',\n",
       "        'context',\n",
       "        '_route_extra_instructions',\n",
       "        'reset_tracer']},\n",
       "      {'name': 'stop_generation', 'docstring': None, 'args': ['self']},\n",
       "      {'name': 'update_toolkit',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'toolkit']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.react.exceptions': {'path': 'agents/react/exceptions.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/exceptions.py',\n",
       "   'package': 'neurosurfer.agents.react',\n",
       "   'classes': [{'name': 'ToolCallParseError',\n",
       "     'docstring': None,\n",
       "     'methods': []},\n",
       "    {'name': 'ToolExecutionError', 'docstring': None, 'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.react.__init__': {'path': 'agents/react/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/react/__init__.py',\n",
       "   'package': 'neurosurfer.agents.react',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.graph.loader': {'path': 'agents/graph/loader.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/loader.py',\n",
       "   'package': 'neurosurfer.agents.graph',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'load_graph_from_dict',\n",
       "     'docstring': 'Load a Graph from a raw dict, with:\\n  - Unknown keys warned and ignored\\n  - Detailed error messages on schema violations\\n  - Semantic validation of outputs / depends_on / DAG structure',\n",
       "     'args': ['data']},\n",
       "    {'name': 'load_graph',\n",
       "     'docstring': 'Load a Graph from a YAML or JSON file.\\n\\nFeatures:\\n  - YAML/JSON parse errors are reported with filename and cause.\\n  - Unknown keys are warned and ignored (top-level + per-node).\\n  - Schema errors show precise locations (e.g., nodes[1].id).\\n  - Semantic errors (unknown outputs, bad depends_on, cycles) are reported\\n    via GraphConfigurationError with clear messages.\\n\\nExample:\\n    spec = load_graph(\"flows/blog_workflow.yaml\")',\n",
       "     'args': ['path']}]},\n",
       "  'neurosurfer.agents.graph.manager': {'path': 'agents/graph/manager.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/manager.py',\n",
       "   'package': 'neurosurfer.agents.graph',\n",
       "   'classes': [{'name': 'ManagerConfig', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ManagerAgent',\n",
       "     'docstring': \"LLM-based manager that composes the `user_prompt` for each node's Agent.\\n\\nIt does NOT execute tools or call the underlying Agents itself;\\nit only crafts prompts based on:\\n  - Node spec (purpose / goal / expected_result / tools)\\n  - Original graph inputs\\n  - Dependency results\\n  - Previous node result\",\n",
       "     'methods': [{'name': 'compose_user_prompt',\n",
       "       'docstring': 'Compose the next `user_prompt` for the given node.\\n\\nReturns a plain string to pass into `Agent.run(...)`.',\n",
       "       'args': ['self',\n",
       "        'node',\n",
       "        'graph_inputs',\n",
       "        'dependency_results',\n",
       "        'previous_result',\n",
       "        'temperature',\n",
       "        'max_new_tokens']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.graph.utils': {'path': 'agents/graph/utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/utils.py',\n",
       "   'package': 'neurosurfer.agents.graph',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'get_from_ctx',\n",
       "     'docstring': 'Resolve a dotted path like \"nodes.research.summary\" or \"inputs.topic\".\\nSupports nested dicts and simple list indices.',\n",
       "     'args': ['ctx', 'path']},\n",
       "    {'name': 'render_template',\n",
       "     'docstring': 'Very small templating helper.\\nReplaces `{{ path.to.value }}` with a value looked up via `get_from_ctx`.\\nIf lookup fails, leaves the placeholder untouched.',\n",
       "     'args': ['text', 'ctx']},\n",
       "    {'name': 'topo_sort',\n",
       "     'docstring': 'Topologically sort nodes based on their `depends_on` list.\\nRaises GraphConfigurationError on cycles or unknown dependencies.',\n",
       "     'args': ['nodes']},\n",
       "    {'name': 'import_string',\n",
       "     'docstring': 'Import an object from a \"module:attr\" or \"module.attr\" path.\\nExample:\\n    \"myproj.schemas.Answer\" -> myproj.schemas.Answer',\n",
       "     'args': ['path']},\n",
       "    {'name': 'normalize_and_validate_graph_inputs',\n",
       "     'docstring': 'Enforce graph-level input spec if declared.\\n\\n- If `graph.inputs` is empty:\\n    - dict -> used as-is\\n    - anything else -> wrapped as `{\"query\": inputs}`\\n- If `graph.inputs` is non-empty:\\n    - inputs must be a dict\\n    - missing required keys -> GraphConfigurationError\\n    - extra keys -> warned and ignored\\n    - values are cast according to GraphInput.type',\n",
       "     'args': ['graph', 'inputs']}]},\n",
       "  'neurosurfer.agents.graph.schema': {'path': 'agents/graph/schema.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/schema.py',\n",
       "   'package': 'neurosurfer.agents.graph',\n",
       "   'classes': [{'name': 'NodeMode', 'docstring': None, 'methods': []},\n",
       "    {'name': 'GraphInput',\n",
       "     'docstring': 'Specification for a top-level graph input.\\n\\nNormalized form:\\n  name: str\\n  type: str     (string|integer|float|boolean|object|array, or synonyms)\\n  required: bool\\n  description: Optional[str]',\n",
       "     'methods': []},\n",
       "    {'name': 'NodePolicy',\n",
       "     'docstring': 'Per-node policy that can override some AgentConfig settings and add\\nnode-level execution constraints (e.g., timeout).\\n\\nYAML example:\\n    nodes:\\n      - id: research\\n        policy:\\n          retries: 1\\n          timeout_s: 30\\n          max_new_tokens: 180\\n          temperature: 0.2\\n          allow_input_pruning: false\\n          repair_with_llm: true\\n          strict_tool_call: true',\n",
       "     'methods': []},\n",
       "    {'name': 'GraphNode', 'docstring': None, 'methods': []},\n",
       "    {'name': 'Graph',\n",
       "     'docstring': None,\n",
       "     'methods': [{'name': 'node_map', 'docstring': None, 'args': ['self']}]},\n",
       "    {'name': 'NodeExecutionResult', 'docstring': None, 'methods': []},\n",
       "    {'name': 'GraphExecutionResult', 'docstring': None, 'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.graph.export': {'path': 'agents/graph/export.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/export.py',\n",
       "   'package': 'neurosurfer.agents.graph',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'export',\n",
       "     'docstring': 'Export node results to disk for nodes that have `export=True`.\\n\\nRules:\\n- If node.export_path is a file (has extension) -> use that exact file.\\n- If node.export_path is a directory / bare path -> write\\n    `{node_id}_{timestamp}.ext` inside it.\\n- If node.export_path is not set -> write into `export_base_dir`\\n    (or `base_dir` override) as `{node_id}_{timestamp}.ext`.\\n\\nContent:\\n- If structured_output is present -> JSON by default.\\n- Else if tool_call_output is present -> JSON by default.\\n- Else -> raw_output as Markdown by default.\\n\\nIf the final path has `.json`, will always write JSON.\\nIf `.md` / `.txt`, will write Markdown (embedding JSON if needed).',\n",
       "     'args': ['graph_results', 'export_base_dir']},\n",
       "    {'name': 'export_single_node',\n",
       "     'docstring': None,\n",
       "     'args': ['node', 'result', 'base_dir']},\n",
       "    {'name': 'resolve_export_path',\n",
       "     'docstring': 'Decide where to write the file for a node.',\n",
       "     'args': ['node', 'started_at', 'base_dir', 'default_ext']},\n",
       "    {'name': 'build_json_payload',\n",
       "     'docstring': 'Build a JSON-friendly representation of the node result.',\n",
       "     'args': ['node', 'result', 'content_kind']},\n",
       "    {'name': 'build_markdown_payload',\n",
       "     'docstring': 'Build a Markdown representation of the node result.',\n",
       "     'args': ['node', 'result', 'content_kind']}]},\n",
       "  'neurosurfer.agents.graph.errors': {'path': 'agents/graph/errors.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/errors.py',\n",
       "   'package': 'neurosurfer.agents.graph',\n",
       "   'classes': [{'name': 'GraphError',\n",
       "     'docstring': 'Base exception for graph-related issues.',\n",
       "     'methods': []},\n",
       "    {'name': 'GraphConfigurationError',\n",
       "     'docstring': 'Invalid graph spec, missing tools, cycles, etc.',\n",
       "     'methods': []},\n",
       "    {'name': 'GraphExecutionError',\n",
       "     'docstring': 'Errors that occur during graph execution.',\n",
       "     'methods': []},\n",
       "    {'name': 'NodeExecutionError',\n",
       "     'docstring': 'A single node failed in an unexpected way.',\n",
       "     'methods': []},\n",
       "    {'name': 'ValidationError', 'docstring': None, 'methods': []},\n",
       "    {'name': 'PlanningError', 'docstring': None, 'methods': []},\n",
       "    {'name': 'NodeError', 'docstring': None, 'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.graph.executor': {'path': 'agents/graph/executor.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/executor.py',\n",
       "   'package': 'neurosurfer.agents.graph',\n",
       "   'classes': [{'name': 'GraphExecutor',\n",
       "     'docstring': 'Execute a `Graph` DAG using:\\n  - A single shared `llm` (BaseChatModel) for all nodes by default.\\n  - A single shared `Toolkit` (all tools), with per-node subsets based on YAML.\\n  - A `ManagerAgent` (by default using the same `llm`) to compose inter-node prompts.\\n\\nUsers only need:\\n  - YAML flow (Graph)\\n  - `llm` instance\\n  - `toolkit` instance\\n\\nFeatures:\\n  - Per-node policy (NodePolicy) allowing AgentConfig-like overrides:\\n      * budget: max_new_tokens, temperature, return_stream_by_default\\n      * retries: override AgentConfig.retry.max_route_retries\\n      * timeout_s: soft node-level timeout flag\\n      * toggles: allow_input_pruning, repair_with_llm, strict_tool_call, etc.\\n  - Top-level graph inputs (`Graph.inputs`) that:\\n      * validate runtime `inputs`\\n      * cast values to expected types\\n      * can be interpolated into node prompts via `{input_name}`.',\n",
       "     'methods': [{'name': 'run',\n",
       "       'docstring': 'Execute the entire graph once.\\n\\nParameters\\n----------\\ninputs:\\n    Runtime inputs to the graph.\\n\\n    If the graph declares `inputs` in YAML:\\n      - Must be a mapping (dict)\\n      - Validated and cast according to GraphInput\\n      - Extra keys are warned and ignored\\n\\n    If the graph does NOT declare `inputs`:\\n      - If `inputs` is a dict, it\\'s used as-is\\n      - Otherwise, it\\'s wrapped as: {\"query\": inputs}\\nmanager_temperature:\\n    Temperature used for ManagerAgent when composing prompts.\\nmanager_max_new_tokens:\\n    Max new tokens for ManagerAgent responses.\\n\\nReturns\\n-------\\nGraphExecutionResult\\n    Contains the graph spec, all node results, and the final outputs.',\n",
       "       'args': ['self',\n",
       "        'inputs',\n",
       "        'manager_temperature',\n",
       "        'manager_max_new_tokens']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.graph.templates': {'path': 'agents/graph/templates.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/templates.py',\n",
       "   'package': 'neurosurfer.agents.graph',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.graph.agent': {'path': 'agents/graph/agent.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/agent.py',\n",
       "   'package': 'neurosurfer.agents.graph',\n",
       "   'classes': [{'name': 'GraphAgent',\n",
       "     'docstring': 'High-level agent wrapper around `GraphExecutor`.\\n\\nUsers can think of this as:\\n  - \"Use a normal Agent for single-step reasoning\"\\n  - \"Use GraphAgent when you want a DAG/flow of Agents\"\\n\\nTypical usage\\n-------------\\n    agent = GraphAgent(\\n        graph_yaml=\"blog_workflow.yml\",\\n        llm=LLM,\\n        toolkit=toolkit,\\n    )\\n\\n    result = agent.run(\\n        inputs={\\n            \"topic_title\": \"...\",\\n            \"query\": \"...\",\\n            \"audience\": \"...\",\\n            \"tone\": \"...\",\\n        }\\n    )',\n",
       "     'methods': [{'name': 'run',\n",
       "       'docstring': 'Execute the graph once with the given inputs.\\n\\nParameters\\n----------\\ninputs:\\n    Runtime inputs to the graph (dict or scalar). Delegated to\\n    `normalize_and_validate_graph_inputs` inside GraphExecutor.\\nmanager_temperature:\\n    Optional override for ManagerAgent temperature.\\nmanager_max_new_tokens:\\n    Optional override for ManagerAgent max_new_tokens.\\n\\nReturns\\n-------\\nGraphExecutionResult\\n    Contains the graph spec, all node results, and the final outputs.',\n",
       "       'args': ['self',\n",
       "        'inputs',\n",
       "        'manager_temperature',\n",
       "        'manager_max_new_tokens']},\n",
       "      {'name': 'get_artifact',\n",
       "       'docstring': \"Shortcut to fetch a node's raw artifact from the ArtifactStore.\",\n",
       "       'args': ['self', 'node_id']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.graph.model_pool': {'path': 'agents/graph/model_pool.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/model_pool.py',\n",
       "   'package': 'neurosurfer.agents.graph',\n",
       "   'classes': [{'name': 'ModelProfile', 'docstring': None, 'methods': []},\n",
       "    {'name': 'ModelPool',\n",
       "     'docstring': 'Token/slot-based concurrency control.\\n- Each model has a semaphore (slots).\\n- On OOM, we record timestamp and shrink allowed tokens for a while.',\n",
       "     'methods': [{'name': 'from_llms',\n",
       "       'docstring': None,\n",
       "       'args': ['cls', 'llms', 'default_concurrency']},\n",
       "      {'name': 'register_model',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'model_name', 'profile']},\n",
       "      {'name': 'bucket', 'docstring': None, 'args': ['self', 'model_name']},\n",
       "      {'name': 'release', 'docstring': None, 'args': ['self', 'model_name']},\n",
       "      {'name': 'notify_oom',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'model_name']},\n",
       "      {'name': 'recommend_max_new_tokens',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'model_name', 'requested']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.graph.__init__': {'path': 'agents/graph/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/__init__.py',\n",
       "   'package': 'neurosurfer.agents.graph',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.graph.artifacts': {'path': 'agents/graph/artifacts.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/graph/artifacts.py',\n",
       "   'package': 'neurosurfer.agents.graph',\n",
       "   'classes': [{'name': 'ArtifactStore',\n",
       "     'docstring': 'Minimal artifact store for graph runs.\\n\\nCurrently in-memory dict keyed by string IDs.\\nLater you can plug in disk / DB / S3 implementations.',\n",
       "     'methods': [{'name': 'put',\n",
       "       'docstring': None,\n",
       "       'args': ['self', 'key', 'value']},\n",
       "      {'name': 'get', 'docstring': None, 'args': ['self', 'key']},\n",
       "      {'name': 'as_dict', 'docstring': None, 'args': ['self']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.agent.responses': {'path': 'agents/agent/responses.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/agent/responses.py',\n",
       "   'package': 'neurosurfer.agents.agent',\n",
       "   'classes': [{'name': 'ToolCallResponse', 'docstring': None, 'methods': []},\n",
       "    {'name': 'StructuredResponse', 'docstring': None, 'methods': []},\n",
       "    {'name': 'AgentResponse', 'docstring': None, 'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.agent.config': {'path': 'agents/agent/config.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/agent/config.py',\n",
       "   'package': 'neurosurfer.agents.agent',\n",
       "   'classes': [{'name': 'RouterRetryPolicy',\n",
       "     'docstring': 'Retry tuning for routing + tool execution.',\n",
       "     'methods': []},\n",
       "    {'name': 'AgentConfig',\n",
       "     'docstring': 'Top-level configuration for the Agent.',\n",
       "     'methods': []}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.agent.templates': {'path': 'agents/agent/templates.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/agent/templates.py',\n",
       "   'package': 'neurosurfer.agents.agent',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.agent.agent': {'path': 'agents/agent/agent.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/agent/agent.py',\n",
       "   'package': 'neurosurfer.agents.agent',\n",
       "   'classes': [{'name': 'Agent',\n",
       "     'docstring': 'Generic LLM Agent with optional tool-calling and structured outputs.\\n\\nThis Agent is responsible for:\\n  - Plain LLM calls (free-text responses).\\n  - Structured JSON outputs validated against Pydantic models.\\n  - Tool routing and execution when a `Toolkit` is provided.\\n  - JSON prompting / parsing / repair (keeps BaseChatModel slim).\\n  - Optional tracing of key steps in the agentic flow via a pluggable `Tracer`.\\n\\nParameters\\n----------\\nllm:\\n    Concrete chat model implementing `BaseChatModel`. The agent calls this\\n    for both routing and final answers.\\ntoolkit:\\n    Optional `Toolkit` to enable tool routing + execution. If provided,\\n    `run()` will attempt to route to a tool by default.\\nconfig:\\n    Agent configuration object (see `AgentConfig`), including retry and\\n    JSON repair behaviour.\\nlogger:\\n    Python logger used for diagnostic messages.\\nverbose:\\n    If True, the agent logs more detail (e.g. selected tool, errors).\\ntracer:\\n    Optional `Tracer` used to record and span steps in the agent flow.\\nlog_tracing:\\n    If True, tracer if enabled will log spans to the console per step.\\n    Only applicable if `tracer` is not None.\\n\\nTracing behaviour\\n-----------------\\nWhen tracing is enabled, spans are opened around:\\n    * `agent.run`\\n    * `agent.structured_call`\\n    * `agent.route_and_call`\\n    * `agent.route_and_call.router_llm_call`\\n    * `agent.route_and_call.tool_execute`\\n    * `agent.free_text_call`\\nTracing results are available via the `traces` attribute of the `AgentResult` returned by `run()`, `structured_call()`, `route_and_call()`, and `free_text_call()`. ',\n",
       "     'methods': [{'name': 'run',\n",
       "       'docstring': 'Run a single agent step.\\n\\nDepending on the configuration and parameters, this will:\\n  - Call the LLM directly and return a free-text answer; OR\\n  - Use structured output with a Pydantic schema; OR\\n  - Route to a tool via the toolkit and return a `ToolCallResponse`.\\n\\nParameters\\n----------\\nsystem_prompt:\\n    Optional system prompt for the LLM. If omitted, a simple default\\n    \"helpful assistant\" prompt is used.\\nuser_prompt:\\n    Main user query / instruction. If omitted, `query` is used.\\nquery:\\n    Alias for `user_prompt`, mainly for backwards compatibility.\\noutput_schema:\\n    Optional Pydantic model class. If provided and no toolkit is set,\\n    the agent will attempt a structured JSON response, validate it\\n    against this model, and return a `StructuredResponse`.\\nstream:\\n    If True, requests streaming from the underlying LLM. If None,\\n    falls back to `config.return_stream_by_default`.\\ntemperature:\\n    Sampling temperature for the LLM. If None, uses `config.temperature`.\\nmax_new_tokens:\\n    Maximum number of new tokens to generate. If None, uses\\n    `config.max_new_tokens`.\\ncontext:\\n    Additional context dictionary merged into tool inputs when\\n    executing a tool. Ignored for free-text and structured calls.\\n_route_extra_instructions:\\n    Additional routing instructions appended to the system prompt for\\n    tool selection. Mostly internal.\\nstrict_tool_call:\\n    If True, the router must select a valid tool and repair invalid\\n    inputs; free-text fallback is disabled. If None, falls back to\\n    `config.strict_tool_call`.\\n\\nReturns\\n-------\\nAgentResponse\\n    - `response`: Union[str, Generator[str, None, None], StructuredResponse, ToolCallResponse]\\n        - Free-text answer (string or streaming generator of strings),\\n        or\\n        - StructuredResponse (for Pydantic-validated outputs), or\\n        - ToolCallResponse (for tool routing mode).\\n    - `traces`: Tracing results for the run.',\n",
       "       'args': ['self',\n",
       "        'system_prompt',\n",
       "        'user_prompt',\n",
       "        'query',\n",
       "        'output_schema',\n",
       "        'stream',\n",
       "        'temperature',\n",
       "        'max_new_tokens',\n",
       "        'context',\n",
       "        '_route_extra_instructions',\n",
       "        'strict_tool_call',\n",
       "        'reset_tracer']}]}],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.agent.__init__': {'path': 'agents/agent/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/agent/__init__.py',\n",
       "   'package': 'neurosurfer.agents.agent',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.agents.agent.schema_utils': {'path': 'agents/agent/schema_utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/agents/agent/schema_utils.py',\n",
       "   'package': 'neurosurfer.agents.agent',\n",
       "   'classes': [{'name': 'StructuredPromptOptions',\n",
       "     'docstring': None,\n",
       "     'methods': []}],\n",
       "   'functions': [{'name': 'model_to_structure_block',\n",
       "     'docstring': 'Pretty, width-aware schema shape:\\n\\nCar: {\\n  make: str\\n  model: str\\n  features: [\\n    {\\n      name: str\\n      description: str\\n      location: { name: str, description: str }  // wrapped if short; multiline if long\\n    }\\n  ]  // add as many as needed\\n}',\n",
       "     'args': ['schema_cls',\n",
       "      'title',\n",
       "      'comment_for_arrays',\n",
       "      'max_inline_chars',\n",
       "      'indent',\n",
       "      'array_comment',\n",
       "      'object_comment',\n",
       "      'trail_commas']},\n",
       "    {'name': 'build_structured_system_prompt',\n",
       "     'docstring': None,\n",
       "     'args': ['base_system_prompt',\n",
       "      'schema_cls',\n",
       "      'options',\n",
       "      'use_model_json_schema']},\n",
       "    {'name': 'maybe_unwrap_named_root',\n",
       "     'docstring': 'If model returned {\"Car\": {...}} for schema `Car`, unwrap to {...}.\\nOtherwise return the original string.',\n",
       "     'args': ['json_obj', 'schema_cls']}]},\n",
       "  'neurosurfer.cli.processes': {'path': 'cli/processes.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/cli/processes.py',\n",
       "   'package': 'neurosurfer.cli',\n",
       "   'classes': [],\n",
       "   'functions': []},\n",
       "  'neurosurfer.cli.serve': {'path': 'cli/serve.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/cli/serve.py',\n",
       "   'package': 'neurosurfer.cli',\n",
       "   'classes': [{'name': 'ServeOptions', 'docstring': None, 'methods': []}],\n",
       "   'functions': [{'name': 'needs_npm_install',\n",
       "     'docstring': None,\n",
       "     'args': ['ui_root', 'mode']}]},\n",
       "  'neurosurfer.cli.utils': {'path': 'cli/utils.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/cli/utils.py',\n",
       "   'package': 'neurosurfer.cli',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'eprint', 'docstring': None, 'args': ['*args']},\n",
       "    {'name': 'which',\n",
       "     'docstring': \"Robust 'which': return resolved path if the command is in PATH.\",\n",
       "     'args': ['cmd', 'verbose']},\n",
       "    {'name': 'env_truthy', 'docstring': None, 'args': ['name', 'default']},\n",
       "    {'name': 'find_packaged_ui_dir',\n",
       "     'docstring': 'Returns neurosurfer/ui_build if bundled and contains index.html.',\n",
       "     'args': []},\n",
       "    {'name': 'has_package_json', 'docstring': None, 'args': ['path']},\n",
       "    {'name': 'looks_like_build_dir', 'docstring': None, 'args': ['path']},\n",
       "    {'name': 'detect_ui_root', 'docstring': None, 'args': ['arg']},\n",
       "    {'name': 'effective_public_host', 'docstring': None, 'args': ['host']},\n",
       "    {'name': 'open_browser_safe', 'docstring': None, 'args': ['url']},\n",
       "    {'name': 'print_ready_banner',\n",
       "     'docstring': None,\n",
       "     'args': ['backend_url', 'ui_url']}]},\n",
       "  'neurosurfer.cli.main': {'path': 'cli/main.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/cli/main.py',\n",
       "   'package': 'neurosurfer.cli',\n",
       "   'classes': [],\n",
       "   'functions': [{'name': 'build_parser', 'docstring': None, 'args': []},\n",
       "    {'name': 'main', 'docstring': None, 'args': ['argv']}]},\n",
       "  'neurosurfer.cli.__init__': {'path': 'cli/__init__.py',\n",
       "   'abs_path': '/home/nomi/workspace/neurosurfer/neurosurfer/cli/__init__.py',\n",
       "   'package': 'neurosurfer.cli',\n",
       "   'classes': [],\n",
       "   'functions': []}},\n",
       " 'summary': {'module_count': 140, 'class_count': 158, 'function_count': 103},\n",
       " 'errors': []}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files = scan_results.results[\"python_files\"]\n",
    "module_paths = None\n",
    "code_index_results = code_index_tool(python_files=python_files, module_paths=module_paths)\n",
    "code_index_results.results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3dfd2cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<neurosurfer.agents.graph.agent.GraphAgent object at 0x7188e4571190>\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-26 14:42:44\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: directory_scan\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmanager\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m1.\u001b[0m\u001b[2m904s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmanager\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;4;33mðŸ§  Thinking\u001b[0m\u001b[1;4;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mscan_repo\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'scan_repo'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'scan_repo'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\n",
      "        \u001b[1;32mINFO: Selected tool: directory_scan\u001b[0m\n",
      "        \u001b[1;32mINFO: Raw inputs: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'include_patterns'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m'**'\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'exclude_patterns'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'max_depth'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m10\u001b[0m\u001b[1;32m}\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'scan_repo'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m987s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'scan_repo'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-26 14:42:47\u001b[0m | \u001b[96mdir_scanning.py:__call__\u001b[0m | DirectoryScanTool scanning project_root=/home/nomi/workspace/neurosurfer/neurosurfer docs_root=/home/nomi/workspace/neurosurfer/docs max_depth=10\n",
      "        \u001b[1;32mINFO: Tool \u001b[0m\u001b[1;32m'directory_scan'\u001b[0m\u001b[1;32m Tool Return: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'project_root'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m'/home/nomi/workspace/neurosurfer/neurosurfer'\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'docs_root'\u001b[0m\u001b[1;32m: '\u001b[0m\u001b[1;32m/home/nomi/workspace/\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'scan_repo'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m004s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'scan_repo'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m993s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mscan_repo\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-26 14:42:47\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: code_symbol_index\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmanager\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m3.\u001b[0m\u001b[2m183s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmanager\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;4;33mðŸ§  Thinking\u001b[0m\u001b[1;4;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mbuild_symbol_index\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'build_symbol_index'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'build_symbol_index'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\n",
      "        \u001b[1;32mINFO: Selected tool: code_symbol_index\u001b[0m\n",
      "        \u001b[1;32mINFO: Raw inputs: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'module_paths'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m'neurosurfer.agents.*'\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'neurosurfer.models.*'\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'neurosurfer.rag.*'\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'neurosurfer.server.*'\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'neurosurfer.cli.*'\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'neurosurfer.tools.*'\u001b[0m\u001b[1;32m]\u001b[0m\u001b[1;32m}\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'build_symbol_index'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m2.\u001b[0m\u001b[2m571s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'build_symbol_index'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-26 14:42:53\u001b[0m | \u001b[96mcode_symbol_index.py:__call__\u001b[0m | CodeSymbolIndexTool requires either `python_files` argument or `project_root` configured in CodeSymbolIndexConfig.\n",
      "        \u001b[1;32mINFO: Tool \u001b[0m\u001b[1;32m'code_symbol_index'\u001b[0m\u001b[1;32m Tool Return: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'modules'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m}\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'summary'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'module_count'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m0\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'class_count'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m0\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'function_count'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m0\u001b[0m\u001b[1;32m}\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'errors'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m[\u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'build_symbol_index'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m001s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'build_symbol_index'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m2.\u001b[0m\u001b[2m573s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mbuild_symbol_index\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing End!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmanager\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m2.\u001b[0m\u001b[2m526s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmanager\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;4;33mðŸ§  Thinking\u001b[0m\u001b[1;4;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mplan_docs\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'plan_docs'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     â–¶ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'plan_docs'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\n",
      "\u001b[2m     â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'plan_docs'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m58.\u001b[0m\u001b[2m105s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m â—€ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'plan_docs'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m58.\u001b[0m\u001b[2m106s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mplan_docs\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;4;32mFinal response:\u001b[0m\n",
      "```json\n",
      "{\n",
      "  \"DocPlan\": {\n",
      "    \"index.md\": {\n",
      "      \"title\": \"Neurosurfer Documentation\",\n",
      "      \"path\": \"/index.md\"\n",
      "    },\n",
      "    \"contributing.md\": {\n",
      "      \"title\": \"Contributing to Neurosurfer\",\n",
      "      \"path\": \"/contributing.md\"\n",
      "    },\n",
      "    \"getting-started.md\": {\n",
      "      \"title\": \"Getting Started\",\n",
      "      \"path\": \"/getting-started.md\"\n",
      "    },\n",
      "    \"cli.md\": {\n",
      "      \"title\": \"CLI Reference\",\n",
      "      \"path\": \"/cli.md\"\n",
      "    },\n",
      "    \"examples/index.md\": {\n",
      "      \"title\": \"Examples\",\n",
      "      \"path\": \"/examples/index.md\"\n",
      "    },\n",
      "    \"examples/models-examples.md\": {\n",
      "      \"title\": \"Model Examples\",\n",
      "      \"path\": \"/examples/models-examples.md\"\n",
      "    },\n",
      "    \"examples/custom-tools-examples.md\": {\n",
      "      \"title\": \"Custom Tools Examples\",\n",
      "      \"path\": \"/examples/custom-tools-examples.md\"\n",
      "    },\n",
      "    \"examples/server-app-example.md\": {\n",
      "      \"title\": \"Server App Example\",\n",
      "      \"path\": \"/examples/server-app-example.md\"\n",
      "    },\n",
      "    \"examples/rag-examples.md\": {\n",
      "      \"title\": \"RAG Examples\",\n",
      "      \"path\": \"/examples/rag-examples.md\"\n",
      "    },\n",
      "    \"examples/agents-examples.md\": {\n",
      "      \"title\": \"Agent Examples\",\n",
      "      \"path\": \"/examples/agents-examples.md\"\n",
      "    },\n",
      "    \"server/index.md\": {\n",
      "      \"title\": \"Server\",\n",
      "      \"path\": \"/server/index.md\"\n",
      "    },\n",
      "    \"server/neurosurferui.md\": {\n",
      "      \"title\": \"Neurosurfer UI\",\n",
      "      \"path\": \"/server/neurosurferui.md\"\n",
      "    },\n",
      "    \"server/example-app.md\": {\n",
      "      \"title\": \"Example App\",\n",
      "      \"path\": \"/server/example-app.md\"\n",
      "    },\n",
      "    \"server/backend/index.md\": {\n",
      "      \"title\": \"Backend\",\n",
      "      \"path\": \"/server/backend/index.md\"\n",
      "    },\n",
      "    \"server/backend/custom-endpoints.md\": {\n",
      "      \"title\": \"Custom Endpoints\",\n",
      "      \"path\": \"/server/backend/custom-endpoints.md\"\n",
      "    },\n",
      "    \"server/backend/lifecycle-hooks.md\": {\n",
      "      \"title\": \"Lifecycle Hooks\",\n",
      "      \"path\": \"/server/backend/lifecycle-hooks.md\"\n",
      "    },\n",
      "    \"server/backend/auth.md\": {\n",
      "      \"title\": \"Authentication\",\n",
      "      \"path\": \"/server/backend/auth.md\"\n",
      "    },\n",
      "    \"server/backend/chat-handlers.md\": {\n",
      "      \"title\": \"Chat Handlers\",\n",
      "      \"path\": \"/server/backend/chat-handlers.md\"\n",
      "    },\n",
      "    \"api-reference/configuration.md\": {\n",
      "      \"title\": \"Configuration\",\n",
      "      \"path\": \"/api-reference/configuration.md\"\n",
      "    },\n",
      "    \"api-reference/index.md\": {\n",
      "      \"title\": \"API Reference\",\n",
      "      \"path\": \"/api-reference/index.md\"\n",
      "    },\n",
      "    \"api-reference/vectorstores/index.md\": {\n",
      "      \"title\": \"Vectorstores\",\n",
      "      \"path\": \"/api-reference/vectorstores/index.md\"\n",
      "    },\n",
      "    \"api-reference/vectorstores/in_memory.md\": {\n",
      "      \"title\": \"In-Memory Vectorstore\",\n",
      "      \"path\": \"/api-reference/vectorstores/in_memory.md\"\n",
      "    },\n",
      "    \"api-reference/vectorstores/base-vectordb.md\": {\n",
      "      \"title\": \"Base VectorDB\",\n",
      "      \"path\": \"/api-reference/vectorstores/base-vectordb.md\"\n",
      "    },\n",
      "    \"api-reference/vectorstores/chroma.md\": {\n",
      "      \"title\": \"Chroma Vectorstore\",\n",
      "      \"path\": \"/api-reference/vectorstores/chroma.md\"\n",
      "    },\n",
      "    \"api-reference/models/index.md\": {\n",
      "      \"title\": \"Models\",\n",
      "      \"path\": \"/api-reference/models/index.md\"\n",
      "    },\n",
      "    \"api-reference/models/embedders/index.md\": {\n",
      "      \"title\": \"Embedders\",\n",
      "      \"path\": \"/api-reference/models/embedders/index.md\"\n",
      "    },\n",
      "    \"api-reference/models/embedders/base-embedder.md\": {\n",
      "      \"title\": \"Base Embedder\",\n",
      "      \"path\": \"/api-reference/models/embedders/base-embedder.md\"\n",
      "    },\n",
      "    \"api-reference/models/embedders/sentence-transformer.md\": {\n",
      "      \"title\": \"Sentence Transformer Embedder\",\n",
      "      \"path\": \"/api-reference/models/embedders/sentence-transformer.md\"\n",
      "    },\n",
      "    \"api-reference/models/embedders/llamacpp-embedder.md\": {\n",
      "      \"title\": \"LLaMACPP Embedder\",\n",
      "      \"path\": \"/api-reference/models/embedders/llamacpp-embedder.md\"\n",
      "    },\n",
      "    \"api-reference/models/chat-models/index.md\": {\n",
      "      \"title\": \"Chat Models\",\n",
      "      \"path\": \"/api-reference/models/chat-models/index.md\"\n",
      "    },\n",
      "    \"api-reference/models/chat-models/base-model.md\": {\n",
      "      \"title\": \"Base Model\",\n",
      "      \"path\": \"/api-reference/models/chat-models/base-model.md\"\n",
      "    },\n",
      "    \"api-reference/models/chat-models/llamacpp-model.md\": {\n",
      "      \"title\": \"LLaMACPP Model\",\n",
      "      \"path\": \"/api-reference/models/chat-models/llamacpp-model.md\"\n",
      "    },\n",
      "    \"api-reference/models/chat-models/openai-model.md\": {\n",
      "      \"title\": \"OpenAI Model\",\n",
      "      \"path\": \"/api-reference/models/chat-models/openai-model.md\"\n",
      "    },\n",
      "    \"api-reference/models/chat-models/transformers-model.md\": {\n",
      "      \"title\": \"Transformers Model\",\n",
      "      \"path\": \"/api-reference/models/chat-models/transformers-model.md\"\n",
      "    },\n",
      "    \"api-reference/models/chat-models/unsloth-model.md\": {\n",
      "      \"title\": \"Unsloth Model\",\n",
      "      \"path\": \"/api-reference/models/chat-models/unsloth-model.md\"\n",
      "    },\n",
      "    \"api-reference/tools/index.md\": {\n",
      "      \"title\": \"Tools\",\n",
      "      \"path\": \"/api-reference/tools/index.md\"\n",
      "    },\n",
      "    \"api-reference/tools/tool-spec.md\": {\n",
      "      \"title\": \"Tool Specification\",\n",
      "      \"path\": \"/api-reference/tools/tool-spec.md\"\n",
      "    },\n",
      "    \"api-reference/tools/base-tool.md\": {\n",
      "      \"title\": \"Base Tool\",\n",
      "      \"path\": \"/api-reference/tools/base-tool.md\"\n",
      "    },\n",
      "    \"api-reference/tools/toolkit.md\": {\n",
      "      \"title\": \"Toolkit\",\n",
      "      \"path\": \"/api-reference/tools/toolkit.md\"\n",
      "    },\n",
      "    \"api-reference/tools/builtin-tools/index.md\": {\n",
      "      \"title\": \"Builtin Tools\",\n",
      "      \"path\": \"/api-reference/tools/builtin-tools/index.md\"\n",
      "    },\n",
      "    \"api-reference/tools/builtin-tools/sql/index.md\": {\n",
      "      \"title\": \"SQL Tools\",\n",
      "      \"path\": \"/api-reference/tools/builtin-tools/sql/index.md\"\n",
      "    },\n",
      "    \"api-reference/tools/builtin-tools/sql/sql_executor.md\": {\n",
      "      \"title\": \"SQL Executor\",\n",
      "      \"path\": \"/api-reference/tools/builtin-tools/sql/sql_executor.md\"\n",
      "    },\n",
      "    \"api-reference/tools/builtin-tools/sql/relevant_table_schema_retriever.md\": {\n",
      "      \"title\": \"Relevant Table Schema Retriever\",\n",
      "      \"path\": \"/api-reference/tools/builtin-tools/sql/relevant_table_schema_retriever.md\"\n",
      "    },\n",
      "    \"api-reference/tools/builtin-tools/sql/sql_query_generator.md\": {\n",
      "      \"title\": \"SQL Query Generator\",\n",
      "      \"path\": \"/api-reference/tools/builtin-tools/sql/sql_query_generator.md\"\n",
      "    },\n",
      "    \"api-reference/tools/builtin-tools/sql/final_answer_formatter.md\": {\n",
      "      \"title\": \"Final Answer Formatter\",\n",
      "      \"path\": \"/api-reference/tools/builtin-tools/sql/final_answer_formatter.md\"\n",
      "    },\n",
      "    \"api-reference/tools/builtin-tools/sql/db_insights_tool.md\": {\n",
      "      \"title\": \"DB Insights Tool\",\n",
      "      \"path\": \"/api-reference/tools/builtin-tools/sql/db_insights_tool.md\"\n",
      "    },\n",
      "    \"api-reference/rag/index.md\": {\n",
      "      \"title\": \"RAG\",\n",
      "      \"path\": \"/api-reference/rag/index.md\"\n",
      "    },\n",
      "    \"api-reference/rag/ingestor.md\": {\n",
      "      \"title\": \"Ingestor\",\n",
      "      \"path\": \"/api-reference/rag/ingestor.md\"\n",
      "    },\n",
      "    \"api-reference/rag/filereader.md\": {\n",
      "      \"title\": \"File Reader\",\n",
      "      \"path\": \"/api-reference/rag/filereader.md\"\n",
      "    },\n",
      "    \"api-reference/rag/chunker.md\": {\n",
      "      \"title\": \"Chunker\",\n",
      "      \"path\": \"/api-reference/rag/chunker.md\"\n",
      "    },\n",
      "    \"api-reference/agents/index.md\": {\n",
      "      \"title\": \"Agents\",\n",
      "      \"path\": \"/api-reference/agents/index.md\"\n",
      "    },\n",
      "    \"api-reference/agents/rag-agent.md\": {\n",
      "      \"title\": \"RAG Agent\",\n",
      "      \"path\": \"/api-reference/agents/rag-agent.md\"\n",
      "    },\n",
      "    \"api-reference/agents/react-agent.md\": {\n",
      "      \"title\": \"React Agent\",\n",
      "      \"path\": \"/api-reference/agents/react-agent.md\"\n",
      "    },\n",
      "    \"api-reference/agents/sql-agent.md\": {\n",
      "      \"title\": \"SQL Agent\",\n",
      "      \"path\": \"/api-reference/agents/sql-agent.md\"\n",
      "    },\n",
      "    \"api-reference/agents/tools_router_agent.md\": {\n",
      "      \"title\": \"Tools Router Agent\",\n",
      "      \"path\": \"/api-reference/agents/tools_router_agent.md\"\n",
      "    },\n",
      "    \"api-reference/database/index.md\": {\n",
      "      \"title\": \"Database\",\n",
      "      \"path\": \"/api-reference/database/index.md\"\n",
      "    },\n",
      "    \"api-reference/database/sql_schema_store.md\": {\n",
      "      \"title\": \"SQL Schema Store\",\n",
      "      \"path\": \"/api-reference/database/sql_schema_store.md\"\n",
      "    },\n",
      "    \"api-reference/database/sql_database.md\": {\n",
      "      \"title\": \"SQL Database\",\n",
      "      \"path\": \"/api-reference/database/sql_database.md\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-26 14:43:53\u001b[0m | \u001b[96mexport.py:export_single_node\u001b[0m | Exported node scan_repo output to exports/scan_repo_20251126_144246.json\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-26 14:43:53\u001b[0m | \u001b[96mexport.py:export_single_node\u001b[0m | Exported node build_symbol_index output to exports/build_symbol_index_20251126_144250.json\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-26 14:43:53\u001b[0m | \u001b[96mexport.py:export_single_node\u001b[0m | Exported node plan_docs output to exports/plan_docs_20251126_144255.md\n"
     ]
    }
   ],
   "source": [
    "from neurosurfer.models.chat_models.base import BaseChatModel\n",
    "from neurosurfer.agents.graph import GraphAgent, ManagerConfig\n",
    "\n",
    "graph_agent = GraphAgent(\n",
    "    llm=LLM,\n",
    "    graph_yaml=\"docgen_workflow.yml\",\n",
    "    toolkit=toolkit,\n",
    "    manager_config=ManagerConfig(\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=4096,\n",
    "    ),\n",
    "    manager_llm=LLM,\n",
    "    log_traces=True\n",
    ")\n",
    "\n",
    "# Run workflow\n",
    "graph_inputs = {\n",
    "    \"project_root\": \"/home/nomi/workspace/neurosurfer/neurosurfer\",\n",
    "    \"docs_root\": \"/home/nomi/workspace/neurosurfer/docs\",\n",
    "    \"include_patterns\": [\"neurosurfer.agents.*\"],\n",
    "    \"exclude_patterns\": [],\n",
    "    \"sections\": [],\n",
    "    \"doc_template\": \"\",\n",
    "    \"mode\": \"structured\",\n",
    "}\n",
    "print(graph_agent)\n",
    "results = graph_agent.run(inputs=graph_inputs)\n",
    "# result = await run_async(executor.run(inputs=graph_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE iterating generator ===\n",
      "[]\n",
      "\n",
      "=== DURING streaming ===\n",
      "yielded: token_0\n",
      "current traces: ['TRACE: token_0']\n",
      "yielded: token_1\n",
      "current traces: ['TRACE: token_0', 'TRACE: token_1']\n",
      "yielded: token_2\n",
      "current traces: ['TRACE: token_0', 'TRACE: token_1', 'TRACE: token_2']\n",
      "\n",
      "=== AFTER streaming ===\n",
      "['TRACE: token_0', 'TRACE: token_1', 'TRACE: token_2']\n"
     ]
    }
   ],
   "source": [
    "# from dataclasses import dataclass, field\n",
    "# from typing import Generator, Optional, Union, List\n",
    "\n",
    "# @dataclass\n",
    "# class TraceResult:\n",
    "#     steps: List[str] = field(default_factory=list)\n",
    "#     def add(self, msg: str):\n",
    "#         self.steps.append(msg)\n",
    "\n",
    "# class DummyTracer:\n",
    "#     def __init__(self):\n",
    "#         self.results = TraceResult()\n",
    "#     def trace(self, msg: str):\n",
    "#         self.results.add(msg)\n",
    "\n",
    "# @dataclass\n",
    "# class AgentResponse:\n",
    "#     response: Union[str, Generator[str, None, None]]\n",
    "#     traces: Optional[TraceResult] = None\n",
    "\n",
    "# class DummyAgent:\n",
    "#     def __init__(self):\n",
    "#         self.tracer = DummyTracer()\n",
    "\n",
    "#     def run(self) -> AgentResponse:\n",
    "#         def generator():\n",
    "#             for i in range(3):\n",
    "#                 msg = f\"token_{i}\"\n",
    "#                 self.tracer.trace(f\"TRACE: {msg}\")   # tracing inside generator\n",
    "#                 yield msg\n",
    "#         return AgentResponse(\n",
    "#             response=generator(),\n",
    "#             traces=self.tracer.results  # shared reference\n",
    "#         )\n",
    "\n",
    "# agent = DummyAgent()\n",
    "# res = agent.run()\n",
    "\n",
    "# print(\"=== BEFORE iterating generator ===\")\n",
    "# print(res.traces.steps)   # should be empty\n",
    "\n",
    "# print(\"\\n=== DURING streaming ===\")\n",
    "# for tok in res.response:\n",
    "#     print(\"yielded:\", tok)\n",
    "#     print(\"current traces:\", res.traces.steps)\n",
    "\n",
    "# print(\"\\n=== AFTER streaming ===\")\n",
    "# print(res.traces.steps)   # should have all traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5bb4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# print(results.final['plan_docs'].replace('```json', '').replace('```', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a52404d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': [{'step_id': 1,\n",
       "   'kind': 'agent',\n",
       "   'label': 'agent.run',\n",
       "   'node_id': None,\n",
       "   'agent_id': 'scan_repo',\n",
       "   'started_at': 1764051228.8918066,\n",
       "   'duration_ms': 840,\n",
       "   'inputs': {'agent_type': 'Agent',\n",
       "    'has_toolkit': True,\n",
       "    'structured': False,\n",
       "    'stream': False,\n",
       "    'strict_tool_call': False},\n",
       "   'outputs': {},\n",
       "   'meta': {},\n",
       "   'ok': True,\n",
       "   'error': None,\n",
       "   'logs': []},\n",
       "  {'step_id': 2,\n",
       "   'kind': 'llm.call',\n",
       "   'label': 'agent.route_and_call.router_llm_call',\n",
       "   'node_id': None,\n",
       "   'agent_id': 'scan_repo',\n",
       "   'started_at': 1764051228.8921106,\n",
       "   'duration_ms': 834,\n",
       "   'inputs': {'attempt': 1,\n",
       "    'strict_tool_call': False,\n",
       "    'system_prompt_len': 1959,\n",
       "    'user_prompt_len': 342,\n",
       "    'user_prompt': 'Scan the project_root and docs_root to build a structured index of packages, modules, and existing markdown docs. Include all Python files and markdown documents, and organize them into a compact JSON object with the following keys: \"packages\", \"modules\", \"python_files\", and \"doc_files\". Use the directory_scan tool to assist with this task.',\n",
       "    'system_prompt': 'You are a stateless tool router. \\nYou may either call a tool or respond directly with a natural language message â€” whichever best fits the user query.\\n\\nIf you decide to call a tool, respond **only** with one-line valid JSON in the exact format below:\\n{\"tool\": \"<tool_name>\", \"inputs\": {<param>: <value>}}\\n\\nIf you decide to respond directly, emit your message as a plain string (not JSON).\\n\\nRules:\\n- Choose at most ONE tool per request.\\n- Use only explicit parameters defined by that tool. Do NOT invent or rename parameters.\\n- Include only required parameters unless an optional one is clearly implied.\\n- If no tool fits the request or inputs are ambiguous, output:\\n  {\"tool\": \"none\", \"inputs\": {}}\\n- Otherwise, respond in plain text when a natural language answer is more suitable.\\n\\nTOOLS CATALOG:\\nAvailable tools:\\nTool Name: `directory_scan`\\nDescription: Scan a project directory (and optional docs directory) to build a structured index of Python modules, packages, and documentation files. Useful as a first step for automated documentation generation.\\nWhen to use: Use this tool when you need an overview of the codebase and docs structure: which packages, modules, Python files, and markdown files are present, along with basic counts.\\nTool Inputs:\\n- `include_patterns`: array (optional) â€” Optional list of glob patterns to include, e.g. [\\'neurosurfer/models/**\\'].\\n- `exclude_patterns`: array (optional) â€” Optional list of glob patterns to exclude, e.g. [\\'neurosurfer/tests/**\\'].\\n- `max_depth`: integer (optional) â€” Optional maximum depth to scan. \\nTool Return: object â€” JSON object describing the project structure, with keys like:\\n- project_root: normalized absolute path\\n- docs_root: normalized absolute path (if provided & exists)\\n- python_files: list of objects {path, abs_path, module, package}\\n- doc_files: list of objects {path, abs_path}\\n- packages: list of dotted package names\\n- modules: list of dotted module names\\n- summary: basic counts\\n\\n\\n',\n",
       "    'temperature': 0.7,\n",
       "    'max_new_tokens': 512,\n",
       "    'stream': False},\n",
       "   'outputs': {'model_response': '{\"tool\": \"directory_scan\", \"inputs\": {\"include_patterns\": [\"**\"], \"exclude_patterns\": []}}',\n",
       "    'model_response_len': 90},\n",
       "   'meta': {},\n",
       "   'ok': True,\n",
       "   'error': None,\n",
       "   'logs': [{'ts': 1764051229.726261,\n",
       "     'message': 'Selected tool: directory_scan',\n",
       "     'data': {},\n",
       "     'type': 'info'},\n",
       "    {'ts': 1764051229.7267768,\n",
       "     'message': \"Raw inputs: {'include_patterns': ['**'], 'exclude_patterns': []}\",\n",
       "     'data': {},\n",
       "     'type': 'info'}]},\n",
       "  {'step_id': 3,\n",
       "   'kind': 'tool.execute',\n",
       "   'label': 'agent.route_and_call.tool_execute',\n",
       "   'node_id': None,\n",
       "   'agent_id': 'scan_repo',\n",
       "   'started_at': 1764051229.7274964,\n",
       "   'duration_ms': 4,\n",
       "   'inputs': {'tool_name': 'directory_scan',\n",
       "    'payload': {'include_patterns': ['neurosurfer.agents.*'],\n",
       "     'exclude_patterns': [],\n",
       "     'dependencies': {},\n",
       "     'project_root': '/home/nomi/workspace/neurosurfer/neurosurfer',\n",
       "     'docs_root': '/home/nomi/workspace/neurosurfer/docs',\n",
       "     'sections': [],\n",
       "     'doc_template': '',\n",
       "     'mode': 'structured'}},\n",
       "   'outputs': {'tool_return': {'project_root': '/home/nomi/workspace/neurosurfer/neurosurfer',\n",
       "     'docs_root': '/home/nomi/workspace/neurosurfer/docs',\n",
       "     'python_files': [],\n",
       "     'doc_files': [{'path': 'index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/index.md'},\n",
       "      {'path': 'contributing.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/contributing.md'},\n",
       "      {'path': 'cli.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/cli.md'},\n",
       "      {'path': 'getting-started.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/getting-started.md'},\n",
       "      {'path': 'examples/models-examples.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/examples/models-examples.md'},\n",
       "      {'path': 'examples/custom-tools-examples.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/examples/custom-tools-examples.md'},\n",
       "      {'path': 'examples/server-app-example.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/examples/server-app-example.md'},\n",
       "      {'path': 'examples/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/examples/index.md'},\n",
       "      {'path': 'examples/rag-examples.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/examples/rag-examples.md'},\n",
       "      {'path': 'examples/agents-examples.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/examples/agents-examples.md'},\n",
       "      {'path': 'server/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/server/index.md'},\n",
       "      {'path': 'server/neurosurferui.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/server/neurosurferui.md'},\n",
       "      {'path': 'server/example-app.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/server/example-app.md'},\n",
       "      {'path': 'server/backend/custom-endpoints.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/server/backend/custom-endpoints.md'},\n",
       "      {'path': 'server/backend/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/server/backend/index.md'},\n",
       "      {'path': 'server/backend/lifecycle-hooks.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/server/backend/lifecycle-hooks.md'},\n",
       "      {'path': 'server/backend/auth.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/server/backend/auth.md'},\n",
       "      {'path': 'server/backend/chat-handlers.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/server/backend/chat-handlers.md'},\n",
       "      {'path': 'api-reference/configuration.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/configuration.md'},\n",
       "      {'path': 'api-reference/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/index.md'},\n",
       "      {'path': 'api-reference/vectorstores/in_memory.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/vectorstores/in_memory.md'},\n",
       "      {'path': 'api-reference/vectorstores/base-vectordb.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/vectorstores/base-vectordb.md'},\n",
       "      {'path': 'api-reference/vectorstores/chroma.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/vectorstores/chroma.md'},\n",
       "      {'path': 'api-reference/vectorstores/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/vectorstores/index.md'},\n",
       "      {'path': 'api-reference/models/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/models/index.md'},\n",
       "      {'path': 'api-reference/models/embedders/sentence-transformer.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/models/embedders/sentence-transformer.md'},\n",
       "      {'path': 'api-reference/models/embedders/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/models/embedders/index.md'},\n",
       "      {'path': 'api-reference/models/embedders/base-embedder.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/models/embedders/base-embedder.md'},\n",
       "      {'path': 'api-reference/models/embedders/llamacpp-embedder.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/models/embedders/llamacpp-embedder.md'},\n",
       "      {'path': 'api-reference/models/chat-models/base-model.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/models/chat-models/base-model.md'},\n",
       "      {'path': 'api-reference/models/chat-models/llamacpp-model.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/models/chat-models/llamacpp-model.md'},\n",
       "      {'path': 'api-reference/models/chat-models/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/models/chat-models/index.md'},\n",
       "      {'path': 'api-reference/models/chat-models/openai-model.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/models/chat-models/openai-model.md'},\n",
       "      {'path': 'api-reference/models/chat-models/transformers-model.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/models/chat-models/transformers-model.md'},\n",
       "      {'path': 'api-reference/models/chat-models/unsloth-model.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/models/chat-models/unsloth-model.md'},\n",
       "      {'path': 'api-reference/tools/tool-spec.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/tools/tool-spec.md'},\n",
       "      {'path': 'api-reference/tools/base-tool.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/tools/base-tool.md'},\n",
       "      {'path': 'api-reference/tools/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/tools/index.md'},\n",
       "      {'path': 'api-reference/tools/toolkit.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/tools/toolkit.md'},\n",
       "      {'path': 'api-reference/tools/builtin-tools/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/tools/builtin-tools/index.md'},\n",
       "      {'path': 'api-reference/tools/builtin-tools/sql/db_insights_tool.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/tools/builtin-tools/sql/db_insights_tool.md'},\n",
       "      {'path': 'api-reference/tools/builtin-tools/sql/sql_executor.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/tools/builtin-tools/sql/sql_executor.md'},\n",
       "      {'path': 'api-reference/tools/builtin-tools/sql/relevant_table_schema_retriever.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/tools/builtin-tools/sql/relevant_table_schema_retriever.md'},\n",
       "      {'path': 'api-reference/tools/builtin-tools/sql/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/tools/builtin-tools/sql/index.md'},\n",
       "      {'path': 'api-reference/tools/builtin-tools/sql/final_answer_formatter.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/tools/builtin-tools/sql/final_answer_formatter.md'},\n",
       "      {'path': 'api-reference/tools/builtin-tools/sql/sql_query_generator.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/tools/builtin-tools/sql/sql_query_generator.md'},\n",
       "      {'path': 'api-reference/rag/ingestor.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/rag/ingestor.md'},\n",
       "      {'path': 'api-reference/rag/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/rag/index.md'},\n",
       "      {'path': 'api-reference/rag/filereader.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/rag/filereader.md'},\n",
       "      {'path': 'api-reference/rag/chunker.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/rag/chunker.md'},\n",
       "      {'path': 'api-reference/agents/rag-agent.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/agents/rag-agent.md'},\n",
       "      {'path': 'api-reference/agents/react-agent.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/agents/react-agent.md'},\n",
       "      {'path': 'api-reference/agents/sql-agent.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/agents/sql-agent.md'},\n",
       "      {'path': 'api-reference/agents/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/agents/index.md'},\n",
       "      {'path': 'api-reference/agents/tools_router_agent.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/agents/tools_router_agent.md'},\n",
       "      {'path': 'api-reference/database/sql_schema_store.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/database/sql_schema_store.md'},\n",
       "      {'path': 'api-reference/database/sql_database.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/database/sql_database.md'},\n",
       "      {'path': 'api-reference/database/index.md',\n",
       "       'abs_path': '/home/nomi/workspace/neurosurfer/docs/api-reference/database/index.md'}],\n",
       "     'packages': ['neurosurfer',\n",
       "      'neurosurfer.agents',\n",
       "      'neurosurfer.agents.agent',\n",
       "      'neurosurfer.agents.graph',\n",
       "      'neurosurfer.agents.rag',\n",
       "      'neurosurfer.agents.react',\n",
       "      'neurosurfer.cli',\n",
       "      'neurosurfer.db',\n",
       "      'neurosurfer.models.chat_models',\n",
       "      'neurosurfer.models.embedders',\n",
       "      'neurosurfer.server',\n",
       "      'neurosurfer.server.api',\n",
       "      'neurosurfer.server.db',\n",
       "      'neurosurfer.server.schemas',\n",
       "      'neurosurfer.server.services.rag',\n",
       "      'neurosurfer.tools',\n",
       "      'neurosurfer.tools.common',\n",
       "      'neurosurfer.tools.sql',\n",
       "      'neurosurfer.tools.websearch',\n",
       "      'neurosurfer.tools.websearch.engines',\n",
       "      'neurosurfer.tracing',\n",
       "      'neurosurfer.utils',\n",
       "      'neurosurfer.vectorstores'],\n",
       "     'modules': [],\n",
       "     'summary': {'python_file_count': 0,\n",
       "      'doc_file_count': 58,\n",
       "      'package_count': 23,\n",
       "      'module_count': 0}},\n",
       "    'extras': {}},\n",
       "   'meta': {},\n",
       "   'ok': True,\n",
       "   'error': None,\n",
       "   'logs': [{'ts': 1764051229.7315056,\n",
       "     'message': \"Tool 'directory_scan' Tool Return: {'project_root': '/home/nomi/workspace/neurosurfer/neurosurfer', 'docs_root': '/home/nomi/workspace/...\",\n",
       "     'data': {},\n",
       "     'type': 'info'}]}],\n",
       " 'meta': {'agent_type': 'generic_agent',\n",
       "  'agent_config': {'allow_input_pruning': True,\n",
       "   'repair_with_llm': True,\n",
       "   'strict_tool_call': False,\n",
       "   'temperature': 0.7,\n",
       "   'max_new_tokens': 512,\n",
       "   'return_stream_by_default': False,\n",
       "   'retry': {'max_route_retries': 2,\n",
       "    'max_tool_retries': 1,\n",
       "    'backoff_sec': 0.7},\n",
       "   'strict_json': True,\n",
       "   'max_json_repair_attempts': 1},\n",
       "  'model': '/home/nomi/workspace/Model_Weights/Qwen3-8B-unsloth-bnb-4bit',\n",
       "  'toolkit': True,\n",
       "  'verbose': True,\n",
       "  'log_steps': True}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.nodes['scan_repo'].traces.model_dump()\n",
    "# results.nodes['scan_repo'].traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7f9e61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan the project_root and docs_root to build a structured index of packages, modules, and existing markdown docs. Focus on Python files and documentation in the specified directories, using the include_patterns and exclude_patterns provided. Output a compact JSON object with the keys \"packages\", \"modules\", \"python_files\", and \"doc_files\".\n"
     ]
    }
   ],
   "source": [
    "print(results.nodes['scan_repo'].traces.steps[1].inputs['user_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd2c59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a795f88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseChatModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     34\u001b[39m     strict_json: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m                  \u001b[38;5;66;03m# enforce RFC 8259 JSON\u001b[39;00m\n\u001b[32m     35\u001b[39m     max_repair_attempts: \u001b[38;5;28mint\u001b[39m = \u001b[32m1\u001b[39m              \u001b[38;5;66;03m# for malformed JSON repairs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mNodeBudget\u001b[39;00m(\u001b[43mBaseChatModel\u001b[49m):\n\u001b[32m     40\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    Budget / LLM-related overrides per node.\u001b[39;00m\n\u001b[32m     42\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m \u001b[33;03m        - return_stream_by_default -> AgentConfig.return_stream_by_default\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     49\u001b[39m     max_new_tokens: Optional[\u001b[38;5;28mint\u001b[39m] = Field(\n\u001b[32m     50\u001b[39m         default=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     51\u001b[39m         description=\u001b[33m\"\u001b[39m\u001b[33mOverride AgentConfig.max_new_tokens for this node only.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'BaseChatModel' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class RouterRetryPolicy:\n",
    "    \"\"\"Retry tuning for routing + tool execution.\"\"\"\n",
    "    max_route_retries: int = 2\n",
    "    max_tool_retries: int = 1\n",
    "    backoff_sec: float = 0.7  # linear backoff\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    \"\"\"\n",
    "    Top-level configuration for the Agent.\n",
    "    \"\"\"\n",
    "    # Routing:\n",
    "    allow_input_pruning: bool = True    # drop unknown inputs not in ToolSpec\n",
    "    repair_with_llm: bool = True        # ask LLM to repair invalid routing/inputs\n",
    "    strict_tool_call: bool = False      # router must output JSON; else can answer in plain text\n",
    "    # synonyms: Dict[str, Dict[str, str]] = field(default_factory=dict)  # field -> {from: to}\n",
    "\n",
    "    # LLM defaults:\n",
    "    temperature: float = 0.7\n",
    "    max_new_tokens: int = 512\n",
    "    return_stream_by_default: bool = False\n",
    "\n",
    "    # Retries:\n",
    "    retry: RouterRetryPolicy = field(default_factory=RouterRetryPolicy)\n",
    "\n",
    "    # Structured-output options:\n",
    "    strict_json: bool = True                  # enforce RFC 8259 JSON\n",
    "    max_repair_attempts: int = 1              # for malformed JSON repairs\n",
    "\n",
    "\n",
    "\n",
    "class NodeBudget(BaseChatModel):\n",
    "    \"\"\"\n",
    "    Budget / LLM-related overrides per node.\n",
    "\n",
    "    These map directly to AgentConfig fields:\n",
    "        - temperature      -> AgentConfig.temperature\n",
    "        - max_new_tokens   -> AgentConfig.max_new_tokens\n",
    "        - return_stream_by_default -> AgentConfig.return_stream_by_default\n",
    "    \"\"\"\n",
    "\n",
    "    max_new_tokens: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=\"Override AgentConfig.max_new_tokens for this node only.\",\n",
    "    )\n",
    "    temperature: Optional[float] = Field(\n",
    "        default=None,\n",
    "        description=\"Override AgentConfig.temperature for this node only.\",\n",
    "    )\n",
    "    return_stream_by_default: Optional[bool] = Field(\n",
    "        default=None,\n",
    "        description=\"Override AgentConfig.return_stream_by_default for this node only.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class NodePolicy(BaseChatModel):\n",
    "    \"\"\"\n",
    "    Per-node policy that can override some AgentConfig settings and add\n",
    "    node-level execution constraints (e.g., timeout).\n",
    "\n",
    "    YAML example:\n",
    "\n",
    "        nodes:\n",
    "          - id: research\n",
    "            policy:\n",
    "              retries: 1\n",
    "              timeout_s: 30\n",
    "              budget:\n",
    "                max_new_tokens: 180\n",
    "                temperature: 0.2\n",
    "              allow_input_pruning: false\n",
    "              repair_with_llm: true\n",
    "              strict_tool_call: true\n",
    "    \"\"\"\n",
    "\n",
    "    retries: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=\"Override AgentConfig.retry.max_route_retries for this node.\",\n",
    "    )\n",
    "    timeout_s: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Soft timeout for this node in seconds. Execution isn't forcibly \"\n",
    "            \"cancelled but the node will be marked as errored if exceeded.\"\n",
    "        ),\n",
    "    )\n",
    "    budget: Optional[NodeBudget] = None\n",
    "\n",
    "    # Direct AgentConfig-like overrides\n",
    "    allow_input_pruning: Optional[bool] = None\n",
    "    repair_with_llm: Optional[bool] = None\n",
    "    strict_tool_call: Optional[bool] = None\n",
    "    strict_json: Optional[bool] = None\n",
    "    max_repair_attempts: Optional[int] = None\n",
    "\n",
    "    class Config:\n",
    "        extra = \"ignore\"  # ignore unknown keys under 'policy'\n",
    "\n",
    "c = AgentConfig()\n",
    "\n",
    "p = NodePolicy(budget=NodeBudget(temperature=1.2))\n",
    "\n",
    "print(c)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93052a6d",
   "metadata": {},
   "source": [
    "### Python API version (no YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "926be727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: True\n",
      "Answer:\n",
      " The calculator result for your request is ${compute.text}. This means that after performing the calculation based on your input, the final answer is ${compute.text}. Let me know if you need further assistance!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from graph import Graph, Node, NodePolicy, GraphConfig, GraphExecutor\n",
    "from neurosurfer.tools import Toolkit\n",
    "from neurosurfer.models.chat_models.openai import OpenAIModel\n",
    "\n",
    "# Reuse your existing toolkit + model\n",
    "llm = LLM  # already created in your environment\n",
    "tk = toolkit\n",
    "\n",
    "graph = Graph(\n",
    "    name=\"calc_and_explain\",\n",
    "    config=GraphConfig(max_concurrency=2),\n",
    "    inputs_schema={\"prompt\": str},\n",
    "    nodes=[\n",
    "        Node(\n",
    "            id=\"rewrite\",\n",
    "            fn=\"general_query_assistant\",  # adjust name if needed\n",
    "            inputs={\n",
    "                # swap \"query\" -> \"prompt\" if your tool expects \"prompt\"\n",
    "                \"query\": (\n",
    "                    \"You will receive a user request. Extract a SINGLE pure arithmetic expression that can be \"\n",
    "                    \"evaluated by a calculator (e.g., '(42 * 7) - 5^2' or '0.035 * 12000').\\n\"\n",
    "                    \"- Do NOT include explanations.\\n\"\n",
    "                    \"- Return ONLY the expression as plain text.\\n\\n\"\n",
    "                    \"User request:\\n${inputs.prompt}\"\n",
    "                )\n",
    "            },\n",
    "            outputs=[\"num1\", \"num2\", \"operation\"],\n",
    "            policy=NodePolicy(\n",
    "                retries=1,\n",
    "                timeout_s=30,\n",
    "                budget={\"max_new_tokens\": 128, \"temperature\": 0.1},\n",
    "            ),\n",
    "        ),\n",
    "        Node(\n",
    "            id=\"compute\",\n",
    "            fn=\"calculator\",\n",
    "            inputs={\"num1\": \"${rewrite.num1}\", \"num2\": \"${rewrite.num2}\", \"operation\": \"${rewrite.operation}\"},\n",
    "            outputs=[\"text\"],\n",
    "            policy=NodePolicy(retries=0, timeout_s=15),\n",
    "        ),\n",
    "        Node(\n",
    "            id=\"explain\",\n",
    "            fn=\"general_query_assistant\",\n",
    "            inputs={\n",
    "                \"query\": (\n",
    "                    \"Original request: ${inputs.prompt}\\n\"\n",
    "                    \"Calculator result: ${compute.text}\\n\\n\"\n",
    "                    \"Write a brief, user-friendly explanation of the result (one short paragraph).\"\n",
    "                )\n",
    "            },\n",
    "            outputs=[\"text\"],\n",
    "            policy=NodePolicy(\n",
    "                retries=1,\n",
    "                timeout_s=30,\n",
    "                budget={\"max_new_tokens\": 180, \"temperature\": 0.2},\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    outputs={\"answer\": \"${explain.text}\"},\n",
    ")\n",
    "\n",
    "executor = GraphExecutor(llm=llm, toolkit=tk, max_concurrency=2)\n",
    "\n",
    "result = await run_async(\n",
    "    executor.run(graph, inputs={\"prompt\": \"Compute 3.5% of 12000 and explain\"}, stream=True)\n",
    ")\n",
    "\n",
    "print(\"OK:\", result.ok)\n",
    "print(\"Answer:\\n\", result.outputs[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e25a8",
   "metadata": {},
   "source": [
    "### Planner-based path (using the YAML as a skeleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5250b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, tempfile, pathlib\n",
    "from graph import PlannerAgent, FlowLoader, GraphExecutor\n",
    "\n",
    "# 1) Write the YAML to a temp file (only for this demo)\n",
    "yaml_text = r\"\"\"\n",
    "name: calc_and_explain\n",
    "inputs:\n",
    "  prompt: str\n",
    "config:\n",
    "  max_concurrency: 2\n",
    "nodes:\n",
    "  - id: rewrite\n",
    "    kind: task\n",
    "    fn: general_query_assistant\n",
    "    inputs:\n",
    "      query: |\n",
    "        You will receive a user request. Extract a SINGLE pure arithmetic expression that can be\n",
    "        evaluated by a calculator (e.g., \"(42 * 7) - 5^2\" or \"0.035 * 12000\").\n",
    "        - Do NOT include explanations.\n",
    "        - Return ONLY the expression as plain text.\n",
    "\n",
    "        User request:\n",
    "        ${inputs.prompt}\n",
    "    outputs: [\"text\"]\n",
    "    policy: { retries: 1, timeout_s: 30, budget: { max_new_tokens: 128, temperature: 0.1 } }\n",
    "\n",
    "  - id: compute\n",
    "    kind: task\n",
    "    fn: calculator\n",
    "    inputs: { expression: ${rewrite.text} }\n",
    "    outputs: [\"text\"]\n",
    "\n",
    "  - id: explain\n",
    "    kind: task\n",
    "    fn: general_query_assistant\n",
    "    inputs:\n",
    "      query: |\n",
    "        Original request: ${inputs.prompt}\n",
    "        Calculator result: ${compute.text}\n",
    "\n",
    "        Write a brief, user-friendly explanation of the result (one short paragraph).\n",
    "    outputs: [\"text\"]\n",
    "    policy: { retries: 1, timeout_s: 30, budget: { max_new_tokens: 180, temperature: 0.2 } }\n",
    "\n",
    "outputs: { answer: ${explain.text} }\n",
    "\"\"\".strip()\n",
    "\n",
    "tmp = pathlib.Path(tempfile.gettempdir()) / \"calc_and_explain.yml\"\n",
    "tmp.write_text(yaml_text)\n",
    "\n",
    "# 2) Use the planner with a skeleton (so it returns your YAML-based Graph)\n",
    "planner = PlannerAgent(llm=LLM)  # LLM not used when skeleton is set\n",
    "graph = planner.plan_from_query(query=\"Compute 3.5% of 12000 and explain\", skeleton=str(tmp))\n",
    "\n",
    "# 3) Execute\n",
    "executor = GraphExecutor(llm=LLM, toolkit=toolkit, max_concurrency=2)\n",
    "result = asyncio.run(executor.run(graph, inputs={\"prompt\": \"Compute 3.5% of 12000 and explain\"}))\n",
    "\n",
    "print(\"OK:\", result.ok)\n",
    "print(result.outputs[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225587f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1f9ca96",
   "metadata": {},
   "source": [
    "Test ToolsRouterAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98d83d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:32\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Using tool: calculator\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:32\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Raw inputs: {'num1': 20.0, 'num2': 90.0, 'operation': 'multiply'}\n",
      "1800.0"
     ]
    }
   ],
   "source": [
    "query = \"Perform the calculation 20 * 90\"\n",
    "\n",
    "for chunk in tools_router_agent.run(query, temperature=0.7, max_new_tokens=4000):\n",
    "    print(chunk, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c6635c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:33\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Using tool: general_query_assistant\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:33\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Raw inputs: {'query': 'Tell me a light-hearted joke!'}\n",
      "Why don't skeletons fight each other? They don't have the guts!None"
     ]
    }
   ],
   "source": [
    "query = \"Tell me a light-hearted joke!\"\n",
    "\n",
    "for chunk in tools_router_agent.run(query, temperature=0.7, max_new_tokens=4000):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafe29f",
   "metadata": {},
   "source": [
    "## ReactAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aa87f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[ðŸ§ ] Chain of Thoughts...\n",
      "Thought: I will first calculate 300 - 300 using the calculator tool, and then I will use the general_query_assistant tool to find a light-hearted joke about the result.\n",
      "\n",
      "Action: {\n",
      "  \"tool\": \"calculator\",\n",
      "  \"inputs\": {\n",
      "    \"num1\": 300,\n",
      "    \"num2\": 300,\n",
      "    \"operation\": \"subtract\"\n",
      "  },\n",
      "  \"final_answer\": false\n",
      "}"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>ðŸ”§<span style=\"font-weight: bold\">]</span> Tool: calculator\n",
       "<span style=\"font-weight: bold\">[</span>ðŸ“¤<span style=\"font-weight: bold\">]</span> Inputs: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'num1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'operation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'subtract'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0mðŸ”§\u001b[1m]\u001b[0m Tool: calculator\n",
       "\u001b[1m[\u001b[0mðŸ“¤\u001b[1m]\u001b[0m Inputs: \u001b[1m{\u001b[0m\u001b[32m'num1'\u001b[0m: \u001b[1;36m300\u001b[0m, \u001b[32m'num2'\u001b[0m: \u001b[1;36m300\u001b[0m, \u001b[32m'operation'\u001b[0m: \u001b[32m'subtract'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Observation:</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mObservation:\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[ðŸ§ ] Chain of Thoughts...\n",
      "Thought: The result of the calculation is 0. Now, I will use the general_query_assistant tool to find a light-hearted joke about the result.\n",
      "\n",
      "Action: {\n",
      "  \"tool\": \"general_query_assistant\",\n",
      "  \"inputs\": {\n",
      "    \"query\": \"Tell me a light-hearted joke about the number 0.\"\n",
      "  },\n",
      "  \"final_answer\": true\n",
      "}"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>ðŸ”§<span style=\"font-weight: bold\">]</span> Tool: general_query_assistant\n",
       "<span style=\"font-weight: bold\">[</span>ðŸ“¤<span style=\"font-weight: bold\">]</span> Inputs: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Tell me a light-hearted joke about the number 0.'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0mðŸ”§\u001b[1m]\u001b[0m Tool: general_query_assistant\n",
       "\u001b[1m[\u001b[0mðŸ“¤\u001b[1m]\u001b[0m Inputs: \u001b[1m{\u001b[0m\u001b[32m'query'\u001b[0m: \u001b[32m'Tell me a light-hearted joke about the number 0.'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the number 0 break up with the number 8?  \n",
      "Because it found someone more \"8\" (8) than a zero!"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Observation:</span> Why did the number <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> break up with the number <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>?  \n",
       "Because it found someone more <span style=\"color: #008000; text-decoration-color: #008000\">\"8\"</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">)</span> than a zero!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mObservation:\u001b[0m Why did the number \u001b[1;36m0\u001b[0m break up with the number \u001b[1;36m8\u001b[0m?  \n",
       "Because it found someone more \u001b[32m\"8\"\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m than a zero!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[ðŸ§ ] Chain of Thoughts...\n",
      "Thought: The calculation result is 0, and the joke provided is ready. The final answer is complete.\n",
      "\n",
      "Final Answer: The result of 300 - 300 is 0. Here's a light-hearted joke about it: Why did the number 0 break up with the number 8? Because it found someone more \"8\" (8) than a zero!"
     ]
    }
   ],
   "source": [
    "from neurosurfer.agents.react import ReActAgent, ReActConfig\n",
    "\n",
    "react_agent = ReActAgent(\n",
    "    toolkit=toolkit,\n",
    "    llm=LLM,\n",
    "    specific_instructions=\"Always be concise in your answers. Break the task into steps if needed.\",\n",
    "    config=ReActConfig(\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=4096,\n",
    "        allow_input_pruning=True,\n",
    "        repair_with_llm=True,\n",
    "        skip_special_tokens=True,\n",
    "        verbose=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# print(react_agent._system_prompt())\n",
    "TASK = \"\"\"Calculate 300 - 300. Then tell me a light-hearted joke about that result.\"\"\"\n",
    "\n",
    "for chunk in react_agent.run(TASK):\n",
    "    print(chunk, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41580e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c7a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1d90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766512d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad2c2b-4a95-4ac0-9727-4c746e97a629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ea014-d67c-4af9-b2f6-1a8d80d63b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
