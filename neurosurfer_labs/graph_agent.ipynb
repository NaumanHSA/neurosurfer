{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b35214-7a1f-403c-9a7a-b42e481ab431",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# Go up one directory from `b/` to project root\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affc4a2e-d532-4dee-8b50-cab72fd229c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë                                                                  ‚ïë\n",
      "‚ïë ‚ñì‚ñì‚ñì‚ñì‚ñì   ‚ñì‚ñì‚ñì‚ñì                                  ‚ñì‚ñì‚ñì                ‚ïë\n",
      "‚ïë  ‚ñì‚ñì ‚ñì‚ñì   ‚ñì‚ñì  ‚ñì‚ñì‚ñì‚ñì ‚ñì  ‚ñì ‚ñì ‚ñì ‚ñì‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì ‚ñì  ‚ñì ‚ñì ‚ñì  ‚ñì   ‚ñì‚ñì‚ñì‚ñì ‚ñì ‚ñì       ‚ïë\n",
      "‚ïë  ‚ñì‚ñì  ‚ñì‚ñì  ‚ñì‚ñì  ‚ñì‚ñÅ‚ñÅ‚ñì ‚ñì  ‚ñì ‚ñì‚ñì‚ñè ‚ñì  ‚ñì ‚ñì‚ñÅ  ‚ñì  ‚ñì ‚ñì‚ñì‚ñè ‚ñì‚ñì‚ñì  ‚ñì‚ñÅ‚ñÅ‚ñì ‚ñì‚ñì        ‚ïë\n",
      "‚ïë  ‚ñì‚ñì   ‚ñì‚ñì ‚ñì‚ñì  ‚ñì    ‚ñì  ‚ñì ‚ñì   ‚ñì  ‚ñì   ‚ñì ‚ñì  ‚ñì ‚ñì    ‚ñì   ‚ñì    ‚ñì         ‚ïë\n",
      "‚ïë ‚ñì‚ñì‚ñì‚ñì   ‚ñì‚ñì‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì‚ñì ‚ñì   ‚ñì‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì‚ñì ‚ñì    ‚ñì   ‚ñì‚ñì‚ñì‚ñì ‚ñì         ‚ïë\n",
      "‚ïë ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ‚ïë\n",
      "‚ïë Orchestrate Agents - RAG - SQL Tools - Multi-LLM - FastAPI Ready ‚ïë\n",
      "‚ïë Faster builds, clearer flows, production-first                   ‚ïë\n",
      "‚ïë                                                                  ‚ïë\n",
      "‚ïë Version: 0.1.3 | Python: 3.11.13                                 ‚ïë\n",
      "‚ïë OS: Linux 6.14.0-35-generic (x86_64)                             ‚ïë\n",
      "‚ïë Torch: 2.7.1+cu126   CUDA: yes (12.6)                            ‚ïë\n",
      "‚ïë MPS: no (built: False)                                           ‚ïë\n",
      "‚ïë Transformers: 4.51.3   SentEmb: 5.1.0                            ‚ïë\n",
      "‚ïë Accelerate: 1.10.1   bnb: 0.47.0                                 ‚ïë\n",
      "‚ïë Unsloth: 2025.8.10                                               ‚ïë\n",
      "‚ïë                                                                  ‚ïë\n",
      "‚ïë Detected CUDA devices: NVIDIA GeForce RTX 3080 Ti                ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomi/anaconda3/envs/LLMs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 10:17:04\u001b[0m | \u001b[96mconfig.py:<module>\u001b[0m | PyTorch version 2.7.1+cu126 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomi/anaconda3/envs/LLMs/lib/python3.11/importlib/__init__.py:126: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 11-18 10:17:05 [__init__.py:241] Automatically detected platform cuda.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 10:17:07\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Initializing Transformers model.\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-18 10:17:07\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Model is already quantized. Ignoring load_in_4bit=True.\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 10:17:07\u001b[0m | \u001b[96mmodeling.py:get_balanced_memory\u001b[0m | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 10:17:08\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Transformers model initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from neurosurfer.models.chat_models.transformers import TransformersModel\n",
    "from neurosurfer import config \n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "DEFAULT_TRANSFORMERS_MODEL_PARAMS = dict({\n",
    "    \"model_name\": \"/home/nomi/workspace/Model_Weights/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"max_seq_length\": 16_000,\n",
    "    \"load_in_4bit\": True,\n",
    "    \"enable_thinking\": False,  # main_gpu interpretation\n",
    "    \"verbose\": False\n",
    "})\n",
    "\n",
    "LOGGER = logging.getLogger()\n",
    "LLM = TransformersModel(\n",
    "    **DEFAULT_TRANSFORMERS_MODEL_PARAMS,\n",
    "    stop_words=[\"Observation:\"],\n",
    "    logger = logging.getLogger(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16ff5798-3ed2-4b38-86cd-2498ca0e57be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't skeletons fight each other?  \n",
      "Because they don't have the *guts*! üòÑ"
     ]
    }
   ],
   "source": [
    "# streaming response example\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "system_prompt = \"You are a joker.\"\n",
    "user_prompt = \"\"\"Tell me a short and light-hearted joke.\"\"\"\n",
    "\n",
    "stream_response = LLM.ask(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=user_prompt,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "md_display = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream_response:\n",
    "    chunk = chunk.choices[0].delta.content or \"\"\n",
    "    print(chunk, flush=True, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808255b3",
   "metadata": {},
   "source": [
    "### Agent Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3030e4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Response:\n",
      "\u001b[1;4;33müß† Thinking\u001b[0m\u001b[1;4;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmain_agent\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\n",
      "\u001b[2m     ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m881s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m882s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmain_agent\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;4;32mFinal response:\u001b[0m\n",
      "AI, or Artificial Intelligence, is the simulation of human intelligence in machines that are programmed to think, learn, and perform tasks typically requiring human cognition.\n"
     ]
    }
   ],
   "source": [
    "# agent normal response\n",
    "from neurosurfer.agents import Agent, AgentConfig\n",
    "# from neurosurfer.tracing import RichTracer\n",
    "from pydantic import BaseModel\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    strict_tool_call=True,\n",
    "    return_stream_by_default=True,\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "agent = Agent(llm=LLM, config=agent_config, log_traces=True)\n",
    "\n",
    "# normal response\n",
    "print(\"Normal Response:\")\n",
    "agent_response = agent.run(user_prompt=\"What is AI (one line)?\", stream=False)\n",
    "# print(agent_response.response)\n",
    "\n",
    "# # streaming response\n",
    "# print(\"\\n\\nStreaming Response:\")\n",
    "# for c in agent.run(user_prompt=\"What are top 3 applications of AI (one line)?\").response:\n",
    "#     print(c, flush=True, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d91b3b",
   "metadata": {},
   "source": [
    "**Structured Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344ffe52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseChatModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Structured Response examples\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mAIApplication\u001b[39;00m(BaseChatModel):\n\u001b[1;32m      3\u001b[0m     title: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m      4\u001b[0m     description: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseChatModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Structured Response examples\n",
    "class AIApplication(BaseChatModel):\n",
    "    title: str\n",
    "    description: str\n",
    "\n",
    "class AI(BaseChatModel):\n",
    "    definition: str\n",
    "    history: str\n",
    "    modern_frameworks: str\n",
    "    applications: list[AIApplication]\n",
    "\n",
    "user_query = \"What is AI and list 3 of its top application, and 3 concerns.\"\n",
    "agent_response = agent.run(user_prompt=user_query, output_schema=AI)\n",
    "\n",
    "print(\"\\n\\nStructured Response:\")\n",
    "print(agent_response.response.json_obj)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4f0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step_id': 2,\n",
       " 'kind': 'llm.call',\n",
       " 'label': 'agent.structured_call.first_pass',\n",
       " 'node_id': None,\n",
       " 'agent_id': 'main_agent',\n",
       " 'started_at': 1763359998.1342587,\n",
       " 'duration_ms': 4659,\n",
       " 'inputs': {'schema': 'AI',\n",
       "  'system_prompt_len': 52,\n",
       "  'user_prompt_len': 61,\n",
       "  'user_prompt': 'What is AI and list 3 of its top application, and 3 concerns.',\n",
       "  'system_prompt': \"You are a precise and rule-abiding assistant.  \\nYour task is to produce only a single valid JSON object following the schema below.\\n\\nStructured Output Contract:\\n- Output only JSON ‚Äî no markdown, code fences, or explanations.  \\n- JSON must be strictly valid (RFC 8259): use double quotes for all keys and string values.  \\n- Do not include extra keys or any text outside the JSON object.  \\n- All required fields must be present, even if empty.  \\n- Arrays must contain at least one object when applicable.  \\n- The JSON must be a single complete object (not pretty-printed, no trailing commas).  \\n- Failure to comply with this structure means your response is invalid.\\n\\nExpected JSON Structure:\\n{'$defs': {'AIApplication': {'properties': {'title': {'title': 'Title', 'type': 'string'}, 'description': {'title': 'Description', 'type': 'string'}}, 'required': ['title', 'description'], 'title': 'AIApplication', 'type': 'object'}}, 'properties': {'definition': {'title': 'Definition', 'type': 'string'}, 'history': {'title': 'History', 'type': 'string'}, 'modern_frameworks': {'title': 'Modern Frameworks', 'type': 'string'}, 'applications': {'items': {'$ref': '#/$defs/AIApplication'}, 'title': 'Applications', 'type': 'array'}}, 'required': ['definition', 'history', 'modern_frameworks', 'applications'], 'title': 'AI', 'type': 'object'}\\n\\nNow generate your response strictly following this contract.\\n\",\n",
       "  'temperature': 0.7,\n",
       "  'max_new_tokens': 4096,\n",
       "  'stream': False},\n",
       " 'outputs': {'model_response': '{\"definition\":\"Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think, learn, and make decisions.\",\"history\":\"The concept of AI dates back to the 1950s, with the term \\'artificial intelligence\\' coined in 1956. Early developments focused on problem-solving and logical reasoning, while recent advancements have been driven by machine learning and big data.\",\"modern_frameworks\":\"Modern frameworks such as TensorFlow, PyTorch, and Keras have revolutionized AI development by providing scalable tools for building and deploying machine learning models.\",\"applications\":[{\"title\":\"Healthcare\",\"description\":\"AI is used for diagnostics, personalized treatment plans, and drug discovery.\"},{\"title\":\"Finance\",\"description\":\"AI powers fraud detection, algorithmic trading, and risk management systems.\"},{\"title\":\"Autonomous Vehicles\",\"description\":\"AI enables self-driving cars through computer vision and real-time data processing.\"}]}',\n",
       "  'model_response_len': 991},\n",
       " 'meta': {},\n",
       " 'ok': True,\n",
       " 'error': None,\n",
       " 'logs': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_response.traces.steps[1].model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf48079",
   "metadata": {},
   "source": [
    "**Tool Calling Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e9634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-17 10:14:12\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: calculator\n",
      "Agent with choice between tools and plain text:\n",
      "\u001b[1;4;33müß† Thinking\u001b[0m\u001b[1;4;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;4;36mTracing Start!\u001b[0m\n",
      "\u001b[2m ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\n",
      "        \u001b[1;32mINFO: Returning plain response\u001b[0m\n",
      "\u001b[2m     ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m1.\u001b[0m\u001b[2m506s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m1.\u001b[0m\u001b[2m508s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36mTracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;4;32mFinal response:\u001b[0m\n",
      "AI stands for Artificial Intelligence, which refers to the simulation of human intelligence in machines that are programmed to think, learn, and make decisions. These systems can perform tasks such as problem-solving, understanding natural language, recognizing patterns, and adapting to new information.\n",
      "\n",
      "\n",
      "Agent with strict tool call:\n",
      "\u001b[1;4;33müß† Thinking\u001b[0m\u001b[1;4;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;4;36mTracing Start!\u001b[0m\n",
      "\u001b[2m ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\n",
      "        \u001b[1;32mINFO: Selected tool: calculator\u001b[0m\n",
      "        \u001b[1;32mINFO: Raw inputs: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'num1'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m100\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'num2'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m, \u001b[0m\u001b[1;32m'operation'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m'divide'\u001b[0m\u001b[1;32m}\u001b[0m\n",
      "\u001b[2m     ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m969s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m     ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\n",
      "        \u001b[1;32mINFO: Tool \u001b[0m\u001b[1;32m'calculator'\u001b[0m\u001b[1;32m returned: \u001b[0m\u001b[1;32m25.0\u001b[0m\n",
      "\u001b[2m     ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m001s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m971s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36mTracing End!\u001b[0m\n",
      "\n",
      "ToolCallResponse(selected_tool='calculator', inputs={'num1': 100.0, 'num2': 4.0, 'operation': 'divide'}, returns='25.0', final=False, extras={})\n"
     ]
    }
   ],
   "source": [
    "from neurosurfer.tools.toolkit import Toolkit\n",
    "from neurosurfer.tools.tool_spec import ToolSpec, ToolParam, ToolReturn\n",
    "from neurosurfer.tools.base_tool import BaseTool, ToolResponse\n",
    "\n",
    "# Simple Calculator Tool\n",
    "class CalculatorTool(BaseTool):\n",
    "    spec = ToolSpec(\n",
    "        name=\"calculator\",\n",
    "        description=\"Perform basic arithmetic operations such as addition, subtraction, multiplication, and division.\",\n",
    "        when_to_use=\"Use this tool when you need to perform basic arithmetic operations.\",\n",
    "        inputs=[\n",
    "            ToolParam(name=\"num1\", type=\"float\", description=\"The first number.\", required=True),\n",
    "            ToolParam(name=\"num2\", type=\"float\", description=\"The second number.\", required=True),\n",
    "            ToolParam(name=\"operation\", type=\"string\", description=\"The operation to perform strictly one of ['add', 'subtract', 'multiply', 'divide'].\", required=True)\n",
    "        ],\n",
    "        returns=ToolReturn(type=\"float\", description=\"The result of the arithmetic operation.\")\n",
    "    )\n",
    "\n",
    "    def __init__(self, final_answer: bool = False):\n",
    "        self.final_answer = final_answer\n",
    "\n",
    "    def __call__(self, num1: float, num2: float, operation: str) -> ToolResponse:\n",
    "        if operation not in [\"add\", \"subtract\", \"multiply\", \"divide\"]:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                results=\"Invalid operation. Supported operations are 'add', 'subtract', 'multiply', and 'divide'.\",\n",
    "                extras={}\n",
    "            )\n",
    "        \n",
    "        if operation == \"divide\" and num2 == 0:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                results=\"Division by zero is not allowed.\",\n",
    "                extras={}\n",
    "            )\n",
    "        try:\n",
    "            num1 = float(num1)\n",
    "            num2 = float(num2)\n",
    "            if operation == \"add\":\n",
    "                result = num1 + num2\n",
    "            elif operation == \"subtract\":\n",
    "                result = num1 - num2\n",
    "            elif operation == \"multiply\":\n",
    "                result = num1 * num2\n",
    "            elif operation == \"divide\":\n",
    "                result = num1 / num2\n",
    "        except Exception as e:\n",
    "            return ToolResponse(\n",
    "                final_answer=False,\n",
    "                results=f\"An error occurred: {str(e)}\",\n",
    "                extras={}\n",
    "            )\n",
    "        \n",
    "        return ToolResponse(\n",
    "            final_answer=self.final_answer,\n",
    "            results=float(result),\n",
    "            extras={}\n",
    "        )\n",
    "\n",
    "calculator_tool = CalculatorTool()\n",
    "toolkit = Toolkit(tools=[calculator_tool])\n",
    "\n",
    "# print(\"Tool description:\")\n",
    "# print(calculator_tool.get_tool_description())\n",
    "# print()\n",
    "\n",
    "agent = Agent(llm=LLM, toolkit=toolkit, verbose=True)\n",
    "\n",
    "print(\"Agent with choice between tools and plain text:\")\n",
    "agent_response = agent.run(user_prompt=\"What is AI?\", strict_tool_call=False)\n",
    "# print(agent_response.response)\n",
    "\n",
    "print(\"\\n\\nAgent with strict tool call:\")\n",
    "agent_response = agent.run(user_prompt=\"What is one forth of a 100?\", strict_tool_call=True)\n",
    "print(agent_response.response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a stateless tool router.\n",
      "Your task is to select exactly ONE tool from the catalog below and output STRICT JSON describing how to call it.\n",
      "\n",
      "Always respond with a single one-line valid JSON object:\n",
      "{\"tool\": \"<tool_name>\", \"inputs\": {<param>: <value>}}\n",
      "\n",
      "Rules:\n",
      "- Output MUST contain exactly the keys \"tool\" and \"inputs\".\n",
      "- Select at most one tool; if none applies or inputs are unclear, use:\n",
      "  {\"tool\": \"none\", \"inputs\": {}}\n",
      "- Use only parameters explicitly defined by that tool ‚Äî do NOT invent, rename, or add extra fields.\n",
      "- Include only required parameters unless an optional one is obviously needed.\n",
      "- Do NOT produce natural language; emit JSON only.\n",
      "\n",
      "TOOLS CATALOG:\n",
      "Available tools:\n",
      "Tool Name: `calculator`\n",
      "Description: Perform basic arithmetic operations such as addition, subtraction, multiplication, and division.\n",
      "When to use: Use this tool when you need to perform basic arithmetic operations.\n",
      "Tool Inputs:\n",
      "- `num1`: float (required) ‚Äî The first number.\n",
      "- `num2`: float (required) ‚Äî The second number.\n",
      "- `operation`: string (required) ‚Äî The operation to perform strictly one of ['add', 'subtract', 'multiply', 'divide'].\n",
      "Tool Return: float ‚Äî The result of the arithmetic operation.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step_json = agent_response.traces.steps[1].model_dump()\n",
    "# print(step_json)\n",
    "\n",
    "print()\n",
    "print(step_json[\"inputs\"][\"system_prompt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f992320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich.console import Console\n",
    "\n",
    "# console = Console(force_jupyter=False, force_terminal=True, width=200)\n",
    "\n",
    "# msg = \"step.tool.execute label='agent.route_and_call.tool_execute' agent_id=None took 0.001s; error=False\"\n",
    "# console.print(f\"[dim]{msg}[/dim]\")\n",
    "# console.print(f\"[dim]{msg}[/dim]\")\n",
    "# console.print(f\"[cyan underline]Hello World!\")\n",
    "# console.print(\"FOO\", style=\"white on blue\")\n",
    "# console.print(\"[bold italic yellow on red blink]This text is impossible to read\")\n",
    "# console.print(\"[bold red]alert![/bold red] Something happened\")\n",
    "# console.print(\"[bold red]\\\\[trace]![/bold red] Something happened\")\n",
    "# console.print(\"[underline][bold green]Tracing Start![/bold green] Something happened\")\n",
    "# console.print(\"[bold]Bold[italic] bold and italic [/bold]italic[/italic]\")\n",
    "# console.print(\"Visit my [link=https://www.willmcgugan.com]blog[/link]!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b1381",
   "metadata": {},
   "source": [
    "## RAG wiring so the agent ‚Äúunderstands‚Äù the Neurosurf codebase\n",
    "\n",
    "You‚Äôll ingest the repo once, then run a retriever to answer code questions. The Planner can call the retriever first to form a precise implementation plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f5bf38-1a5b-4ad1-be5f-b5073dbeaaf0",
   "metadata": {},
   "source": [
    "### FileReader and Chunker Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a35db287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scripts/index_repo_for_rag.py\n",
    "# from pathlib import Path\n",
    "# from neurosurfer.vectorstores.chroma import ChromaVectorStore\n",
    "# from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n",
    "# from neurosurfer.agents.rag.chunker import Chunker\n",
    "# from neurosurfer.agents.rag.filereader import FileReader\n",
    "# from neurosurfer.agents.rag import RAGAgent, RAGAgentConfig, RAGIngestorConfig\n",
    "\n",
    "# chunker = Chunker()\n",
    "# file_reader = FileReader()\n",
    "\n",
    "# embedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\n",
    "\n",
    "# rag_agent = RAGAgent(\n",
    "#     llm=LLM,\n",
    "#     embedder=embedder,\n",
    "#     file_reader=file_reader,\n",
    "#     chunker=chunker,\n",
    "#     config=RAGAgentConfig(\n",
    "#         top_k=5,\n",
    "#         fixed_max_new_tokens=1024,\n",
    "#         clear_collection_on_init=True\n",
    "#     ),\n",
    "#     ingestor_config=RAGIngestorConfig(\n",
    "#         batch_size=64,\n",
    "#         max_workers=4,\n",
    "#         deduplicate=True,\n",
    "#         normalize_embeddings=True,\n",
    "#         default_metadata=None,\n",
    "#         tmp_dir=\"./rag-storage\",\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# dir_path = \"../neurosurfer\"\n",
    "# summary = rag_agent.ingest(sources=dir_path)\n",
    "\n",
    "# summary\n",
    "\n",
    "# retrival_results = rag_agent.retrieve(user_query=\"Explain how graph agent is initialized\", top_k=10)\n",
    "\n",
    "# print(\"max_new_tokens\", retrival_results.max_new_tokens)\n",
    "# print()\n",
    "# print(retrival_results.context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d61982-ac42-46b7-8a6b-bc3ed2248907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # scripts/index_repo_for_rag.py\n",
    "# from pathlib import Path\n",
    "# from neurosurfer.rag.ingestor import RAGIngestor\n",
    "# from neurosurfer.rag.chunker import Chunker\n",
    "# from neurosurfer.rag.filereader import FileReader\n",
    "# from neurosurfer.vectorstores.chroma import ChromaVectorStore\n",
    "# from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n",
    "\n",
    "# embedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\n",
    "# vs = ChromaVectorStore(collection_name=\"neurosurf-repo\")\n",
    "# ing = RAGIngestor(\n",
    "#     embedder=embedder,\n",
    "#     vector_store=vs, \n",
    "#     chunker=Chunker(), \n",
    "#     file_reader=FileReader(),\n",
    "#     default_metadata={\"collection\": \"neurosurf\"}\n",
    "# )\n",
    "\n",
    "# root_dir = Path(os.getcwd()).parent.joinpath(\"neurosurfer\")\n",
    "# ing.add_directory(root_dir)  # the repo root\n",
    "# print(ing.build())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4c8c2",
   "metadata": {},
   "source": [
    "## Graph AGENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec7eb2",
   "metadata": {},
   "source": [
    "### YAML Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4e3d42ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key:  f443633b...\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-18 14:44:24\u001b[0m | \u001b[96magent.py:__init__\u001b[0m | No vectorstore provided to RAGAgent, using default ChromaVectorStore. Initializing collection `neurosurfer-rag-agent`\n",
      "[Init] ChromaVectorStore initialized with collection: neurosurfer-rag-agent\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 14:44:35\u001b[0m | \u001b[96mSentenceTransformer.py:__init__\u001b[0m | Use pytorch device_name: cuda:0\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 14:44:35\u001b[0m | \u001b[96msentence_transformer.py:__init__\u001b[0m | SentenceTransformer embedding model initialized.\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 14:44:35\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: web_search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 14:44:40\u001b[0m | \u001b[96m_common.py:_log_backoff\u001b[0m | Backing off send_request(...) for 0.8s (requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Read timed out. (read timeout=15))\n"
     ]
    }
   ],
   "source": [
    "# test web search tool\n",
    "from neurosurfer.tools.websearch import WebSearchTool, WebSearchConfig\n",
    "from neurosurfer.tools.toolkit import Toolkit\n",
    "\n",
    "api_key = os.getenv(\"SERPAPI_KEY\", \"API Key not found...\")\n",
    "print(\"API Key: \", f\"{api_key[:8]}...\")\n",
    "\n",
    "web_search_tool = WebSearchTool(\n",
    "    config=WebSearchConfig(\n",
    "        engine=\"serpapi\",\n",
    "        engine_kwargs={\"api_key\": api_key},\n",
    "        max_results=3,\n",
    "        enable_crawl=True,\n",
    "        max_crawl_results=2,\n",
    "        content_words_limit=2000,\n",
    "        content_limit_strategy=\"distributive\",\n",
    "        summarize=False,\n",
    "        top_k=10,\n",
    "    ),\n",
    "    llm=LLM,\n",
    ")\n",
    "\n",
    "# searches = web_search_tool(query=\"Importance of sleep in health.\")\n",
    "# print(searches)\n",
    "\n",
    "toolkit = Toolkit(tools=[web_search_tool])\n",
    "# print(toolkit.registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346487b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Importance of sleep in health.',\n",
       " 'summary': \"Top 3 results out of ~988,000,000 results for: 'Importance of sleep in health.'\\n1. How Sleep Works - Why Is Sleep Important? - NHLBI - NIH ‚Äî https://www.nhlbi.nih.gov/health/sleep/why-sleep-important\\n2. Better sleep: Why it's important for your health and tips to ... ‚Äî https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03\\n3. Sleep is essential to health - Journal of Clinical Sleep Medicine ‚Äî https://jcsm.aasm.org/doi/10.5664/jcsm.9476\",\n",
       " 'provider': 'serpapi',\n",
       " 'elapsed_ms': 3004,\n",
       " 'rag_content': 'Source: 355e311e80b4be26:cc35f490\\nlth Sleep has become a precious commodity ‚Äì we love it and need it, but rarely get enough of it. Busy schedules, kids, anxiety and technology can all get in the way of a good night sleep. Getting enough sleep can play an important role in your weight, emotional wellbeing, blood pressure, diabetes, mental and physical performance, and more. Remember that adults aren\\'t the only ones who need good sleep. It\\'s also critical that children get even more sleep than adults. Why is sleep important for health? The three pillars of health are nutrition, physical exercise, and sleep. All three of these are connected. For example, if you don\\'t sleep well, you may not eat well. People get food cravings when they haven\\'t slept well, and they often crave a food with lots of carbohydrates (carbs) like a cookie. And when you are tired, the last thing you want to do is go to the gym. People who are fully functioning pay attention to all three. They must all be working together for better health. Here are\\n\\n---\\n\\nSource: 1569daa75eb865f1:cc35f490\\nBack To How Sleep Works Why Is Sleep Important? 0 How Sleep Works MENU Home Health Topics < Back To How Sleep Works Why Is Sleep Important? How Sleep Works Your Sleep/Wake Cycle Sleep Phases and Stages Why Is Sleep Important? How Much Sleep Is Enough? MORE INFORMATION Fact Sheets and Handouts Research How Sleep Works How Sleep Works Why Is Sleep Important? Language switcher English Espa√±ol IN THIS ARTICLE View More View Less Sleep plays a vital role in good health and well-being throughout your life. The way you feel while you are awake depends in part on what happens while you are sleeping. During sleep, your body is working to support healthy brain function and maintain your physical health. In children and teens, sleep also helps support growth and development. Getting inadequate sleep over time can raise your risk for chronic (long-term) health problems. It can also affect how well you think, react, work, learn, and get along with others. Learn how sleep affects your heart and circ\\n\\n---\\n\\nSource: 4bd421ad2e29abac:cc35f490\\nBetter sleep: Why it‚Äôs important for your health and tips to sleep soundly | Cultivating Health | UC Davis Health search Search all UC Davis health Main Menu add menu Main Menu close Main Menu Main Menu remove UC Davis Health Home Patients & Visitors Services & Specialties Health Care Professionals Schools & Programs Research News About UC Davis Health Giving Careers search Search √ó Search all UC Davis Health Search All UC Davis Health Search Search enhanced by Google close Search all UC Davis Health Search All UC Davis Health Search Search enhanced by Google Skip to main content Cultivating Health Show menu menu Menu Cancer Care Children\\'s Health Fitness Heart Health Mental Health All Articles notifications Subscribe Mental Health MARCH 15, 2023 Better sleep: Why it‚Äôs important for your health and tips to sleep soundly By Cultivating Health Sleep has become a precious commodity ‚Äì we love it and need it, but rarely get enough of it. Busy schedules, kids, anxiety and technology can all\\n\\n---\\n\\nSource: 91de3a34d7fa00e2:cc35f490\\nant to do is go to the gym. People who are fully functioning pay attention to all three. They must all be working together for better health. Here are some other health benefits of sleep: promotes growth helps heart health supports weight management helps combat germs and keep your immune system strong reduces risk of injury increases attention span boosts memory and learning Find out if melatonin is safe, its side effects and if it helps you sleep How much sleep should adults get? Studies show that adults should get seven to eight hours a night for good health. Some people insist that they can get away with four or five hours of sleep. While these so-called \"short sleepers\" do exist, they are a very small percentage of the population. The rest of the self-identified \"short sleepers\" are mostly staying alert by drinking coffee or other caffeinated drinks. Not getting enough sleep can raise the risk of health consequences. However, getting enough sleep isn\\'t just about the number of hou\\n\\n---\\n\\nSource: e388173c38819c5e:cc35f490\\nrm) health problems. It can also affect how well you think, react, work, learn, and get along with others. Learn how sleep affects your heart and circulatory system, metabolism , respiratory system, and immune system and how much sleep is enough. BROCHURE This brochure describes the differences between the types of sleep needed to feel awake and to be healthy and offers tips for getting a good night‚Äôs sleep. View the brochure Heart and circulatory system When you fall asleep and enter non-REM sleep , your blood pressure and heart rate fall. During sleep, your parasympathetic system controls your body, and your heart does not work as hard as it does when you are awake. During REM sleep and when waking, your sympathetic system is activated, increasing your heart rate and blood pressure to the usual levels when you are awake and relaxed. A sharp increase in blood pressure and heart rate upon waking has been linked to angina, or chest pain, and heart attacks . People who do not sleep enoug\\n\\n---\\n\\nSource: 527693a794d92a2b:cc35f490\\nffeinated drinks. Not getting enough sleep can raise the risk of health consequences. However, getting enough sleep isn\\'t just about the number of hours you\\'re asleep. It\\'s also about the quality of sleep and that you stay on a regular schedule so that you feel rested when you wake up. Learn about anxiety symptoms and when to know if you need help How much sleep should children get? According to the U.S. Department of Health and Human Services , these are the recommended number of hours of sleep based on a child\\'s age: Newborns: 14-17 hours a day Babies: 12-16 hours a day (including naps) Toddlers: 11-14 hours a day (including naps) Preschoolers: 10-13 hours a day (including naps) School-aged children: 9-12 hours each night Teenagers: 8-10 hours each night What are some health risks of not getting enough sleep? Not enough sleep or routinely getting broken sleep is linked with seven of the 15 leading causes of death in the U.S. These include: Heart disease Cancerous tumors Diseases rela\\n\\n---\\n\\nSource: f5deaa2fd34dff22:cc35f490\\nuctive pulmonary disease (COPD) . Asthma symptoms are usually worse during early morning sleep. Likewise, breathing problems in people who have lung diseases such as COPD can become worse during sleep. Sleep also affects different parts of your immune system, which become more active at different times of day. For example, when you sleep, a particular type of immune cell works harder. That is why people who do not sleep enough may be more likely to get colds and other infections. FACT SHEET Sleep Fact Sheet Learn some sleep terms and find out about treatments that can help with sleep apnea. View the fact sheet Problems with thinking and memory Sleep helps with learning and the formation of long-term memories. Not getting enough sleep or enough high-quality sleep can lead to problems focusing on tasks and thinking clearly. Read our Sleep Deprivation and Deficiency page for more information on how lack of sleep affects performance of daily activities, including driving and schoolwork. Bo\\n\\n---\\n\\nSource: baa9ae9e26cf11b5:cc35f490\\nHow Sleep Works - Why Is Sleep Important? | NHLBI, NIH Skip to main content An official website of the United States government Here‚Äôs how you know Here‚Äôs how you know Official websites use .gov A .gov website belongs to an official government organization in the United States. Secure .gov websites use HTTPS A lock ( A locked padlock ) or https:// means you‚Äôve safely connected to the .gov website. Share sensitive information only on official, secure websites. Search Query: Health Topics All Health Topics A-Z Asthma Heart-Healthy Living High Blood Pressure Sickle Cell Disease Sleep Apnea Calculate Your BMI Health Education Education Programs and Initiatives The Heart Truth¬Æ Learn More Breathe Better¬Æ Blood Diseases & Disorders Education Program Publications and Resources Research Clinical Trials and Studies Research Focus Areas Blood Disorders and Blood Safety Sleep Science and Sleep Disorders Lung Diseases Health Disparities Heart and Vascular Diseases Precision Medicine Activities Obe\\n\\n---\\n\\nSource: dbd50916457b69e1:cc35f490\\n, laptop, etc.) in an area of the house other than the bedrooms. Sleep in a dark room because light stimulates our brains. Use an alarm clock rather than your smartphone or tablet as a wakeup device. Keep room temperatures on the cooler side ‚Äì ideally low to mid-60s. Aim for a consistent bedtime routine and sleep schedule to help your body stay on a regular track. Find a good time for you to go to sleep every night and wake up at the same time every morning. It\\'s also important to keep that same schedule even on the weekends. Find out about social media\\'s impact on our mental health and tips to use it safely What happens to your brain when you don\\'t get enough sleep? Sleep deprivation affects your ability to remember, concentrate, and make good decisions. Your reaction time is also reduced. A sleep-deprived driver has the same poor response time as someone who is legally drunk. Not getting enough sleep makes us more emotionally unstable. Lack of sleep can cause you to have very strong\\n\\n---\\n\\nSource: 10685d511e3aaad8:cc35f490\\nse time as someone who is legally drunk. Not getting enough sleep makes us more emotionally unstable. Lack of sleep can cause you to have very strong emotions, such as extreme sadness or anger. Does sleep play a role in Alzheimer\\'s disease? One thing that connects almost all mental and nervous system disorders is some level of wake and sleep disruption. Health experts know that treating sleep disruptions can help stabilize neurologic disorders. But left untreated, sleep disruption may contribute to the progression of disease. One example is Alzheimer\\'s disease . We know that sleep is disrupted in the early stages of the disease. If we could address that early on, perhaps the progression of the disease could be delayed. Patrick M. Fuller , a neuroscientist who studies how the brain regulates sleeping and waking, contributed and reviewed this article. Fuller is a professor in UC Davis Health\\'s Department of Neurological Surgery and vice chair for research. Explore related topics Mental H',\n",
       " 'llm_summary': 'Sleep is crucial for overall health and well-being, playing a vital role in physical health, mental function, emotional stability, and immune system support (source: https://www.nhlbi.nih.gov/health/sleep/why-sleep-important). \\n\\n**Key Points:**\\n- Sleep supports brain function, including memory consolidation and learning (source: https://jcsm.aasm.org/doi/10.5664/jcsm.9476).\\n- It helps regulate bodily functions such as heart health, blood pressure, and metabolism (source: https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03).\\n- Chronic sleep deprivation is linked to an increased risk of chronic diseases, including heart disease, diabetes, and weakened immune function (source: https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03).\\n- Adults are recommended to get 7‚Äì8 hours of sleep per night, while children and teens require more (source: https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03).\\n\\n**Caveats:**\\n- The quality of sleep is as important as the quantity. Poor sleep quality can have similar negative effects as insufficient sleep (source: https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03).\\n- Some individuals, known as \"short sleepers,\" may function with less sleep, but this is rare and not recommended for most people (source: https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03).\\n- There is some debate about the exact impact of sleep on conditions like Alzheimer\\'s disease, with some studies suggesting a potential link between sleep disruption and disease progression (source: https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03).'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# searches.results\n",
    "# # print(searches.results[\"rag_content\"])\n",
    "# print(searches.results[\"llm_summary\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3dbe3",
   "metadata": {},
   "source": [
    "**GrpahAgent from YML file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bf1ccb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-18 14:58:05\u001b[0m | \u001b[96mutils.py:normalize_and_validate_graph_inputs\u001b[0m | Ignoring extra inputs not declared in graph spec: audience, tone\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 14:58:05\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: web_search\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmanager\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\n",
      "\u001b[2m ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m2.\u001b[0m\u001b[2m249s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmanager\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;4;33müß† Thinking\u001b[0m\u001b[1;4;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mresearch\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\n",
      "        \u001b[1;32mINFO: Selected tool: web_search\u001b[0m\n",
      "        \u001b[1;32mINFO: Raw inputs: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'query'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m'The Missing Middle Layer: Why LLM Systems Need Tool Routers, Not Bigger Models'\u001b[0m\u001b[1;32m}\u001b[0m\n",
      "\u001b[2m     ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m1.\u001b[0m\u001b[2m079s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m     ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-18 14:58:09\u001b[0m | \u001b[96mingestor.py:ingest\u001b[0m | Some sources were skipped as unsupported: [None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 16.04it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 53.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 14:58:09\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGRetriever] Retrieved 10 chunks\n",
      "        \u001b[1;32mINFO: Tool \u001b[0m\u001b[1;32m'web_search'\u001b[0m\u001b[1;32m Tool Return: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'query'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m'The Missing Middle Layer: Why LLM Systems Need Tool Routers, Not Bigger Models'\u001b[0m\u001b[1;32m, 'summary\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "\u001b[2m     ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m933s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m2.\u001b[0m\u001b[2m014s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mresearch\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing End!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmanager\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m790s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;92mTrue\u001b[0m\n",
      "\u001b[1;4;31mError executing node outline: CUDA out of memory. Tried to allocate \u001b[0m\u001b[1;4;31m158.00\u001b[0m\u001b[1;4;31m MiB. \u001b[0m\n",
      "\u001b[1;4;31mGPU \u001b[0m\u001b[1;4;31m0\u001b[0m\u001b[1;4;31m has a total capacity of \u001b[0m\u001b[1;4;31m11.61\u001b[0m\u001b[1;4;31m GiB of which \u001b[0m\u001b[1;4;31m120.00\u001b[0m\u001b[1;4;31m MiB is free. Including \u001b[0m\n",
      "\u001b[1;4;31mnon-PyTorch memory, this process has \u001b[0m\u001b[1;4;31m10.70\u001b[0m\u001b[1;4;31m GiB memory in use. Of the allocated \u001b[0m\n",
      "\u001b[1;4;31mmemory \u001b[0m\u001b[1;4;31m9.91\u001b[0m\u001b[1;4;31m GiB is allocated by PyTorch, and \u001b[0m\u001b[1;4;31m484.27\u001b[0m\u001b[1;4;31m MiB is reserved by PyTorch \u001b[0m\n",
      "\u001b[1;4;31mbut unallocated. If reserved but unallocated memory is large try setting \u001b[0m\n",
      "\u001b[1;4;31mPYTORCH_CUDA_ALLOC_CONF\u001b[0m\u001b[1;4;31m=\u001b[0m\u001b[1;4;31mexpandable_segments\u001b[0m\u001b[1;4;31m:\u001b[0m\u001b[1;3;4;31mTrue\u001b[0m\u001b[1;4;31m to avoid fragmentation.  See \u001b[0m\n",
      "\u001b[1;4;31mdocumentation for Memory Management  \u001b[0m\n",
      "\u001b[1;4;31m(\u001b[0m\u001b[1;4;31mhttps://pytorch.org/docs/stable/notes/cuda.html#environment-variables\u001b[0m\u001b[1;4;31m)\u001b[0m\n",
      "\u001b[1;4;33mReturning partial results from executed nodes\u001b[0m\u001b[1;4;33m...\u001b[0m\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 14:58:10\u001b[0m | \u001b[96mexport.py:export_single_node\u001b[0m | Exported node research output to exports/research_20251118_145807.json\n"
     ]
    }
   ],
   "source": [
    "from neurosurfer.models.chat_models.base import BaseChatModel\n",
    "from neurosurfer.agents.graph import GraphAgent, ManagerConfig\n",
    "\n",
    "graph_agent = GraphAgent(\n",
    "    llm=LLM,\n",
    "    graph_yaml=\"blog_workflow.yml\",\n",
    "    toolkit=toolkit,\n",
    "    manager_config=ManagerConfig(\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=4096,\n",
    "    ),\n",
    "    manager_llm=LLM,\n",
    "    log_traces=True\n",
    ")\n",
    "\n",
    "# Run workflow\n",
    "graph_inputs = {\n",
    "    \"topic_title\": \"The Missing Middle Layer: Why LLM Systems Need Tool Routers, Not Bigger Models\",\n",
    "    \"query\": \"Compose a 2000-2500 word blog on why tool-routing layers matter more than scaling LLM size, covering practical design patterns, examples, and tradeoffs.\",\n",
    "    \"audience\": \"Intermediate ML engineers\",\n",
    "    \"tone\": \"Practical and slightly opinionated\",\n",
    "}\n",
    "\n",
    "results = graph_agent.run(inputs=graph_inputs)\n",
    "# result = await run_async(executor.run(inputs=graph_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205b51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6cb6796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"graph_agent_results.json\", \"w\") as writer:\n",
    "    json.dump(results.model_dump(), writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "303ddfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Review of Draft: \"Understanding Tool-Augmented LLM Agents: Architecture, Workflow, and Best Practices\"\n",
      "\n",
      "---\n",
      "\n",
      "## ‚úÖ **Strengths**\n",
      "\n",
      "- **Comprehensive Overview**: The draft provides a well-structured overview of tool-augmented LLM agents, covering key components like architecture, workflow, and best practices.\n",
      "- **Clear Terminology**: The use of consistent terminology (e.g., \"tool-augmented agents\", \"agent loop\") helps maintain clarity.\n",
      "- **Practical Focus**: The inclusion of best practices and workflow diagrams adds practical value for developers and researchers.\n",
      "- **Up-to-Date References**: The draft references recent advancements in LLM agent systems, such as the use of retrieval-augmented generation (RAG) and tool integration.\n",
      "\n",
      "---\n",
      "\n",
      "## ‚ùå **Issues and Concerns**\n",
      "\n",
      "### 1. **Technical Inaccuracies**\n",
      "\n",
      "- **Overgeneralization of Tool Integration**: The draft refers to \"tools\" in a broad sense, but does not clearly distinguish between different types of tools (e.g., API-based, database, or external services). This may lead to confusion about what constitutes a \"tool\" in this context.\n",
      "- **Ambiguity in Agent Loop**: The description of the \"agent loop\" is somewhat vague. It mentions \"planning, execution, and feedback\" but lacks a clear sequence or explanation of how the agent decides when to use a tool.\n",
      "- **Misrepresentation of LLM Behavior**: The draft suggests that LLMs \"can autonomously choose when to use tools,\" which may not be accurate. In practice, the decision to use a tool is often guided by the agent's design, not the LLM itself, unless explicitly programmed with a tool-use policy.\n",
      "\n",
      "### 2. **Missing Explanations**\n",
      "\n",
      "- **Lack of Context on Tool Selection**: The draft does not explain how tools are selected or prioritized in an agent system. This is a critical aspect of agent design and should be elaborated.\n",
      "- **Insufficient Coverage of Tool Limitations**: The draft briefly mentions tool limitations but does not delve into common issues such as latency, API rate limits, or error handling, which are important for real-world deployment.\n",
      "- **No Mention of Evaluation Metrics**: There is no discussion of how to evaluate the performance of tool-augmented agents, such as accuracy, efficiency, or robustness.\n",
      "\n",
      "### 3. **Stylistic and Clarity Issues**\n",
      "\n",
      "- **Repetition of Concepts**: Some sections repeat similar ideas, which can be streamlined to improve readability.\n",
      "- **Lack of Visual Aids**: While the draft mentions diagrams, it does not include them. Including visual aids (e.g., architecture diagrams or workflow charts) would greatly enhance understanding.\n",
      "- **Ambiguous Terminology**: Terms like \"tool-augmented\" are used without a clear definition, which may confuse readers unfamiliar with the concept.\n",
      "\n",
      "---\n",
      "\n",
      "## üõ†Ô∏è **Recommendations**\n",
      "\n",
      "### 1. **Clarify the Role of Tools in Agent Systems**\n",
      "- Define what constitutes a \"tool\" in the context of LLM agents.\n",
      "- Differentiate between internal and external tools, and explain how they are integrated into the agent's decision-making process.\n",
      "\n",
      "### 2. **Elaborate on the Agent Loop and Tool Integration**\n",
      "- Provide a step-by-step explanation of the agent loop, including how the LLM decides to use a tool.\n",
      "- Include examples of how tools are invoked and how their outputs are processed.\n",
      "\n",
      "### 3. **Add a Section on Tool Selection and Prioritization**\n",
      "- Discuss strategies for selecting and prioritizing tools (e.g., based on relevance, reliability, or cost).\n",
      "- Explain how agents handle tool limitations and errors.\n",
      "\n",
      "### 4. **Include Evaluation and Performance Metrics**\n",
      "- Introduce common evaluation metrics for tool-augmented agents, such as task success rate, response time, and error recovery.\n",
      "- Provide examples of real-world use cases where these metrics are applied.\n",
      "\n",
      "### 5. **Incorporate Visual Aids**\n",
      "- Add diagrams to illustrate the architecture and workflow of tool-augmented agents.\n",
      "- Consider including a comparison table of different tool integration strategies.\n",
      "\n",
      "### 6. **Improve Clarity and Avoid Ambiguity**\n",
      "- Define key terms like \"tool-augmented\" and \"agent loop\" at the beginning or in a dedicated glossary.\n",
      "- Avoid vague statements like \"LLMs can autonomously choose when to use tools\" and instead clarify that this behavior is guided by the agent's design and policy.\n",
      "\n",
      "---\n",
      "\n",
      "## ‚úÖ **Final Thoughts**\n",
      "\n",
      "The draft provides a solid foundation for understanding tool-augmented LLM agents, but it would benefit from greater technical precision, clearer explanations, and more structured content. With the suggested improvements, it could serve as a valuable resource for both researchers and practitioners in the field of LLM agent development.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(results[\"results\"][\"review\"].raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a795f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentConfig(allow_input_pruning=True, repair_with_llm=True, strict_tool_call=False, temperature=0.7, max_new_tokens=512, return_stream_by_default=False, retry=RouterRetryPolicy(max_route_retries=2, max_tool_retries=1, backoff_sec=0.7), strict_json=True, max_repair_attempts=1)\n",
      "retries=None timeout_s=None budget=NodeBudget(max_new_tokens=None, temperature=1.2, return_stream_by_default=None) allow_input_pruning=None repair_with_llm=None strict_tool_call=None strict_json=None max_repair_attempts=None\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class RouterRetryPolicy:\n",
    "    \"\"\"Retry tuning for routing + tool execution.\"\"\"\n",
    "    max_route_retries: int = 2\n",
    "    max_tool_retries: int = 1\n",
    "    backoff_sec: float = 0.7  # linear backoff\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    \"\"\"\n",
    "    Top-level configuration for the Agent.\n",
    "    \"\"\"\n",
    "    # Routing:\n",
    "    allow_input_pruning: bool = True    # drop unknown inputs not in ToolSpec\n",
    "    repair_with_llm: bool = True        # ask LLM to repair invalid routing/inputs\n",
    "    strict_tool_call: bool = False      # router must output JSON; else can answer in plain text\n",
    "    # synonyms: Dict[str, Dict[str, str]] = field(default_factory=dict)  # field -> {from: to}\n",
    "\n",
    "    # LLM defaults:\n",
    "    temperature: float = 0.7\n",
    "    max_new_tokens: int = 512\n",
    "    return_stream_by_default: bool = False\n",
    "\n",
    "    # Retries:\n",
    "    retry: RouterRetryPolicy = field(default_factory=RouterRetryPolicy)\n",
    "\n",
    "    # Structured-output options:\n",
    "    strict_json: bool = True                  # enforce RFC 8259 JSON\n",
    "    max_repair_attempts: int = 1              # for malformed JSON repairs\n",
    "\n",
    "\n",
    "\n",
    "class NodeBudget(BaseChatModel):\n",
    "    \"\"\"\n",
    "    Budget / LLM-related overrides per node.\n",
    "\n",
    "    These map directly to AgentConfig fields:\n",
    "        - temperature      -> AgentConfig.temperature\n",
    "        - max_new_tokens   -> AgentConfig.max_new_tokens\n",
    "        - return_stream_by_default -> AgentConfig.return_stream_by_default\n",
    "    \"\"\"\n",
    "\n",
    "    max_new_tokens: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=\"Override AgentConfig.max_new_tokens for this node only.\",\n",
    "    )\n",
    "    temperature: Optional[float] = Field(\n",
    "        default=None,\n",
    "        description=\"Override AgentConfig.temperature for this node only.\",\n",
    "    )\n",
    "    return_stream_by_default: Optional[bool] = Field(\n",
    "        default=None,\n",
    "        description=\"Override AgentConfig.return_stream_by_default for this node only.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class NodePolicy(BaseChatModel):\n",
    "    \"\"\"\n",
    "    Per-node policy that can override some AgentConfig settings and add\n",
    "    node-level execution constraints (e.g., timeout).\n",
    "\n",
    "    YAML example:\n",
    "\n",
    "        nodes:\n",
    "          - id: research\n",
    "            policy:\n",
    "              retries: 1\n",
    "              timeout_s: 30\n",
    "              budget:\n",
    "                max_new_tokens: 180\n",
    "                temperature: 0.2\n",
    "              allow_input_pruning: false\n",
    "              repair_with_llm: true\n",
    "              strict_tool_call: true\n",
    "    \"\"\"\n",
    "\n",
    "    retries: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=\"Override AgentConfig.retry.max_route_retries for this node.\",\n",
    "    )\n",
    "    timeout_s: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Soft timeout for this node in seconds. Execution isn't forcibly \"\n",
    "            \"cancelled but the node will be marked as errored if exceeded.\"\n",
    "        ),\n",
    "    )\n",
    "    budget: Optional[NodeBudget] = None\n",
    "\n",
    "    # Direct AgentConfig-like overrides\n",
    "    allow_input_pruning: Optional[bool] = None\n",
    "    repair_with_llm: Optional[bool] = None\n",
    "    strict_tool_call: Optional[bool] = None\n",
    "    strict_json: Optional[bool] = None\n",
    "    max_repair_attempts: Optional[int] = None\n",
    "\n",
    "    class Config:\n",
    "        extra = \"ignore\"  # ignore unknown keys under 'policy'\n",
    "\n",
    "c = AgentConfig()\n",
    "\n",
    "p = NodePolicy(budget=NodeBudget(temperature=1.2))\n",
    "\n",
    "print(c)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93052a6d",
   "metadata": {},
   "source": [
    "### Python API version (no YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "926be727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: True\n",
      "Answer:\n",
      " The calculator result for your request is ${compute.text}. This means that after performing the calculation based on your input, the final answer is ${compute.text}. Let me know if you need further assistance!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from graph import Graph, Node, NodePolicy, GraphConfig, GraphExecutor\n",
    "from neurosurfer.tools import Toolkit\n",
    "from neurosurfer.models.chat_models.openai import OpenAIModel\n",
    "\n",
    "# Reuse your existing toolkit + model\n",
    "llm = LLM  # already created in your environment\n",
    "tk = toolkit\n",
    "\n",
    "graph = Graph(\n",
    "    name=\"calc_and_explain\",\n",
    "    config=GraphConfig(max_concurrency=2),\n",
    "    inputs_schema={\"prompt\": str},\n",
    "    nodes=[\n",
    "        Node(\n",
    "            id=\"rewrite\",\n",
    "            fn=\"general_query_assistant\",  # adjust name if needed\n",
    "            inputs={\n",
    "                # swap \"query\" -> \"prompt\" if your tool expects \"prompt\"\n",
    "                \"query\": (\n",
    "                    \"You will receive a user request. Extract a SINGLE pure arithmetic expression that can be \"\n",
    "                    \"evaluated by a calculator (e.g., '(42 * 7) - 5^2' or '0.035 * 12000').\\n\"\n",
    "                    \"- Do NOT include explanations.\\n\"\n",
    "                    \"- Return ONLY the expression as plain text.\\n\\n\"\n",
    "                    \"User request:\\n${inputs.prompt}\"\n",
    "                )\n",
    "            },\n",
    "            outputs=[\"num1\", \"num2\", \"operation\"],\n",
    "            policy=NodePolicy(\n",
    "                retries=1,\n",
    "                timeout_s=30,\n",
    "                budget={\"max_new_tokens\": 128, \"temperature\": 0.1},\n",
    "            ),\n",
    "        ),\n",
    "        Node(\n",
    "            id=\"compute\",\n",
    "            fn=\"calculator\",\n",
    "            inputs={\"num1\": \"${rewrite.num1}\", \"num2\": \"${rewrite.num2}\", \"operation\": \"${rewrite.operation}\"},\n",
    "            outputs=[\"text\"],\n",
    "            policy=NodePolicy(retries=0, timeout_s=15),\n",
    "        ),\n",
    "        Node(\n",
    "            id=\"explain\",\n",
    "            fn=\"general_query_assistant\",\n",
    "            inputs={\n",
    "                \"query\": (\n",
    "                    \"Original request: ${inputs.prompt}\\n\"\n",
    "                    \"Calculator result: ${compute.text}\\n\\n\"\n",
    "                    \"Write a brief, user-friendly explanation of the result (one short paragraph).\"\n",
    "                )\n",
    "            },\n",
    "            outputs=[\"text\"],\n",
    "            policy=NodePolicy(\n",
    "                retries=1,\n",
    "                timeout_s=30,\n",
    "                budget={\"max_new_tokens\": 180, \"temperature\": 0.2},\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    outputs={\"answer\": \"${explain.text}\"},\n",
    ")\n",
    "\n",
    "executor = GraphExecutor(llm=llm, toolkit=tk, max_concurrency=2)\n",
    "\n",
    "result = await run_async(\n",
    "    executor.run(graph, inputs={\"prompt\": \"Compute 3.5% of 12000 and explain\"}, stream=True)\n",
    ")\n",
    "\n",
    "print(\"OK:\", result.ok)\n",
    "print(\"Answer:\\n\", result.outputs[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e25a8",
   "metadata": {},
   "source": [
    "### Planner-based path (using the YAML as a skeleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5250b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, tempfile, pathlib\n",
    "from graph import PlannerAgent, FlowLoader, GraphExecutor\n",
    "\n",
    "# 1) Write the YAML to a temp file (only for this demo)\n",
    "yaml_text = r\"\"\"\n",
    "name: calc_and_explain\n",
    "inputs:\n",
    "  prompt: str\n",
    "config:\n",
    "  max_concurrency: 2\n",
    "nodes:\n",
    "  - id: rewrite\n",
    "    kind: task\n",
    "    fn: general_query_assistant\n",
    "    inputs:\n",
    "      query: |\n",
    "        You will receive a user request. Extract a SINGLE pure arithmetic expression that can be\n",
    "        evaluated by a calculator (e.g., \"(42 * 7) - 5^2\" or \"0.035 * 12000\").\n",
    "        - Do NOT include explanations.\n",
    "        - Return ONLY the expression as plain text.\n",
    "\n",
    "        User request:\n",
    "        ${inputs.prompt}\n",
    "    outputs: [\"text\"]\n",
    "    policy: { retries: 1, timeout_s: 30, budget: { max_new_tokens: 128, temperature: 0.1 } }\n",
    "\n",
    "  - id: compute\n",
    "    kind: task\n",
    "    fn: calculator\n",
    "    inputs: { expression: ${rewrite.text} }\n",
    "    outputs: [\"text\"]\n",
    "\n",
    "  - id: explain\n",
    "    kind: task\n",
    "    fn: general_query_assistant\n",
    "    inputs:\n",
    "      query: |\n",
    "        Original request: ${inputs.prompt}\n",
    "        Calculator result: ${compute.text}\n",
    "\n",
    "        Write a brief, user-friendly explanation of the result (one short paragraph).\n",
    "    outputs: [\"text\"]\n",
    "    policy: { retries: 1, timeout_s: 30, budget: { max_new_tokens: 180, temperature: 0.2 } }\n",
    "\n",
    "outputs: { answer: ${explain.text} }\n",
    "\"\"\".strip()\n",
    "\n",
    "tmp = pathlib.Path(tempfile.gettempdir()) / \"calc_and_explain.yml\"\n",
    "tmp.write_text(yaml_text)\n",
    "\n",
    "# 2) Use the planner with a skeleton (so it returns your YAML-based Graph)\n",
    "planner = PlannerAgent(llm=LLM)  # LLM not used when skeleton is set\n",
    "graph = planner.plan_from_query(query=\"Compute 3.5% of 12000 and explain\", skeleton=str(tmp))\n",
    "\n",
    "# 3) Execute\n",
    "executor = GraphExecutor(llm=LLM, toolkit=toolkit, max_concurrency=2)\n",
    "result = asyncio.run(executor.run(graph, inputs={\"prompt\": \"Compute 3.5% of 12000 and explain\"}))\n",
    "\n",
    "print(\"OK:\", result.ok)\n",
    "print(result.outputs[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225587f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1f9ca96",
   "metadata": {},
   "source": [
    "Test ToolsRouterAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98d83d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:32\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Using tool: calculator\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:32\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Raw inputs: {'num1': 20.0, 'num2': 90.0, 'operation': 'multiply'}\n",
      "1800.0"
     ]
    }
   ],
   "source": [
    "query = \"Perform the calculation 20 * 90\"\n",
    "\n",
    "for chunk in tools_router_agent.run(query, temperature=0.7, max_new_tokens=4000):\n",
    "    print(chunk, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c6635c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:33\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Using tool: general_query_assistant\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:33\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Raw inputs: {'query': 'Tell me a light-hearted joke!'}\n",
      "Why don't skeletons fight each other? They don't have the guts!None"
     ]
    }
   ],
   "source": [
    "query = \"Tell me a light-hearted joke!\"\n",
    "\n",
    "for chunk in tools_router_agent.run(query, temperature=0.7, max_new_tokens=4000):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafe29f",
   "metadata": {},
   "source": [
    "## ReactAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aa87f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[üß†] Chain of Thoughts...\n",
      "Thought: I will first calculate 300 - 300 using the calculator tool, and then I will use the general_query_assistant tool to find a light-hearted joke about the result.\n",
      "\n",
      "Action: {\n",
      "  \"tool\": \"calculator\",\n",
      "  \"inputs\": {\n",
      "    \"num1\": 300,\n",
      "    \"num2\": 300,\n",
      "    \"operation\": \"subtract\"\n",
      "  },\n",
      "  \"final_answer\": false\n",
      "}"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>üîß<span style=\"font-weight: bold\">]</span> Tool: calculator\n",
       "<span style=\"font-weight: bold\">[</span>üì§<span style=\"font-weight: bold\">]</span> Inputs: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'num1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'operation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'subtract'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0müîß\u001b[1m]\u001b[0m Tool: calculator\n",
       "\u001b[1m[\u001b[0müì§\u001b[1m]\u001b[0m Inputs: \u001b[1m{\u001b[0m\u001b[32m'num1'\u001b[0m: \u001b[1;36m300\u001b[0m, \u001b[32m'num2'\u001b[0m: \u001b[1;36m300\u001b[0m, \u001b[32m'operation'\u001b[0m: \u001b[32m'subtract'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Observation:</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mObservation:\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[üß†] Chain of Thoughts...\n",
      "Thought: The result of the calculation is 0. Now, I will use the general_query_assistant tool to find a light-hearted joke about the result.\n",
      "\n",
      "Action: {\n",
      "  \"tool\": \"general_query_assistant\",\n",
      "  \"inputs\": {\n",
      "    \"query\": \"Tell me a light-hearted joke about the number 0.\"\n",
      "  },\n",
      "  \"final_answer\": true\n",
      "}"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>üîß<span style=\"font-weight: bold\">]</span> Tool: general_query_assistant\n",
       "<span style=\"font-weight: bold\">[</span>üì§<span style=\"font-weight: bold\">]</span> Inputs: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Tell me a light-hearted joke about the number 0.'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0müîß\u001b[1m]\u001b[0m Tool: general_query_assistant\n",
       "\u001b[1m[\u001b[0müì§\u001b[1m]\u001b[0m Inputs: \u001b[1m{\u001b[0m\u001b[32m'query'\u001b[0m: \u001b[32m'Tell me a light-hearted joke about the number 0.'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the number 0 break up with the number 8?  \n",
      "Because it found someone more \"8\" (8) than a zero!"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Observation:</span> Why did the number <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> break up with the number <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>?  \n",
       "Because it found someone more <span style=\"color: #008000; text-decoration-color: #008000\">\"8\"</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">)</span> than a zero!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mObservation:\u001b[0m Why did the number \u001b[1;36m0\u001b[0m break up with the number \u001b[1;36m8\u001b[0m?  \n",
       "Because it found someone more \u001b[32m\"8\"\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m than a zero!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[üß†] Chain of Thoughts...\n",
      "Thought: The calculation result is 0, and the joke provided is ready. The final answer is complete.\n",
      "\n",
      "Final Answer: The result of 300 - 300 is 0. Here's a light-hearted joke about it: Why did the number 0 break up with the number 8? Because it found someone more \"8\" (8) than a zero!"
     ]
    }
   ],
   "source": [
    "from neurosurfer.agents.react import ReActAgent, ReActConfig\n",
    "\n",
    "react_agent = ReActAgent(\n",
    "    toolkit=toolkit,\n",
    "    llm=LLM,\n",
    "    specific_instructions=\"Always be concise in your answers. Break the task into steps if needed.\",\n",
    "    config=ReActConfig(\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=4096,\n",
    "        allow_input_pruning=True,\n",
    "        repair_with_llm=True,\n",
    "        skip_special_tokens=True,\n",
    "        verbose=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# print(react_agent._system_prompt())\n",
    "TASK = \"\"\"Calculate 300 - 300. Then tell me a light-hearted joke about that result.\"\"\"\n",
    "\n",
    "for chunk in react_agent.run(TASK):\n",
    "    print(chunk, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41580e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c7a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1d90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766512d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad2c2b-4a95-4ac0-9727-4c746e97a629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ea014-d67c-4af9-b2f6-1a8d80d63b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
