{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b35214-7a1f-403c-9a7a-b42e481ab431",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# Go up one directory from `b/` to project root\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affc4a2e-d532-4dee-8b50-cab72fd229c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë                                                                  ‚ïë\n",
      "‚ïë ‚ñì‚ñì‚ñì‚ñì‚ñì   ‚ñì‚ñì‚ñì‚ñì                                  ‚ñì‚ñì‚ñì                ‚ïë\n",
      "‚ïë  ‚ñì‚ñì ‚ñì‚ñì   ‚ñì‚ñì  ‚ñì‚ñì‚ñì‚ñì ‚ñì  ‚ñì ‚ñì ‚ñì ‚ñì‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì ‚ñì  ‚ñì ‚ñì ‚ñì  ‚ñì   ‚ñì‚ñì‚ñì‚ñì ‚ñì ‚ñì       ‚ïë\n",
      "‚ïë  ‚ñì‚ñì  ‚ñì‚ñì  ‚ñì‚ñì  ‚ñì‚ñÅ‚ñÅ‚ñì ‚ñì  ‚ñì ‚ñì‚ñì‚ñè ‚ñì  ‚ñì ‚ñì‚ñÅ  ‚ñì  ‚ñì ‚ñì‚ñì‚ñè ‚ñì‚ñì‚ñì  ‚ñì‚ñÅ‚ñÅ‚ñì ‚ñì‚ñì        ‚ïë\n",
      "‚ïë  ‚ñì‚ñì   ‚ñì‚ñì ‚ñì‚ñì  ‚ñì    ‚ñì  ‚ñì ‚ñì   ‚ñì  ‚ñì   ‚ñì ‚ñì  ‚ñì ‚ñì    ‚ñì   ‚ñì    ‚ñì         ‚ïë\n",
      "‚ïë ‚ñì‚ñì‚ñì‚ñì   ‚ñì‚ñì‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì‚ñì ‚ñì   ‚ñì‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì ‚ñì‚ñì‚ñì‚ñì ‚ñì    ‚ñì   ‚ñì‚ñì‚ñì‚ñì ‚ñì         ‚ïë\n",
      "‚ïë ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ‚ïë\n",
      "‚ïë Orchestrate Agents - RAG - SQL Tools - Multi-LLM - FastAPI Ready ‚ïë\n",
      "‚ïë Faster builds, clearer flows, production-first                   ‚ïë\n",
      "‚ïë                                                                  ‚ïë\n",
      "‚ïë Version: 0.1.3 | Python: 3.11.13                                 ‚ïë\n",
      "‚ïë OS: Linux 6.14.0-35-generic (x86_64)                             ‚ïë\n",
      "‚ïë Torch: 2.7.1+cu126   CUDA: yes (12.6)                            ‚ïë\n",
      "‚ïë MPS: no (built: False)                                           ‚ïë\n",
      "‚ïë Transformers: 4.51.3   SentEmb: 5.1.0                            ‚ïë\n",
      "‚ïë Accelerate: 1.10.1   bnb: 0.47.0                                 ‚ïë\n",
      "‚ïë Unsloth: 2025.8.10                                               ‚ïë\n",
      "‚ïë                                                                  ‚ïë\n",
      "‚ïë Detected CUDA devices: NVIDIA GeForce RTX 3080 Ti                ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomi/anaconda3/envs/LLMs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:03:01\u001b[0m | \u001b[96mconfig.py:<module>\u001b[0m | PyTorch version 2.7.1+cu126 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomi/anaconda3/envs/LLMs/lib/python3.11/importlib/__init__.py:126: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 11-20 10:03:02 [__init__.py:241] Automatically detected platform cuda.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:03:04\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Initializing Transformers model.\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-20 10:03:04\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Model is already quantized. Ignoring load_in_4bit=True.\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:03:05\u001b[0m | \u001b[96mmodeling.py:get_balanced_memory\u001b[0m | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:03:14\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Transformers model initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from neurosurfer.models.chat_models.transformers import TransformersModel\n",
    "from neurosurfer import config \n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "DEFAULT_TRANSFORMERS_MODEL_PARAMS = dict({\n",
    "    \"model_name\": \"/home/nomi/workspace/Model_Weights/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"max_seq_length\": 16_000,\n",
    "    \"load_in_4bit\": True,\n",
    "    \"enable_thinking\": False,  # main_gpu interpretation\n",
    "    \"verbose\": False\n",
    "})\n",
    "\n",
    "LOGGER = logging.getLogger()\n",
    "LLM = TransformersModel(\n",
    "    **DEFAULT_TRANSFORMERS_MODEL_PARAMS,\n",
    "    stop_words=[\"Observation:\"],\n",
    "    logger = logging.getLogger(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16ff5798-3ed2-4b38-86cd-2498ca0e57be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't skeletons fight each other?  \n",
      "Because they don't have the *guts*! üòÑ"
     ]
    }
   ],
   "source": [
    "# streaming response example\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "system_prompt = \"You are a joker.\"\n",
    "user_prompt = \"\"\"Tell me a short and light-hearted joke.\"\"\"\n",
    "\n",
    "stream_response = LLM.ask(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=user_prompt,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "md_display = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream_response:\n",
    "    chunk = chunk.choices[0].delta.content or \"\"\n",
    "    print(chunk, flush=True, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808255b3",
   "metadata": {},
   "source": [
    "### Agent Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3030e4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Response:\n",
      "\u001b[1;4;33müß† Thinking\u001b[0m\u001b[1;4;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmain_agent\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\n",
      "\u001b[2m     ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.free_text_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m876s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m877s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmain_agent\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;4;32mFinal response:\u001b[0m\n",
      "AI, or Artificial Intelligence, is the simulation of human intelligence in machines that are programmed to think, learn, and perform tasks typically requiring human cognition.\n"
     ]
    }
   ],
   "source": [
    "# agent normal response\n",
    "from neurosurfer.agents import Agent, AgentConfig\n",
    "# from neurosurfer.tracing import RichTracer\n",
    "from pydantic import BaseModel\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    strict_tool_call=True,\n",
    "    return_stream_by_default=True,\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "agent = Agent(llm=LLM, config=agent_config, log_traces=True)\n",
    "\n",
    "# normal response\n",
    "print(\"Normal Response:\")\n",
    "agent_response = agent.run(user_prompt=\"What is AI (one line)?\", stream=False)\n",
    "# print(agent_response.response)\n",
    "\n",
    "# # streaming response\n",
    "# print(\"\\n\\nStreaming Response:\")\n",
    "# for c in agent.run(user_prompt=\"What are top 3 applications of AI (one line)?\").response:\n",
    "#     print(c, flush=True, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d91b3b",
   "metadata": {},
   "source": [
    "**Structured Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344ffe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-20 10:03:24\u001b[0m | \u001b[96magent.py:run\u001b[0m     | `output_schema` provided with `stream=True`; forcing non-streaming structured output.\n",
      "\u001b[1;4;33müß† Thinking\u001b[0m\u001b[1;4;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmain_agent\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.structured_call.first_pass'\u001b[0m\n",
      "\u001b[2m     ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.structured_call.first_pass'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m5.\u001b[0m\u001b[2m446s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'main_agent'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m5.\u001b[0m\u001b[2m447s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmain_agent\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing End!\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Structured Response:\n",
      "{\n",
      "  \"definition\": \"Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding.\",\n",
      "  \"history\": \"The concept of AI was first introduced in the 1950s. Early developments included the creation of the first AI programs and the establishment of AI as a formal academic discipline. Over the decades, advancements in computing power, data availability, and algorithmic techniques have significantly propelled AI forward.\",\n",
      "  \"modern_frameworks\": \"Modern frameworks such as TensorFlow, PyTorch, and Keras have revolutionized the development and deployment of AI models, making it more accessible and efficient for researchers and developers.\",\n",
      "  \"applications\": [\n",
      "    {\n",
      "      \"title\": \"Healthcare\",\n",
      "      \"description\": \"AI is used in healthcare for diagnostics, personalized treatment plans, and drug discovery.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Finance\",\n",
      "      \"description\": \"AI is applied in finance for fraud detection, algorithmic trading, and risk management.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Autonomous Vehicles\",\n",
      "      \"description\": \"AI powers self-driving cars through computer vision, sensor fusion, and decision-making algorithms.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Structured Response examples\n",
    "class AIApplication(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "\n",
    "class AI(BaseModel):\n",
    "    definition: str\n",
    "    history: str\n",
    "    modern_frameworks: str\n",
    "    applications: list[AIApplication]\n",
    "\n",
    "user_query = \"What is AI and list 3 of its top application, and 3 concerns.\"\n",
    "agent_response = agent.run(user_prompt=user_query, output_schema=AI)\n",
    "\n",
    "print(\"\\n\\nStructured Response:\")\n",
    "print(agent_response.response.json_obj)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b1381",
   "metadata": {},
   "source": [
    "## RAG wiring so the agent ‚Äúunderstands‚Äù the Neurosurf codebase\n",
    "\n",
    "You‚Äôll ingest the repo once, then run a retriever to answer code questions. The Planner can call the retriever first to form a precise implementation plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f5bf38-1a5b-4ad1-be5f-b5073dbeaaf0",
   "metadata": {},
   "source": [
    "### FileReader and Chunker Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a35db287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:08:56\u001b[0m | \u001b[96mSentenceTransformer.py:__init__\u001b[0m | Use pytorch device_name: cuda:0\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:08:56\u001b[0m | \u001b[96msentence_transformer.py:__init__\u001b[0m | SentenceTransformer embedding model initialized.\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-20 10:08:56\u001b[0m | \u001b[96magent.py:__init__\u001b[0m | No vectorstore provided to RAGAgent, using default ChromaVectorStore. Initializing default collection `neurosurfer-rag-agent`\n",
      "[Init] ChromaVectorStore initialized with collection: neurosurfer-rag-agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  6.19it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  9.84it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 15.34it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 16.06it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 17.19it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 15.54it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 17.52it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 15.66it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 16.29it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 13.16it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 15.19it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.30it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 16.97it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 16.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok',\n",
       " 'sources': 141,\n",
       " 'chunks': 889,\n",
       " 'unique_chunks': 889,\n",
       " 'added': 889,\n",
       " 'finished_at': 1763618939.3266816,\n",
       " 'accepted_sources': 1,\n",
       " 'total_docs_in_collection': 889}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from neurosurfer.vectorstores.chroma import ChromaVectorStore\n",
    "from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n",
    "from neurosurfer.agents.rag.chunker import Chunker\n",
    "from neurosurfer.agents.rag.filereader import FileReader\n",
    "from neurosurfer.agents.rag import RAGAgent, RAGAgentConfig, RAGIngestorConfig\n",
    "\n",
    "chunker = Chunker()\n",
    "file_reader = FileReader()\n",
    "embedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\n",
    "\n",
    "rag_agent = RAGAgent(\n",
    "    llm=LLM,\n",
    "    embedder=embedder,\n",
    "    file_reader=file_reader,\n",
    "    chunker=chunker,\n",
    "    config=RAGAgentConfig(\n",
    "        top_k=5,\n",
    "        fixed_max_new_tokens=1024,\n",
    "        clear_collection_on_init=True\n",
    "    ),\n",
    "    ingestor_config=RAGIngestorConfig(\n",
    "        batch_size=64,\n",
    "        max_workers=4,\n",
    "        deduplicate=True,\n",
    "        normalize_embeddings=True,\n",
    "        default_metadata=None,\n",
    "        tmp_dir=\"./rag-storage\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "dir_path = \"../neurosurfer\"\n",
    "summary = rag_agent.ingest(sources=dir_path)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "671e81e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ RetrievalPlan(mode='smart', scope='medium', top_k=15, neighbor_hops=1, notes=None, extra={'scope': 'medium', 'top_k': 15, 'neighbor_hops': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: e3cb346a127a70f3:a1074d30\n",
      "NODE_TOOLS:\n",
      "{tools}\n",
      "GRAPH_INPUTS (as JSON-ish):\n",
      "{graph_inputs}\n",
      "DEPENDENCY_RESULTS (node_id -> result):\n",
      "{dependency_results}\n",
      "PREVIOUS_RESULT (may be empty if none):\n",
      "{prev_txt}\n",
      "Compose the next user_prompt string that this node's agent should receive.\n",
      "Return ONLY that prompt text.\n",
      "\"\"\"\n",
      "DEFAULT_NODE_SYSTEM_TEMPLATE = \"\"\"You are a specialized agent in a larger workflow.\n",
      "Your role:\n",
      "- PURPOSE: {purpose}\n",
      "- GOAL: {goal}\n",
      "- EXPECTED_RESULT: {expected_result}\n",
      "General behaviour:\n",
      "- Be precise and concise unless the task requires extended output.\n",
      "- Use clear structure (headings/bullets) when helpful.\n",
      "- If you are calling tools, interpret their outputs carefully and explain your reasoning.\n",
      "\"\"\"\n",
      "\n",
      "---\n",
      "\n",
      "Source: 0fb16151b97daeb7:d679f9d7\n",
      "                node_config.retry.max_route_retries = p.retries\n",
      "            if p.max_new_tokens is not None:\n",
      "                node_config.max_new_tokens = p.max_new_tokens\n",
      "            if p.temperature is not None:\n",
      "                node_config.temperature = p.temperature\n",
      "        agent_logger = logging.getLogger(f\"neurosurfer.agent.{node.id}\")\n",
      "        agent = Agent(\n",
      "            id=node.id,\n",
      "            llm=self.llm,\n",
      "            toolkit=node_toolkit,\n",
      "            config=node_config,\n",
      "            logger=agent_logger,\n",
      "            tracer=self.tracer,\n",
      "            log_traces=self.log_traces,\n",
      "            verbose=True,\n",
      "        )\n",
      "        self._agents[node.id] = agent\n",
      "        return agent\n",
      "    def _run_node(\n",
      "        self,\n",
      "        *,\n",
      "        node: GraphNode,\n",
      "        graph_inputs: Dict[str, Any],\n",
      "        dependency_results: Dict[str, Any],\n",
      "        previous_result: Any,\n",
      "\n",
      "---\n",
      "\n",
      "Source: 762195c91e7963e6:d679f9d7\n",
      "        \"\"\"Ensure all YAML tool names exist in the master Toolkit.\"\"\"\n",
      "        if self.toolkit is None:\n",
      "            return\n",
      "        missing: set[str] = set()\n",
      "        for node in self.graph.nodes:\n",
      "            for t_name in node.tools:\n",
      "                if t_name not in self.toolkit.registry:\n",
      "                    missing.add(t_name)\n",
      "        if missing:\n",
      "            raise GraphConfigurationError(\n",
      "                f\"YAML refers to unknown tools not registered in Toolkit: {sorted(missing)}\"\n",
      "            )\n",
      "    def _get_agent_for_node(self, node: GraphNode) -> Agent:\n",
      "        \"\"\"Create or reuse an Agent instance for this node.\"\"\"\n",
      "        if node.id in self._agents:\n",
      "            return self._agents[node.id]\n",
      "        # Per-node toolkit: restrict to YAML tools (if any)\n",
      "        node_toolkit: Optional[Toolkit] = None\n",
      "        if self.toolkit is not None and node.tools:\n",
      "            from neurosurfer.tools.toolkit import Toolkit as ToolkitClass\n",
      "            node_toolkit = ToolkitClass()\n",
      "            for name in node.tools:\n",
      "                tool = self.toolkit.registry.get(name)\n",
      "                if tool is None:\n",
      "                    raise GraphConfigurationError(\n",
      "\n",
      "---\n",
      "\n",
      "Source: 6af06ff27cc2ff12:e3c8ef02\n",
      "from .schema import Graph, GraphNode, NodeMode, NodeExecutionResult\n",
      "from .manager import ManagerAgent, ManagerConfig\n",
      "from .executor import GraphExecutor\n",
      "from .loader import load_graph, load_graph_from_dict\n",
      "from .artifacts import ArtifactStore\n",
      "from .agent import GraphAgent\n",
      "__all__ = [\n",
      "    \"GraphAgent\",\n",
      "    \"Graph\",\n",
      "    \"GraphNode\",\n",
      "    \"NodeMode\",\n",
      "    \"NodeExecutionResult\",\n",
      "    \"ManagerAgent\",\n",
      "    \"ManagerConfig\",\n",
      "    \"GraphExecutor\",\n",
      "    \"load_graph\",\n",
      "    \"load_graph_from_dict\",\n",
      "    \"ArtifactStore\",\n",
      "]\n",
      "\n",
      "---\n",
      "\n",
      "Source: e229c71115a0ead1:a1074d30\n",
      "MANAGER_SYSTEM_PROMPT = \"\"\"You are a workflow orchestrator for a multi-agent system.\n",
      "At each step you must compose ONE concise, well-structured `user_prompt` string\n",
      "for the next node in the graph.\n",
      "You receive:\n",
      "- The node's PURPOSE, GOAL, EXPECTED_RESULT, and allowed TOOLS.\n",
      "- The original GRAPH_INPUTS (user-provided input to the workflow).\n",
      "- The DEPENDENCY_RESULTS: outputs from all prerequisite nodes.\n",
      "- The PREVIOUS_RESULT: output from the immediately previous node (may be null).\n",
      "Your job:\n",
      "- Focus on the node's GOAL and EXPECTED_RESULT.\n",
      "- Provide all relevant context from GRAPH_INPUTS and DEPENDENCY_RESULTS.\n",
      "- If TOOLS are available, phrase the prompt to encourage sensible tool usage.\n",
      "- Do NOT describe the graph, nodes, or internal orchestration.\n",
      "- Speak directly to the agent as if it were a normal LLM prompt.\n",
      "Output:\n",
      "- Return ONLY the text for the `user_prompt`. No markdown fences, no JSON.\n",
      "\"\"\"\n",
      "COMPOSE_NEXT_AGENT_PROMPT_TEMPLATE = \"\"\"You are preparing a prompt for the next agent in a workflow.\n",
      "NODE_ID: {node_id}\n",
      "NODE_PURPOSE: {purpose}\n",
      "NODE_GOAL: {goal}\n",
      "NODE_EXPECTED_RESULT: {expected}\n",
      "NODE_TOOLS:\n",
      "{tools}\n",
      "GRAPH_INPUTS (as JSON-ish):\n",
      "\n",
      "---\n",
      "\n",
      "Source: 765e0103c0302234:d679f9d7\n",
      "        graph_inputs: Dict[str, Any],\n",
      "        dependency_results: Dict[str, Any],\n",
      "        previous_result: Any,\n",
      "        manager_temperature: float,\n",
      "        manager_max_new_tokens: int,\n",
      "    ) -> NodeExecutionResult:\n",
      "        agent = self._get_agent_for_node(node)\n",
      "        user_prompt = self.manager.compose_user_prompt(\n",
      "            node=node,\n",
      "            graph_inputs=graph_inputs,\n",
      "            dependency_results=dependency_results,\n",
      "            previous_result=previous_result,\n",
      "            temperature=manager_temperature,\n",
      "            max_new_tokens=manager_max_new_tokens,\n",
      "        )\n",
      "        system_prompt = self._build_system_prompt(node, graph_inputs)\n",
      "        output_schema = self._load_output_schema_if_needed(node)\n",
      "        started_at = time.time()\n",
      "        timeout_s = node.policy.timeout_s if node.policy and node.policy.timeout_s else None\n",
      "        try:\n",
      "            run_kwargs: Dict[str, Any] = {\n",
      "                \"system_prompt\": system_prompt,\n",
      "                \"user_prompt\": user_prompt,\n",
      "                \"temperature\": None,       # per-node AgentConfig handles defaults\n",
      "                \"max_new_tokens\": None,\n",
      "\n",
      "---\n",
      "\n",
      "Source: b976f6015ab5ac2b:5044a1a7\n",
      "        manager_config:\n",
      "            Configuration for ManagerAgent. If None, a default ManagerConfig()\n",
      "            is created.\n",
      "        tracer:\n",
      "            Optional Tracer instance used inside GraphExecutor/Agents.\n",
      "        artifact_store:\n",
      "            Optional ArtifactStore for node outputs.\n",
      "        logger:\n",
      "            Optional logger. If None, a default namespaced logger is used.\n",
      "        log_traces:\n",
      "            Whether node-level agents should log traces through the tracer.\n",
      "        \"\"\"\n",
      "        self.logger = logger or logging.getLogger(\"neurosurfer.agents.GraphAgent\")\n",
      "        # Resolve graph spec with YAML taking precedence\n",
      "        if graph_yaml is not None:\n",
      "            if graph is not None:\n",
      "                self.logger.warning(\n",
      "                    \"Both `graph_yaml` and `graph` provided; \"\n",
      "                    \"using `graph_yaml` and ignoring `graph`.\"\n",
      "                )\n",
      "            graph = load_graph(str(graph_yaml))\n",
      "        if graph is None:\n",
      "            raise ValueError(\"Either `graph_yaml` or `graph` must be provided.\")\n",
      "        if manager_config is None:\n",
      "            manager_config = ManagerConfig()\n",
      "\n",
      "---\n",
      "\n",
      "Source: 1264848542eb2043:5044a1a7\n",
      "from __future__ import annotations\n",
      "import asyncio\n",
      "import logging\n",
      "from pathlib import Path\n",
      "from typing import Any, Optional, Union\n",
      "from neurosurfer.models.chat_models.base import BaseChatModel\n",
      "from neurosurfer.tools import Toolkit\n",
      "from neurosurfer.tracing import Tracer\n",
      "from .schema import Graph, GraphExecutionResult\n",
      "from .executor import GraphExecutor   # the class you showed above\n",
      "from .manager import ManagerConfig\n",
      "from .artifacts import ArtifactStore\n",
      "from .loader import load_graph         # or wherever your loader lives\n",
      "from .export import export\n",
      "class GraphAgent:\n",
      "    \"\"\"\n",
      "    High-level agent wrapper around `GraphExecutor`.\n",
      "    Users can think of this as:\n",
      "      - \"Use a normal Agent for single-step reasoning\"\n",
      "      - \"Use GraphAgent when you want a DAG/flow of Agents\"\n",
      "    Typical usage\n",
      "    -------------\n",
      "        agent = GraphAgent(\n",
      "            graph_yaml=\"blog_workflow.yml\",\n",
      "            llm=LLM,\n",
      "\n",
      "---\n",
      "\n",
      "Source: 496685defe6166b1:d679f9d7\n",
      "        obj = import_string(node.output_schema)\n",
      "        if not isinstance(obj, type) or not issubclass(obj, PydModel):\n",
      "            raise GraphConfigurationError(\n",
      "                f\"output_schema {node.output_schema!r} is not a Pydantic model\"\n",
      "            )\n",
      "        return obj\n",
      "    def _normalize_agent_output(\n",
      "        self, result: Any\n",
      "    ) -> Tuple[Any, Optional[StructuredResponse], Optional[ToolCallResponse]]:\n",
      "        \"\"\"\n",
      "        Collapse the variety of Agent.run outputs into a single representation.\n",
      "        \"\"\"\n",
      "        structured: Optional[StructuredResponse] = None\n",
      "        tool_call: Optional[ToolCallResponse] = None\n",
      "        raw: Any = result\n",
      "        if isinstance(result, StructuredResponse):\n",
      "            structured = result\n",
      "            if result.parsed_output is not None:\n",
      "                raw = result.parsed_output\n",
      "            elif result.json_obj is not None:\n",
      "                raw = result.json_obj\n",
      "            else:\n",
      "                raw = result.model_response\n",
      "        elif isinstance(result, ToolCallResponse):\n",
      "            tool_call = result\n",
      "\n",
      "---\n",
      "\n",
      "Source: b1f346f2b241984a:5044a1a7\n",
      "        agent = GraphAgent(\n",
      "            graph_yaml=\"blog_workflow.yml\",\n",
      "            llm=LLM,\n",
      "            toolkit=toolkit,\n",
      "        )\n",
      "        result = agent.run(\n",
      "            inputs={\n",
      "                \"topic_title\": \"...\",\n",
      "                \"query\": \"...\",\n",
      "                \"audience\": \"...\",\n",
      "                \"tone\": \"...\",\n",
      "            }\n",
      "        )\n",
      "    \"\"\"\n",
      "    def __init__(\n",
      "        self,\n",
      "        *,\n",
      "        llm: BaseChatModel,\n",
      "        graph_yaml: Optional[Union[str, Path]] = None,\n",
      "        graph: Optional[Graph] = None,\n",
      "        toolkit: Optional[Toolkit] = None,\n",
      "        manager_llm: Optional[BaseChatModel] = None,\n",
      "        manager_config: Optional[ManagerConfig] = None,\n",
      "        tracer: Optional[Tracer] = None,\n",
      "        artifact_store: Optional[ArtifactStore] = None,\n",
      "\n",
      "---\n",
      "\n",
      "Source: 61314491cc260e30:4ec956d4\n",
      "    \"\"\"\n",
      "    Unified, simple tracing class.\n",
      "    - Construct once per agent / graph run.\n",
      "    - Use `with tracer.step(...):` around any operation you want to trace.\n",
      "    - At the end, read `tracer.results` and attach it to your AgentResult.\n",
      "    Design goals:\n",
      "      - Zero change in calling code when tracing is disabled.\n",
      "      - Minimal boilerplate at call site.\n",
      "      - Flexible `add(...)` method to store whatever you want.\n",
      "    \"\"\"\n",
      "    def __init__(\n",
      "        self,\n",
      "        *,\n",
      "        enabled: bool = True,\n",
      "        log_steps: bool = False,\n",
      "        span_tracer: Optional[SpanTracer] = None,\n",
      "        meta: Optional[Dict[str, Any]] = None,\n",
      "        logger: Optional[logging.Logger] = None,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            enabled:\n",
      "                If False, `step(...)` returns a no-op context manager. Your\n",
      "                code stays the same, but nothing is recorded.\n",
      "            log_steps:\n",
      "\n",
      "---\n",
      "\n",
      "Source: 52753cb3966b2b5d:ef0661f7\n",
      "    \"\"\"\n",
      "    enabled: bool = True\n",
      "    log_steps: bool = False\n",
      "    max_output_preview_chars: int = 4000\n",
      "class Tracer:\n",
      "    \"\"\"\n",
      "    Unified, simple tracing class for agents / graphs.\n",
      "    Usage example inside an Agent:\n",
      "        class Agent:\n",
      "            def __init__(..., enable_tracing: bool = False, log_tracing_steps: bool = False):\n",
      "                self.tracer = Tracer(\n",
      "                    config=TracerConfig(\n",
      "                        enabled=enable_tracing,\n",
      "                        log_steps=log_tracing_steps,\n",
      "                    ),\n",
      "                    span_tracer=RichTracer() if log_tracing_steps else NullSpanTracer(),\n",
      "                    meta={\"agent_type\": \"generic_agent\"},\n",
      "                )\n",
      "            def run(...):\n",
      "                inputs = {...}\n",
      "                with self.tracer.step(\n",
      "                    kind=\"llm\",\n",
      "                    label=\"agent.llm.ask\",\n",
      "                    inputs=inputs,\n",
      "                    node_id=context.get(\"node_id\"),\n",
      "\n",
      "---\n",
      "\n",
      "Source: 688debd29780c248:d679f9d7\n",
      "      - A `ManagerAgent` (by default using the same `llm`) to compose inter-node prompts.\n",
      "    Users only need:\n",
      "      - YAML flow (Graph)\n",
      "      - `llm` instance\n",
      "      - `toolkit` instance\n",
      "    Features:\n",
      "      - Per-node policy (NodePolicy) allowing AgentConfig-like overrides:\n",
      "          * budget: max_new_tokens, temperature, return_stream_by_default\n",
      "          * retries: override AgentConfig.retry.max_route_retries\n",
      "          * timeout_s: soft node-level timeout flag\n",
      "          * toggles: allow_input_pruning, repair_with_llm, strict_tool_call, etc.\n",
      "      - Top-level graph inputs (`Graph.inputs`) that:\n",
      "          * validate runtime `inputs`\n",
      "          * cast values to expected types\n",
      "          * can be interpolated into node prompts via `{input_name}`.\n",
      "    \"\"\"\n",
      "    def __init__(\n",
      "        self,\n",
      "        graph: Graph,\n",
      "        *,\n",
      "        llm: BaseChatModel,\n",
      "        toolkit: Optional[Toolkit] = None,\n",
      "        manager_llm: Optional[BaseChatModel] = None,\n",
      "        manager_config: Optional[ManagerConfig] = None,\n",
      "        tracer: Optional[Tracer] = None,\n",
      "\n",
      "---\n",
      "\n",
      "Source: 9783a5cbe58bf613:c341a41f\n",
      "from .config import RAGAgentConfig, RetrieveResult, RAGIngestorConfig\n",
      "from .agent import RAGAgent\n",
      "\n",
      "__all__ = [\n",
      "    \"RAGAgent\",\n",
      "    \"RAGAgentConfig\",\n",
      "    \"RAGIngestorConfig\",\n",
      "    \"RetrieveResult\",\n",
      "]\n",
      "\n",
      "---\n",
      "\n",
      "Source: 3fd47daec2b49343:30decb15\n",
      "    Wrapper around Pydantic model construction with version compatibility.\n",
      "    \"\"\"\n",
      "    if hasattr(model, \"model_validate\"):  # Pydantic v2\n",
      "        return model.model_validate(data)  # type: ignore[attr-defined]\n",
      "    # Pydantic v1\n",
      "    return model.parse_obj(data)  # type: ignore[call-arg]\n",
      "def _validate_graph_spec(spec: Graph) -> None:\n",
      "    \"\"\"\n",
      "    Perform semantic validation of a Graph:\n",
      "    - Ensure at least one node exists.\n",
      "    - Ensure all `outputs` refer to existing node IDs.\n",
      "    - Ensure all `depends_on` entries refer to existing node IDs.\n",
      "    - Ensure no node depends on itself.\n",
      "    - Ensure the graph is a DAG (no cycles) via topological sort.\n",
      "    Raises:\n",
      "        GraphConfigurationError on any invalid condition.\n",
      "    \"\"\"\n",
      "    nodes = spec.nodes\n",
      "    if not nodes:\n",
      "        raise GraphConfigurationError(\"Graph must define at least one node.\")\n",
      "    node_ids = {n.id for n in nodes}\n",
      "    # ---- outputs -> valid node IDs ----\n",
      "    unknown_outputs = [out for out in spec.outputs if out not in node_ids]\n",
      "    if unknown_outputs:\n",
      "        raise GraphConfigurationError(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "retrival_results = rag_agent.retrieve(user_query=\"Explain how to use the graph agent and its configurations?\", retrieval_mode=\"smart\")\n",
    "print(retrival_results.context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991e188",
   "metadata": {},
   "source": [
    "**Server RAG Orchestrator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "79d61982-ac42-46b7-8a6b-bc3ed2248907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:30:58\u001b[0m | \u001b[96mdb.py:init_db\u001b[0m    | Database initialized successfully in sqlite:////home/nomi/.cache/Neurosurfer/app-storage/app.db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:30:59\u001b[0m | \u001b[96mSentenceTransformer.py:__init__\u001b[0m | Use pytorch device_name: cuda:0\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:30:59\u001b[0m | \u001b[96msentence_transformer.py:__init__\u001b[0m | SentenceTransformer embedding model initialized.\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-20 10:30:59\u001b[0m | \u001b[96magent.py:__init__\u001b[0m | No vectorstore provided to RAGAgent, using default ChromaVectorStore. Initializing default collection `neurosurfer-rag-agent`\n",
      "[Init] ChromaVectorStore initialized with collection: neurosurfer-rag-agent\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:30:59\u001b[0m | \u001b[96morchestrator.py:_reset_db\u001b[0m | Number of users left: 1\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:30:59\u001b[0m | \u001b[96morchestrator.py:_reset_db\u001b[0m | Number of files left: 0\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:30:59\u001b[0m | \u001b[96morchestrator.py:_reset_db\u001b[0m | Number of threads left: 0\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:30:59\u001b[0m | \u001b[96morchestrator.py:_reset_db\u001b[0m | Number of messages left: 0\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-20 10:30:59\u001b[0m | \u001b[96morchestrator.py:_reset_db\u001b[0m | Number of documents left: 0\n"
     ]
    }
   ],
   "source": [
    "# scripts/index_repo_for_rag.py\n",
    "from pathlib import Path\n",
    "from neurosurfer.vectorstores.chroma import ChromaVectorStore\n",
    "from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n",
    "from neurosurfer.server.services.rag.orchestrator import RAGOrchestrator\n",
    "\n",
    "try:\n",
    "    from neurosurfer.server.db.db import init_db\n",
    "    init_db()\n",
    "except Exception as _e:\n",
    "    print('DB init warning:', _e)\n",
    "    \n",
    "embedder = SentenceTransformerEmbedder(\"intfloat/e5-small-v2\")\n",
    "# vs = ChromaVectorStore(collection_name=\"neurosurf-repo\")\n",
    "RAG = RAGOrchestrator(\n",
    "    embedder=embedder,\n",
    "    gate_llm=LLM,\n",
    "    top_k=10,\n",
    "    verbose=True,\n",
    "    logger=LOGGER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ba6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import base64\n",
    "import os\n",
    "\n",
    "actor_id = 1\n",
    "thread_id = 1\n",
    "user_query = \"Explain the weather tool and write its summary.?\"\n",
    "\n",
    "file_path = \"weather_tool.py\"\n",
    "\n",
    "# os.path.exists(file_path)\n",
    "\n",
    "files: List[Dict[str, Any]] = [{\n",
    "    \"name\": file_path,\n",
    "    \"content\": base64.b64encode(open(file_path, \"rb\").read()).decode(\"utf-8\"),\n",
    "    \"type\": \"text/x-python\",\n",
    "}]\n",
    "\n",
    "# reset db\n",
    "RAG._reset_db()\n",
    "\n",
    "rag_res = RAG.apply(\n",
    "    actor_id=actor_id,\n",
    "    thread_id=thread_id,\n",
    "    user_query=user_query,\n",
    "    files=files,\n",
    ")\n",
    "user_query = rag_res.augmented_query\n",
    "if rag_res.used:\n",
    "    LOGGER.info(f\"[RAG] used context (top_sim={rag_res.meta.get('top_similarity', 0):.3f})\")\n",
    "else:\n",
    "    LOGGER.info(f\"[RAG] no context (reason={rag_res.meta.get('reason')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f3be3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunk in LLM.ask(\n",
    "#     user_prompt=rag_res.augmented_query,\n",
    "#     system_prompt=\"You are a helpful assistant. Provide your answers in a clear and concise manner.\",\n",
    "#     chat_history=[],\n",
    "#     stream=True,\n",
    "# ):\n",
    "#     chunk = chunk.choices[0].delta.content or \"\"\n",
    "#     print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4c8c2",
   "metadata": {},
   "source": [
    "## Server APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9650f6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-21 08:57:37\u001b[0m | \u001b[96m194622058.py:<module>\u001b[0m | GPU Available: True\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-21 08:57:37\u001b[0m | \u001b[96m194622058.py:<module>\u001b[0m | GPU Name: NVIDIA GeForce RTX 3080 Ti\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-21 08:57:37\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Initializing Transformers model.\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-21 08:57:37\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Model is already quantized. Ignoring load_in_4bit=True.\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-21 08:57:37\u001b[0m | \u001b[96mmodeling.py:get_balanced_memory\u001b[0m | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-21 08:57:43\u001b[0m | \u001b[96mtransformers.py:init_model\u001b[0m | Transformers model initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-21 08:57:45\u001b[0m | \u001b[96mSentenceTransformer.py:__init__\u001b[0m | Use pytorch device_name: cuda:0\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-21 08:57:45\u001b[0m | \u001b[96msentence_transformer.py:__init__\u001b[0m | SentenceTransformer embedding model initialized.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from neurosurfer.models.chat_models.transformers import TransformersModel\n",
    "from neurosurfer.models.embedders.sentence_transformer import SentenceTransformerEmbedder\n",
    "from neurosurfer.config import config\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(\"neurosurfer\")\n",
    "try:\n",
    "    import torch\n",
    "    LOGGER.info(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        LOGGER.info(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "except Exception:\n",
    "    LOGGER.warning(\"Torch not found...\")\n",
    "    \n",
    "LLM = TransformersModel(\n",
    "    # model_name=\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    model_name=\"/home/nomi/workspace/Model_Weights/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    max_seq_length=config.base_model.max_seq_length,\n",
    "    load_in_4bit=config.base_model.load_in_4bit,\n",
    "    enable_thinking=config.base_model.enable_thinking,\n",
    "    stop_words=config.base_model.stop_words or [],\n",
    "    logger=LOGGER,\n",
    ")\n",
    "EMBEDDER_MODEL = SentenceTransformerEmbedder(\n",
    "    model_name=\"intfloat/e5-large-v2\", \n",
    "    logger=LOGGER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d42ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-21 08:58:13\u001b[0m | \u001b[96mdb.py:init_db\u001b[0m    | Database initialized successfully in sqlite:////home/nomi/.cache/Neurosurfer/app-storage/app.db\n",
      "\u001b[91mERROR   \u001b[0m | \u001b[90m2025-11-21 08:58:13\u001b[0m | \u001b[96mapp.py:run\u001b[0m       | Failed to run app: asyncio.run() cannot be called from a running event loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/nomi/workspace/neurosurfer/neurosurfer/server/app.py\", line 582, in run\n",
      "    uvicorn.run(\n",
      "  File \"/home/nomi/anaconda3/envs/LLMs/lib/python3.11/site-packages/uvicorn/main.py\", line 580, in run\n",
      "    server.run()\n",
      "  File \"/home/nomi/anaconda3/envs/LLMs/lib/python3.11/site-packages/uvicorn/server.py\", line 67, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nomi/anaconda3/envs/LLMs/lib/python3.11/asyncio/runners.py\", line 186, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: asyncio.run() cannot be called from a running event loop\n",
      "/home/nomi/workspace/neurosurfer/neurosurfer/server/app.py:593: RuntimeWarning: coroutine 'Server.serve' was never awaited\n",
      "  self.logger.error(f\"Failed to run app: {e}\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Generator\n",
    "import os, shutil, logging\n",
    "from neurosurfer.models.chat_models import BaseChatModel as BaseChatModel\n",
    "from neurosurfer.server.app import NeurosurferApp\n",
    "from neurosurfer.server.schemas import ChatCompletionRequest, ChatCompletionResponse, ChatCompletionChunk\n",
    "from neurosurfer.server.runtime import RequestContext\n",
    "from neurosurfer.server.services.rag import RAGOrchestrator\n",
    "\n",
    "from neurosurfer.config import config\n",
    "logging.basicConfig(level=config.app.logs_level.upper())\n",
    "\n",
    "\n",
    "# Create app instance\n",
    "ns = NeurosurferApp(\n",
    "    app_name=config.app.app_name,\n",
    "    api_keys=[],\n",
    "    enable_docs=config.app.enable_docs,\n",
    "    cors_origins=config.app.cors_origins,\n",
    "    host=config.app.host_ip,\n",
    "    port=config.app.host_port,\n",
    "    reload=config.app.reload,\n",
    "    log_level=config.app.logs_level,\n",
    "    workers=config.app.workers\n",
    ")\n",
    "\n",
    "\n",
    "LOGGER: logging.Logger = None\n",
    "RAG: RAGOrchestrator | None = None\n",
    "\n",
    "@ns.on_startup\n",
    "async def load_model():\n",
    "    global EMBEDDER_MODEL, LOGGER, LLM, RAG\n",
    "    # registered models are visible in the UI. You must register a model for it to be available in the UI.\n",
    "    ns.model_registry.add(\n",
    "        llm=LLM,\n",
    "        family=\"llama\",\n",
    "        provider=\"Unsloth\",\n",
    "        description=\"Proxy to Llama\"\n",
    "    )\n",
    "    RAG = RAGOrchestrator(\n",
    "        embedder=EMBEDDER_MODEL,\n",
    "        gate_llm=LLM,\n",
    "        top_k=10,\n",
    "        verbose=True,\n",
    "        logger=LOGGER,\n",
    "    )\n",
    "    # Warmup\n",
    "    joke = LLM.ask(user_prompt=\"Say hi!\", system_prompt=config.base_model.system_prompt, stream=False)\n",
    "    LOGGER.info(f\"LLM ready: {joke.choices[0].message.content}\")\n",
    "\n",
    "@ns.on_shutdown\n",
    "def cleanup():\n",
    "    \"\"\"Clean up temporary files and directories on application shutdown.\"\"\"\n",
    "    pass\n",
    "\n",
    "@ns.chat()\n",
    "def handler(request: ChatCompletionRequest, ctx: RequestContext) -> ChatCompletionResponse | Generator[ChatCompletionChunk, None, None]:\n",
    "    global LLM, RAG\n",
    "    # Resolve actor/thread ids (thread id is part of JSON now)\n",
    "    user_id = ctx.user_id\n",
    "    thread_id = request.thread_id\n",
    "    has_files_message = request.files is not None\n",
    "\n",
    "    # Prepare inputs\n",
    "    user_msgs: List[str] = [m[\"content\"] for m in request.messages if m[\"role\"] == \"user\"]\n",
    "    system_msgs = [m[\"content\"] for m in request.messages if m[\"role\"] == \"system\"]\n",
    "    system_prompt = system_msgs[0] if system_msgs else config.base_model.system_prompt    # First system message or default\n",
    "    user_query = user_msgs[-1] if user_msgs else \"\"    # Last user message\n",
    "\n",
    "    # Minimal chat history excluding system messages, max 10\n",
    "    num_recent = 10\n",
    "    conversation_messages = [msg for msg in request.messages if msg[\"role\"] != \"system\"]\n",
    "    chat_history = conversation_messages[-num_recent:-1]\n",
    "\n",
    "    temperature = request.temperature if (request.temperature and 2 > request.temperature > 0) else config.base_model.temperature\n",
    "    max_tokens = request.max_tokens if (request.max_tokens and request.max_tokens > 512) else config.base_model.max_new_tokens\n",
    "    kwargs = {\"temperature\": temperature, \"max_new_tokens\": max_tokens}\n",
    "\n",
    "    if RAG and thread_id is not None:\n",
    "        rag_res = RAG.apply(\n",
    "            user_id=user_id,\n",
    "            thread_id=thread_id,\n",
    "            user_query=user_query,\n",
    "            has_files_message=has_files_message,\n",
    "        )\n",
    "        user_query = rag_res.augmented_query\n",
    "        if rag_res.used:\n",
    "            LOGGER.info(f\"[RAG] used context (top_sim={rag_res.meta.get('top_similarity', 0):.3f})\")\n",
    "        else:\n",
    "            LOGGER.info(f\"[RAG] no context (reason={rag_res.meta.get('reason')})\")\n",
    "\n",
    "    # Model call (stream or non-stream handled by router)\n",
    "    return LLM.ask(\n",
    "        user_prompt=user_query,\n",
    "        system_prompt=system_prompt,\n",
    "        chat_history=chat_history,\n",
    "        stream=request.stream,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "@ns.stop_generation()\n",
    "def stop_handler():\n",
    "    print(\"Stopping generation...\")\n",
    "    global LLM\n",
    "    LLM.stop_generation()\n",
    "\n",
    "def create_app():\n",
    "    return ns.app\n",
    "\n",
    "def main():\n",
    "    ns.run()\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346487b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Importance of sleep in health.',\n",
       " 'summary': \"Top 3 results out of ~988,000,000 results for: 'Importance of sleep in health.'\\n1. How Sleep Works - Why Is Sleep Important? - NHLBI - NIH ‚Äî https://www.nhlbi.nih.gov/health/sleep/why-sleep-important\\n2. Better sleep: Why it's important for your health and tips to ... ‚Äî https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03\\n3. Sleep is essential to health - Journal of Clinical Sleep Medicine ‚Äî https://jcsm.aasm.org/doi/10.5664/jcsm.9476\",\n",
       " 'provider': 'serpapi',\n",
       " 'elapsed_ms': 3004,\n",
       " 'rag_content': 'Source: 355e311e80b4be26:cc35f490\\nlth Sleep has become a precious commodity ‚Äì we love it and need it, but rarely get enough of it. Busy schedules, kids, anxiety and technology can all get in the way of a good night sleep. Getting enough sleep can play an important role in your weight, emotional wellbeing, blood pressure, diabetes, mental and physical performance, and more. Remember that adults aren\\'t the only ones who need good sleep. It\\'s also critical that children get even more sleep than adults. Why is sleep important for health? The three pillars of health are nutrition, physical exercise, and sleep. All three of these are connected. For example, if you don\\'t sleep well, you may not eat well. People get food cravings when they haven\\'t slept well, and they often crave a food with lots of carbohydrates (carbs) like a cookie. And when you are tired, the last thing you want to do is go to the gym. People who are fully functioning pay attention to all three. They must all be working together for better health. Here are\\n\\n---\\n\\nSource: 1569daa75eb865f1:cc35f490\\nBack To How Sleep Works Why Is Sleep Important? 0 How Sleep Works MENU Home Health Topics < Back To How Sleep Works Why Is Sleep Important? How Sleep Works Your Sleep/Wake Cycle Sleep Phases and Stages Why Is Sleep Important? How Much Sleep Is Enough? MORE INFORMATION Fact Sheets and Handouts Research How Sleep Works How Sleep Works Why Is Sleep Important? Language switcher English Espa√±ol IN THIS ARTICLE View More View Less Sleep plays a vital role in good health and well-being throughout your life. The way you feel while you are awake depends in part on what happens while you are sleeping. During sleep, your body is working to support healthy brain function and maintain your physical health. In children and teens, sleep also helps support growth and development. Getting inadequate sleep over time can raise your risk for chronic (long-term) health problems. It can also affect how well you think, react, work, learn, and get along with others. Learn how sleep affects your heart and circ\\n\\n---\\n\\nSource: 4bd421ad2e29abac:cc35f490\\nBetter sleep: Why it‚Äôs important for your health and tips to sleep soundly | Cultivating Health | UC Davis Health search Search all UC Davis health Main Menu add menu Main Menu close Main Menu Main Menu remove UC Davis Health Home Patients & Visitors Services & Specialties Health Care Professionals Schools & Programs Research News About UC Davis Health Giving Careers search Search √ó Search all UC Davis Health Search All UC Davis Health Search Search enhanced by Google close Search all UC Davis Health Search All UC Davis Health Search Search enhanced by Google Skip to main content Cultivating Health Show menu menu Menu Cancer Care Children\\'s Health Fitness Heart Health Mental Health All Articles notifications Subscribe Mental Health MARCH 15, 2023 Better sleep: Why it‚Äôs important for your health and tips to sleep soundly By Cultivating Health Sleep has become a precious commodity ‚Äì we love it and need it, but rarely get enough of it. Busy schedules, kids, anxiety and technology can all\\n\\n---\\n\\nSource: 91de3a34d7fa00e2:cc35f490\\nant to do is go to the gym. People who are fully functioning pay attention to all three. They must all be working together for better health. Here are some other health benefits of sleep: promotes growth helps heart health supports weight management helps combat germs and keep your immune system strong reduces risk of injury increases attention span boosts memory and learning Find out if melatonin is safe, its side effects and if it helps you sleep How much sleep should adults get? Studies show that adults should get seven to eight hours a night for good health. Some people insist that they can get away with four or five hours of sleep. While these so-called \"short sleepers\" do exist, they are a very small percentage of the population. The rest of the self-identified \"short sleepers\" are mostly staying alert by drinking coffee or other caffeinated drinks. Not getting enough sleep can raise the risk of health consequences. However, getting enough sleep isn\\'t just about the number of hou\\n\\n---\\n\\nSource: e388173c38819c5e:cc35f490\\nrm) health problems. It can also affect how well you think, react, work, learn, and get along with others. Learn how sleep affects your heart and circulatory system, metabolism , respiratory system, and immune system and how much sleep is enough. BROCHURE This brochure describes the differences between the types of sleep needed to feel awake and to be healthy and offers tips for getting a good night‚Äôs sleep. View the brochure Heart and circulatory system When you fall asleep and enter non-REM sleep , your blood pressure and heart rate fall. During sleep, your parasympathetic system controls your body, and your heart does not work as hard as it does when you are awake. During REM sleep and when waking, your sympathetic system is activated, increasing your heart rate and blood pressure to the usual levels when you are awake and relaxed. A sharp increase in blood pressure and heart rate upon waking has been linked to angina, or chest pain, and heart attacks . People who do not sleep enoug\\n\\n---\\n\\nSource: 527693a794d92a2b:cc35f490\\nffeinated drinks. Not getting enough sleep can raise the risk of health consequences. However, getting enough sleep isn\\'t just about the number of hours you\\'re asleep. It\\'s also about the quality of sleep and that you stay on a regular schedule so that you feel rested when you wake up. Learn about anxiety symptoms and when to know if you need help How much sleep should children get? According to the U.S. Department of Health and Human Services , these are the recommended number of hours of sleep based on a child\\'s age: Newborns: 14-17 hours a day Babies: 12-16 hours a day (including naps) Toddlers: 11-14 hours a day (including naps) Preschoolers: 10-13 hours a day (including naps) School-aged children: 9-12 hours each night Teenagers: 8-10 hours each night What are some health risks of not getting enough sleep? Not enough sleep or routinely getting broken sleep is linked with seven of the 15 leading causes of death in the U.S. These include: Heart disease Cancerous tumors Diseases rela\\n\\n---\\n\\nSource: f5deaa2fd34dff22:cc35f490\\nuctive pulmonary disease (COPD) . Asthma symptoms are usually worse during early morning sleep. Likewise, breathing problems in people who have lung diseases such as COPD can become worse during sleep. Sleep also affects different parts of your immune system, which become more active at different times of day. For example, when you sleep, a particular type of immune cell works harder. That is why people who do not sleep enough may be more likely to get colds and other infections. FACT SHEET Sleep Fact Sheet Learn some sleep terms and find out about treatments that can help with sleep apnea. View the fact sheet Problems with thinking and memory Sleep helps with learning and the formation of long-term memories. Not getting enough sleep or enough high-quality sleep can lead to problems focusing on tasks and thinking clearly. Read our Sleep Deprivation and Deficiency page for more information on how lack of sleep affects performance of daily activities, including driving and schoolwork. Bo\\n\\n---\\n\\nSource: baa9ae9e26cf11b5:cc35f490\\nHow Sleep Works - Why Is Sleep Important? | NHLBI, NIH Skip to main content An official website of the United States government Here‚Äôs how you know Here‚Äôs how you know Official websites use .gov A .gov website belongs to an official government organization in the United States. Secure .gov websites use HTTPS A lock ( A locked padlock ) or https:// means you‚Äôve safely connected to the .gov website. Share sensitive information only on official, secure websites. Search Query: Health Topics All Health Topics A-Z Asthma Heart-Healthy Living High Blood Pressure Sickle Cell Disease Sleep Apnea Calculate Your BMI Health Education Education Programs and Initiatives The Heart Truth¬Æ Learn More Breathe Better¬Æ Blood Diseases & Disorders Education Program Publications and Resources Research Clinical Trials and Studies Research Focus Areas Blood Disorders and Blood Safety Sleep Science and Sleep Disorders Lung Diseases Health Disparities Heart and Vascular Diseases Precision Medicine Activities Obe\\n\\n---\\n\\nSource: dbd50916457b69e1:cc35f490\\n, laptop, etc.) in an area of the house other than the bedrooms. Sleep in a dark room because light stimulates our brains. Use an alarm clock rather than your smartphone or tablet as a wakeup device. Keep room temperatures on the cooler side ‚Äì ideally low to mid-60s. Aim for a consistent bedtime routine and sleep schedule to help your body stay on a regular track. Find a good time for you to go to sleep every night and wake up at the same time every morning. It\\'s also important to keep that same schedule even on the weekends. Find out about social media\\'s impact on our mental health and tips to use it safely What happens to your brain when you don\\'t get enough sleep? Sleep deprivation affects your ability to remember, concentrate, and make good decisions. Your reaction time is also reduced. A sleep-deprived driver has the same poor response time as someone who is legally drunk. Not getting enough sleep makes us more emotionally unstable. Lack of sleep can cause you to have very strong\\n\\n---\\n\\nSource: 10685d511e3aaad8:cc35f490\\nse time as someone who is legally drunk. Not getting enough sleep makes us more emotionally unstable. Lack of sleep can cause you to have very strong emotions, such as extreme sadness or anger. Does sleep play a role in Alzheimer\\'s disease? One thing that connects almost all mental and nervous system disorders is some level of wake and sleep disruption. Health experts know that treating sleep disruptions can help stabilize neurologic disorders. But left untreated, sleep disruption may contribute to the progression of disease. One example is Alzheimer\\'s disease . We know that sleep is disrupted in the early stages of the disease. If we could address that early on, perhaps the progression of the disease could be delayed. Patrick M. Fuller , a neuroscientist who studies how the brain regulates sleeping and waking, contributed and reviewed this article. Fuller is a professor in UC Davis Health\\'s Department of Neurological Surgery and vice chair for research. Explore related topics Mental H',\n",
       " 'llm_summary': 'Sleep is crucial for overall health and well-being, playing a vital role in physical health, mental function, emotional stability, and immune system support (source: https://www.nhlbi.nih.gov/health/sleep/why-sleep-important). \\n\\n**Key Points:**\\n- Sleep supports brain function, including memory consolidation and learning (source: https://jcsm.aasm.org/doi/10.5664/jcsm.9476).\\n- It helps regulate bodily functions such as heart health, blood pressure, and metabolism (source: https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03).\\n- Chronic sleep deprivation is linked to an increased risk of chronic diseases, including heart disease, diabetes, and weakened immune function (source: https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03).\\n- Adults are recommended to get 7‚Äì8 hours of sleep per night, while children and teens require more (source: https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03).\\n\\n**Caveats:**\\n- The quality of sleep is as important as the quantity. Poor sleep quality can have similar negative effects as insufficient sleep (source: https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03).\\n- Some individuals, known as \"short sleepers,\" may function with less sleep, but this is rare and not recommended for most people (source: https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03).\\n- There is some debate about the exact impact of sleep on conditions like Alzheimer\\'s disease, with some studies suggesting a potential link between sleep disruption and disease progression (source: https://health.ucdavis.edu/blog/cultivating-health/better-sleep-why-its-important-for-your-health-and-tips-to-sleep-soundly/2023/03).'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# searches.results\n",
    "# # print(searches.results[\"rag_content\"])\n",
    "# print(searches.results[\"llm_summary\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3dbe3",
   "metadata": {},
   "source": [
    "**GrpahAgent from YML file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bf1ccb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-18 14:58:05\u001b[0m | \u001b[96mutils.py:normalize_and_validate_graph_inputs\u001b[0m | Ignoring extra inputs not declared in graph spec: audience, tone\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 14:58:05\u001b[0m | \u001b[96mtoolkit.py:register_tool\u001b[0m | Registered tool: web_search\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmanager\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\n",
      "\u001b[2m ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m2.\u001b[0m\u001b[2m249s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmanager\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing End!\u001b[0m\n",
      "\n",
      "\u001b[1;4;33müß† Thinking\u001b[0m\u001b[1;4;33m...\u001b[0m\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mresearch\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\n",
      "\u001b[2m     ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\n",
      "        \u001b[1;32mINFO: Selected tool: web_search\u001b[0m\n",
      "        \u001b[1;32mINFO: Raw inputs: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'query'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m'The Missing Middle Layer: Why LLM Systems Need Tool Routers, Not Bigger Models'\u001b[0m\u001b[1;32m}\u001b[0m\n",
      "\u001b[2m     ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.router_llm_call'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m1.\u001b[0m\u001b[2m079s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m     ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\n",
      "\u001b[93mWARNING \u001b[0m | \u001b[90m2025-11-18 14:58:09\u001b[0m | \u001b[96mingestor.py:ingest\u001b[0m | Some sources were skipped as unsupported: [None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 16.04it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 53.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 14:58:09\u001b[0m | \u001b[96magent.py:retrieve\u001b[0m | [RAGRetriever] Retrieved 10 chunks\n",
      "        \u001b[1;32mINFO: Tool \u001b[0m\u001b[1;32m'web_search'\u001b[0m\u001b[1;32m Tool Return: \u001b[0m\u001b[1;32m{\u001b[0m\u001b[1;32m'query'\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m'The Missing Middle Layer: Why LLM Systems Need Tool Routers, Not Bigger Models'\u001b[0m\u001b[1;32m, 'summary\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "\u001b[2m     ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.tool.execute\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.route_and_call.tool_execute'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m933s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[2m ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.agent\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'research'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'agent.run'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m2.\u001b[0m\u001b[2m014s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;91mFalse\u001b[0m\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mresearch\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing End!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;4;36m[\u001b[0m\u001b[1;4;36mmanager\u001b[0m\u001b[1;4;36m]\u001b[0m\u001b[1;4;36m Tracing Start!\u001b[0m\n",
      "\u001b[2m ‚ñ∂ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m ‚óÄ \u001b[0m\u001b[1;2m[\u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[1;2m]\u001b[0m\u001b[1;2m[\u001b[0m\u001b[2mstep.llm.call\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33magent_id\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager'\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mlabel\u001b[0m\u001b[2m=\u001b[0m\u001b[2;32m'manager.compose_user_prompt'\u001b[0m\u001b[2m took \u001b[0m\u001b[1;2;36m0.\u001b[0m\u001b[2m790s; \u001b[0m\u001b[2;33merror\u001b[0m\u001b[2m=\u001b[0m\u001b[2;3;92mTrue\u001b[0m\n",
      "\u001b[1;4;31mError executing node outline: CUDA out of memory. Tried to allocate \u001b[0m\u001b[1;4;31m158.00\u001b[0m\u001b[1;4;31m MiB. \u001b[0m\n",
      "\u001b[1;4;31mGPU \u001b[0m\u001b[1;4;31m0\u001b[0m\u001b[1;4;31m has a total capacity of \u001b[0m\u001b[1;4;31m11.61\u001b[0m\u001b[1;4;31m GiB of which \u001b[0m\u001b[1;4;31m120.00\u001b[0m\u001b[1;4;31m MiB is free. Including \u001b[0m\n",
      "\u001b[1;4;31mnon-PyTorch memory, this process has \u001b[0m\u001b[1;4;31m10.70\u001b[0m\u001b[1;4;31m GiB memory in use. Of the allocated \u001b[0m\n",
      "\u001b[1;4;31mmemory \u001b[0m\u001b[1;4;31m9.91\u001b[0m\u001b[1;4;31m GiB is allocated by PyTorch, and \u001b[0m\u001b[1;4;31m484.27\u001b[0m\u001b[1;4;31m MiB is reserved by PyTorch \u001b[0m\n",
      "\u001b[1;4;31mbut unallocated. If reserved but unallocated memory is large try setting \u001b[0m\n",
      "\u001b[1;4;31mPYTORCH_CUDA_ALLOC_CONF\u001b[0m\u001b[1;4;31m=\u001b[0m\u001b[1;4;31mexpandable_segments\u001b[0m\u001b[1;4;31m:\u001b[0m\u001b[1;3;4;31mTrue\u001b[0m\u001b[1;4;31m to avoid fragmentation.  See \u001b[0m\n",
      "\u001b[1;4;31mdocumentation for Memory Management  \u001b[0m\n",
      "\u001b[1;4;31m(\u001b[0m\u001b[1;4;31mhttps://pytorch.org/docs/stable/notes/cuda.html#environment-variables\u001b[0m\u001b[1;4;31m)\u001b[0m\n",
      "\u001b[1;4;33mReturning partial results from executed nodes\u001b[0m\u001b[1;4;33m...\u001b[0m\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-18 14:58:10\u001b[0m | \u001b[96mexport.py:export_single_node\u001b[0m | Exported node research output to exports/research_20251118_145807.json\n"
     ]
    }
   ],
   "source": [
    "from neurosurfer.models.chat_models.base import BaseChatModel\n",
    "from neurosurfer.agents.graph import GraphAgent, ManagerConfig\n",
    "\n",
    "graph_agent = GraphAgent(\n",
    "    llm=LLM,\n",
    "    graph_yaml=\"blog_workflow.yml\",\n",
    "    toolkit=toolkit,\n",
    "    manager_config=ManagerConfig(\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=4096,\n",
    "    ),\n",
    "    manager_llm=LLM,\n",
    "    log_traces=True\n",
    ")\n",
    "\n",
    "# Run workflow\n",
    "graph_inputs = {\n",
    "    \"topic_title\": \"The Missing Middle Layer: Why LLM Systems Need Tool Routers, Not Bigger Models\",\n",
    "    \"query\": \"Compose a 2000-2500 word blog on why tool-routing layers matter more than scaling LLM size, covering practical design patterns, examples, and tradeoffs.\",\n",
    "    \"audience\": \"Intermediate ML engineers\",\n",
    "    \"tone\": \"Practical and slightly opinionated\",\n",
    "}\n",
    "\n",
    "results = graph_agent.run(inputs=graph_inputs)\n",
    "# result = await run_async(executor.run(inputs=graph_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205b51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6cb6796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"graph_agent_results.json\", \"w\") as writer:\n",
    "    json.dump(results.model_dump(), writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "303ddfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Review of Draft: \"Understanding Tool-Augmented LLM Agents: Architecture, Workflow, and Best Practices\"\n",
      "\n",
      "---\n",
      "\n",
      "## ‚úÖ **Strengths**\n",
      "\n",
      "- **Comprehensive Overview**: The draft provides a well-structured overview of tool-augmented LLM agents, covering key components like architecture, workflow, and best practices.\n",
      "- **Clear Terminology**: The use of consistent terminology (e.g., \"tool-augmented agents\", \"agent loop\") helps maintain clarity.\n",
      "- **Practical Focus**: The inclusion of best practices and workflow diagrams adds practical value for developers and researchers.\n",
      "- **Up-to-Date References**: The draft references recent advancements in LLM agent systems, such as the use of retrieval-augmented generation (RAG) and tool integration.\n",
      "\n",
      "---\n",
      "\n",
      "## ‚ùå **Issues and Concerns**\n",
      "\n",
      "### 1. **Technical Inaccuracies**\n",
      "\n",
      "- **Overgeneralization of Tool Integration**: The draft refers to \"tools\" in a broad sense, but does not clearly distinguish between different types of tools (e.g., API-based, database, or external services). This may lead to confusion about what constitutes a \"tool\" in this context.\n",
      "- **Ambiguity in Agent Loop**: The description of the \"agent loop\" is somewhat vague. It mentions \"planning, execution, and feedback\" but lacks a clear sequence or explanation of how the agent decides when to use a tool.\n",
      "- **Misrepresentation of LLM Behavior**: The draft suggests that LLMs \"can autonomously choose when to use tools,\" which may not be accurate. In practice, the decision to use a tool is often guided by the agent's design, not the LLM itself, unless explicitly programmed with a tool-use policy.\n",
      "\n",
      "### 2. **Missing Explanations**\n",
      "\n",
      "- **Lack of Context on Tool Selection**: The draft does not explain how tools are selected or prioritized in an agent system. This is a critical aspect of agent design and should be elaborated.\n",
      "- **Insufficient Coverage of Tool Limitations**: The draft briefly mentions tool limitations but does not delve into common issues such as latency, API rate limits, or error handling, which are important for real-world deployment.\n",
      "- **No Mention of Evaluation Metrics**: There is no discussion of how to evaluate the performance of tool-augmented agents, such as accuracy, efficiency, or robustness.\n",
      "\n",
      "### 3. **Stylistic and Clarity Issues**\n",
      "\n",
      "- **Repetition of Concepts**: Some sections repeat similar ideas, which can be streamlined to improve readability.\n",
      "- **Lack of Visual Aids**: While the draft mentions diagrams, it does not include them. Including visual aids (e.g., architecture diagrams or workflow charts) would greatly enhance understanding.\n",
      "- **Ambiguous Terminology**: Terms like \"tool-augmented\" are used without a clear definition, which may confuse readers unfamiliar with the concept.\n",
      "\n",
      "---\n",
      "\n",
      "## üõ†Ô∏è **Recommendations**\n",
      "\n",
      "### 1. **Clarify the Role of Tools in Agent Systems**\n",
      "- Define what constitutes a \"tool\" in the context of LLM agents.\n",
      "- Differentiate between internal and external tools, and explain how they are integrated into the agent's decision-making process.\n",
      "\n",
      "### 2. **Elaborate on the Agent Loop and Tool Integration**\n",
      "- Provide a step-by-step explanation of the agent loop, including how the LLM decides to use a tool.\n",
      "- Include examples of how tools are invoked and how their outputs are processed.\n",
      "\n",
      "### 3. **Add a Section on Tool Selection and Prioritization**\n",
      "- Discuss strategies for selecting and prioritizing tools (e.g., based on relevance, reliability, or cost).\n",
      "- Explain how agents handle tool limitations and errors.\n",
      "\n",
      "### 4. **Include Evaluation and Performance Metrics**\n",
      "- Introduce common evaluation metrics for tool-augmented agents, such as task success rate, response time, and error recovery.\n",
      "- Provide examples of real-world use cases where these metrics are applied.\n",
      "\n",
      "### 5. **Incorporate Visual Aids**\n",
      "- Add diagrams to illustrate the architecture and workflow of tool-augmented agents.\n",
      "- Consider including a comparison table of different tool integration strategies.\n",
      "\n",
      "### 6. **Improve Clarity and Avoid Ambiguity**\n",
      "- Define key terms like \"tool-augmented\" and \"agent loop\" at the beginning or in a dedicated glossary.\n",
      "- Avoid vague statements like \"LLMs can autonomously choose when to use tools\" and instead clarify that this behavior is guided by the agent's design and policy.\n",
      "\n",
      "---\n",
      "\n",
      "## ‚úÖ **Final Thoughts**\n",
      "\n",
      "The draft provides a solid foundation for understanding tool-augmented LLM agents, but it would benefit from greater technical precision, clearer explanations, and more structured content. With the suggested improvements, it could serve as a valuable resource for both researchers and practitioners in the field of LLM agent development.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(results[\"results\"][\"review\"].raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a795f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentConfig(allow_input_pruning=True, repair_with_llm=True, strict_tool_call=False, temperature=0.7, max_new_tokens=512, return_stream_by_default=False, retry=RouterRetryPolicy(max_route_retries=2, max_tool_retries=1, backoff_sec=0.7), strict_json=True, max_repair_attempts=1)\n",
      "retries=None timeout_s=None budget=NodeBudget(max_new_tokens=None, temperature=1.2, return_stream_by_default=None) allow_input_pruning=None repair_with_llm=None strict_tool_call=None strict_json=None max_repair_attempts=None\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class RouterRetryPolicy:\n",
    "    \"\"\"Retry tuning for routing + tool execution.\"\"\"\n",
    "    max_route_retries: int = 2\n",
    "    max_tool_retries: int = 1\n",
    "    backoff_sec: float = 0.7  # linear backoff\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    \"\"\"\n",
    "    Top-level configuration for the Agent.\n",
    "    \"\"\"\n",
    "    # Routing:\n",
    "    allow_input_pruning: bool = True    # drop unknown inputs not in ToolSpec\n",
    "    repair_with_llm: bool = True        # ask LLM to repair invalid routing/inputs\n",
    "    strict_tool_call: bool = False      # router must output JSON; else can answer in plain text\n",
    "    # synonyms: Dict[str, Dict[str, str]] = field(default_factory=dict)  # field -> {from: to}\n",
    "\n",
    "    # LLM defaults:\n",
    "    temperature: float = 0.7\n",
    "    max_new_tokens: int = 512\n",
    "    return_stream_by_default: bool = False\n",
    "\n",
    "    # Retries:\n",
    "    retry: RouterRetryPolicy = field(default_factory=RouterRetryPolicy)\n",
    "\n",
    "    # Structured-output options:\n",
    "    strict_json: bool = True                  # enforce RFC 8259 JSON\n",
    "    max_repair_attempts: int = 1              # for malformed JSON repairs\n",
    "\n",
    "\n",
    "\n",
    "class NodeBudget(BaseChatModel):\n",
    "    \"\"\"\n",
    "    Budget / LLM-related overrides per node.\n",
    "\n",
    "    These map directly to AgentConfig fields:\n",
    "        - temperature      -> AgentConfig.temperature\n",
    "        - max_new_tokens   -> AgentConfig.max_new_tokens\n",
    "        - return_stream_by_default -> AgentConfig.return_stream_by_default\n",
    "    \"\"\"\n",
    "\n",
    "    max_new_tokens: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=\"Override AgentConfig.max_new_tokens for this node only.\",\n",
    "    )\n",
    "    temperature: Optional[float] = Field(\n",
    "        default=None,\n",
    "        description=\"Override AgentConfig.temperature for this node only.\",\n",
    "    )\n",
    "    return_stream_by_default: Optional[bool] = Field(\n",
    "        default=None,\n",
    "        description=\"Override AgentConfig.return_stream_by_default for this node only.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class NodePolicy(BaseChatModel):\n",
    "    \"\"\"\n",
    "    Per-node policy that can override some AgentConfig settings and add\n",
    "    node-level execution constraints (e.g., timeout).\n",
    "\n",
    "    YAML example:\n",
    "\n",
    "        nodes:\n",
    "          - id: research\n",
    "            policy:\n",
    "              retries: 1\n",
    "              timeout_s: 30\n",
    "              budget:\n",
    "                max_new_tokens: 180\n",
    "                temperature: 0.2\n",
    "              allow_input_pruning: false\n",
    "              repair_with_llm: true\n",
    "              strict_tool_call: true\n",
    "    \"\"\"\n",
    "\n",
    "    retries: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=\"Override AgentConfig.retry.max_route_retries for this node.\",\n",
    "    )\n",
    "    timeout_s: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Soft timeout for this node in seconds. Execution isn't forcibly \"\n",
    "            \"cancelled but the node will be marked as errored if exceeded.\"\n",
    "        ),\n",
    "    )\n",
    "    budget: Optional[NodeBudget] = None\n",
    "\n",
    "    # Direct AgentConfig-like overrides\n",
    "    allow_input_pruning: Optional[bool] = None\n",
    "    repair_with_llm: Optional[bool] = None\n",
    "    strict_tool_call: Optional[bool] = None\n",
    "    strict_json: Optional[bool] = None\n",
    "    max_repair_attempts: Optional[int] = None\n",
    "\n",
    "    class Config:\n",
    "        extra = \"ignore\"  # ignore unknown keys under 'policy'\n",
    "\n",
    "c = AgentConfig()\n",
    "\n",
    "p = NodePolicy(budget=NodeBudget(temperature=1.2))\n",
    "\n",
    "print(c)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93052a6d",
   "metadata": {},
   "source": [
    "### Python API version (no YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "926be727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: True\n",
      "Answer:\n",
      " The calculator result for your request is ${compute.text}. This means that after performing the calculation based on your input, the final answer is ${compute.text}. Let me know if you need further assistance!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from graph import Graph, Node, NodePolicy, GraphConfig, GraphExecutor\n",
    "from neurosurfer.tools import Toolkit\n",
    "from neurosurfer.models.chat_models.openai import OpenAIModel\n",
    "\n",
    "# Reuse your existing toolkit + model\n",
    "llm = LLM  # already created in your environment\n",
    "tk = toolkit\n",
    "\n",
    "graph = Graph(\n",
    "    name=\"calc_and_explain\",\n",
    "    config=GraphConfig(max_concurrency=2),\n",
    "    inputs_schema={\"prompt\": str},\n",
    "    nodes=[\n",
    "        Node(\n",
    "            id=\"rewrite\",\n",
    "            fn=\"general_query_assistant\",  # adjust name if needed\n",
    "            inputs={\n",
    "                # swap \"query\" -> \"prompt\" if your tool expects \"prompt\"\n",
    "                \"query\": (\n",
    "                    \"You will receive a user request. Extract a SINGLE pure arithmetic expression that can be \"\n",
    "                    \"evaluated by a calculator (e.g., '(42 * 7) - 5^2' or '0.035 * 12000').\\n\"\n",
    "                    \"- Do NOT include explanations.\\n\"\n",
    "                    \"- Return ONLY the expression as plain text.\\n\\n\"\n",
    "                    \"User request:\\n${inputs.prompt}\"\n",
    "                )\n",
    "            },\n",
    "            outputs=[\"num1\", \"num2\", \"operation\"],\n",
    "            policy=NodePolicy(\n",
    "                retries=1,\n",
    "                timeout_s=30,\n",
    "                budget={\"max_new_tokens\": 128, \"temperature\": 0.1},\n",
    "            ),\n",
    "        ),\n",
    "        Node(\n",
    "            id=\"compute\",\n",
    "            fn=\"calculator\",\n",
    "            inputs={\"num1\": \"${rewrite.num1}\", \"num2\": \"${rewrite.num2}\", \"operation\": \"${rewrite.operation}\"},\n",
    "            outputs=[\"text\"],\n",
    "            policy=NodePolicy(retries=0, timeout_s=15),\n",
    "        ),\n",
    "        Node(\n",
    "            id=\"explain\",\n",
    "            fn=\"general_query_assistant\",\n",
    "            inputs={\n",
    "                \"query\": (\n",
    "                    \"Original request: ${inputs.prompt}\\n\"\n",
    "                    \"Calculator result: ${compute.text}\\n\\n\"\n",
    "                    \"Write a brief, user-friendly explanation of the result (one short paragraph).\"\n",
    "                )\n",
    "            },\n",
    "            outputs=[\"text\"],\n",
    "            policy=NodePolicy(\n",
    "                retries=1,\n",
    "                timeout_s=30,\n",
    "                budget={\"max_new_tokens\": 180, \"temperature\": 0.2},\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    outputs={\"answer\": \"${explain.text}\"},\n",
    ")\n",
    "\n",
    "executor = GraphExecutor(llm=llm, toolkit=tk, max_concurrency=2)\n",
    "\n",
    "result = await run_async(\n",
    "    executor.run(graph, inputs={\"prompt\": \"Compute 3.5% of 12000 and explain\"}, stream=True)\n",
    ")\n",
    "\n",
    "print(\"OK:\", result.ok)\n",
    "print(\"Answer:\\n\", result.outputs[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e25a8",
   "metadata": {},
   "source": [
    "### Planner-based path (using the YAML as a skeleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5250b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, tempfile, pathlib\n",
    "from graph import PlannerAgent, FlowLoader, GraphExecutor\n",
    "\n",
    "# 1) Write the YAML to a temp file (only for this demo)\n",
    "yaml_text = r\"\"\"\n",
    "name: calc_and_explain\n",
    "inputs:\n",
    "  prompt: str\n",
    "config:\n",
    "  max_concurrency: 2\n",
    "nodes:\n",
    "  - id: rewrite\n",
    "    kind: task\n",
    "    fn: general_query_assistant\n",
    "    inputs:\n",
    "      query: |\n",
    "        You will receive a user request. Extract a SINGLE pure arithmetic expression that can be\n",
    "        evaluated by a calculator (e.g., \"(42 * 7) - 5^2\" or \"0.035 * 12000\").\n",
    "        - Do NOT include explanations.\n",
    "        - Return ONLY the expression as plain text.\n",
    "\n",
    "        User request:\n",
    "        ${inputs.prompt}\n",
    "    outputs: [\"text\"]\n",
    "    policy: { retries: 1, timeout_s: 30, budget: { max_new_tokens: 128, temperature: 0.1 } }\n",
    "\n",
    "  - id: compute\n",
    "    kind: task\n",
    "    fn: calculator\n",
    "    inputs: { expression: ${rewrite.text} }\n",
    "    outputs: [\"text\"]\n",
    "\n",
    "  - id: explain\n",
    "    kind: task\n",
    "    fn: general_query_assistant\n",
    "    inputs:\n",
    "      query: |\n",
    "        Original request: ${inputs.prompt}\n",
    "        Calculator result: ${compute.text}\n",
    "\n",
    "        Write a brief, user-friendly explanation of the result (one short paragraph).\n",
    "    outputs: [\"text\"]\n",
    "    policy: { retries: 1, timeout_s: 30, budget: { max_new_tokens: 180, temperature: 0.2 } }\n",
    "\n",
    "outputs: { answer: ${explain.text} }\n",
    "\"\"\".strip()\n",
    "\n",
    "tmp = pathlib.Path(tempfile.gettempdir()) / \"calc_and_explain.yml\"\n",
    "tmp.write_text(yaml_text)\n",
    "\n",
    "# 2) Use the planner with a skeleton (so it returns your YAML-based Graph)\n",
    "planner = PlannerAgent(llm=LLM)  # LLM not used when skeleton is set\n",
    "graph = planner.plan_from_query(query=\"Compute 3.5% of 12000 and explain\", skeleton=str(tmp))\n",
    "\n",
    "# 3) Execute\n",
    "executor = GraphExecutor(llm=LLM, toolkit=toolkit, max_concurrency=2)\n",
    "result = asyncio.run(executor.run(graph, inputs={\"prompt\": \"Compute 3.5% of 12000 and explain\"}))\n",
    "\n",
    "print(\"OK:\", result.ok)\n",
    "print(result.outputs[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225587f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1f9ca96",
   "metadata": {},
   "source": [
    "Test ToolsRouterAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98d83d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:32\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Using tool: calculator\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:32\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Raw inputs: {'num1': 20.0, 'num2': 90.0, 'operation': 'multiply'}\n",
      "1800.0"
     ]
    }
   ],
   "source": [
    "query = \"Perform the calculation 20 * 90\"\n",
    "\n",
    "for chunk in tools_router_agent.run(query, temperature=0.7, max_new_tokens=4000):\n",
    "    print(chunk, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c6635c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:33\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Using tool: general_query_assistant\n",
      "\u001b[92mINFO    \u001b[0m | \u001b[90m2025-11-06 11:08:33\u001b[0m | \u001b[96mtools_router_agent.py:run\u001b[0m | [router] Raw inputs: {'query': 'Tell me a light-hearted joke!'}\n",
      "Why don't skeletons fight each other? They don't have the guts!None"
     ]
    }
   ],
   "source": [
    "query = \"Tell me a light-hearted joke!\"\n",
    "\n",
    "for chunk in tools_router_agent.run(query, temperature=0.7, max_new_tokens=4000):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafe29f",
   "metadata": {},
   "source": [
    "## ReactAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aa87f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[üß†] Chain of Thoughts...\n",
      "Thought: I will first calculate 300 - 300 using the calculator tool, and then I will use the general_query_assistant tool to find a light-hearted joke about the result.\n",
      "\n",
      "Action: {\n",
      "  \"tool\": \"calculator\",\n",
      "  \"inputs\": {\n",
      "    \"num1\": 300,\n",
      "    \"num2\": 300,\n",
      "    \"operation\": \"subtract\"\n",
      "  },\n",
      "  \"final_answer\": false\n",
      "}"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>üîß<span style=\"font-weight: bold\">]</span> Tool: calculator\n",
       "<span style=\"font-weight: bold\">[</span>üì§<span style=\"font-weight: bold\">]</span> Inputs: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'num1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'operation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'subtract'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0müîß\u001b[1m]\u001b[0m Tool: calculator\n",
       "\u001b[1m[\u001b[0müì§\u001b[1m]\u001b[0m Inputs: \u001b[1m{\u001b[0m\u001b[32m'num1'\u001b[0m: \u001b[1;36m300\u001b[0m, \u001b[32m'num2'\u001b[0m: \u001b[1;36m300\u001b[0m, \u001b[32m'operation'\u001b[0m: \u001b[32m'subtract'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Observation:</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mObservation:\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[üß†] Chain of Thoughts...\n",
      "Thought: The result of the calculation is 0. Now, I will use the general_query_assistant tool to find a light-hearted joke about the result.\n",
      "\n",
      "Action: {\n",
      "  \"tool\": \"general_query_assistant\",\n",
      "  \"inputs\": {\n",
      "    \"query\": \"Tell me a light-hearted joke about the number 0.\"\n",
      "  },\n",
      "  \"final_answer\": true\n",
      "}"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>üîß<span style=\"font-weight: bold\">]</span> Tool: general_query_assistant\n",
       "<span style=\"font-weight: bold\">[</span>üì§<span style=\"font-weight: bold\">]</span> Inputs: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Tell me a light-hearted joke about the number 0.'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0müîß\u001b[1m]\u001b[0m Tool: general_query_assistant\n",
       "\u001b[1m[\u001b[0müì§\u001b[1m]\u001b[0m Inputs: \u001b[1m{\u001b[0m\u001b[32m'query'\u001b[0m: \u001b[32m'Tell me a light-hearted joke about the number 0.'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the number 0 break up with the number 8?  \n",
      "Because it found someone more \"8\" (8) than a zero!"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Observation:</span> Why did the number <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> break up with the number <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>?  \n",
       "Because it found someone more <span style=\"color: #008000; text-decoration-color: #008000\">\"8\"</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">)</span> than a zero!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mObservation:\u001b[0m Why did the number \u001b[1;36m0\u001b[0m break up with the number \u001b[1;36m8\u001b[0m?  \n",
       "Because it found someone more \u001b[32m\"8\"\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m than a zero!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[üß†] Chain of Thoughts...\n",
      "Thought: The calculation result is 0, and the joke provided is ready. The final answer is complete.\n",
      "\n",
      "Final Answer: The result of 300 - 300 is 0. Here's a light-hearted joke about it: Why did the number 0 break up with the number 8? Because it found someone more \"8\" (8) than a zero!"
     ]
    }
   ],
   "source": [
    "from neurosurfer.agents.react import ReActAgent, ReActConfig\n",
    "\n",
    "react_agent = ReActAgent(\n",
    "    toolkit=toolkit,\n",
    "    llm=LLM,\n",
    "    specific_instructions=\"Always be concise in your answers. Break the task into steps if needed.\",\n",
    "    config=ReActConfig(\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=4096,\n",
    "        allow_input_pruning=True,\n",
    "        repair_with_llm=True,\n",
    "        skip_special_tokens=True,\n",
    "        verbose=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# print(react_agent._system_prompt())\n",
    "TASK = \"\"\"Calculate 300 - 300. Then tell me a light-hearted joke about that result.\"\"\"\n",
    "\n",
    "for chunk in react_agent.run(TASK):\n",
    "    print(chunk, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41580e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c7a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1d90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766512d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad2c2b-4a95-4ac0-9727-4c746e97a629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ea014-d67c-4af9-b2f6-1a8d80d63b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
